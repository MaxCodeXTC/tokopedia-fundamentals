{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coursebook: Classification Model**\n",
    "- Part 5 of Python Fundamental Course\n",
    "- Course Length: 24 Hours\n",
    "- Last Updated: July 2019\n",
    "\n",
    "___\n",
    "\n",
    "- Developed by [Algoritma](https://algorit.ma)'s product division and instructors team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "The coursebook is part of the **Python Fundamentals Course** prepared by [Algoritma](https://algorit.ma). The coursebook is intended for a restricted audience only, i.e. the individuals and organizations having received this coursebook directly from the training organization. It may not be reproduced, distributed, translated or adapted in any form outside these individuals and organizations without permission.\n",
    "\n",
    "Algoritma is a data science education center based in Jakarta. We organize workshops and training programs to help working professionals and students gain mastery in various data science sub-fields: data visualization, machine learning, data modeling, statistical inference etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification in Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Objectives\n",
    "In this workshop, we'll extend our understanding of regression algorithms and see what we've learned in the previous workshop can be extended to solve a different kind of problems: classification problems. More specifically, we'll learn to solve binary and multi-class classification models using machine learning algorithms that are easily understood and in the case of logistic regression, readily interpretable. \n",
    "\n",
    "You will learn to develop classification algorithms from scratch, and investigate the mathematical foundations underpinning logistic regressions and nearest neighbors algorithms. My objective is to deliver a 9-hour session that is packed with the depth to help you develop, apply, score and evaluate two of the most highly versatile algorithms widely used today.\n",
    "\n",
    "- **Logistic Regression**\n",
    "    - Understanding Odds  \n",
    "    - Log of Odds  \n",
    "    - Logistic Regression in Practice  \n",
    "    - Assumption and Limitation\n",
    "- **Nearest Neighbors Prediction**\n",
    "    - Calculating distance  \n",
    "    - KNN from Scratch\n",
    "    - Non parametric model\n",
    "- **Model Evaluation**  \n",
    "    - Cross Validation  \n",
    "    - Bias variance trade-off  \n",
    "    - Confusion matrix  \n",
    "- **Classification and Clustering**  \n",
    "    - Exploratory using clustering  \n",
    "    - Goodness of fit  \n",
    "    - Recommendation between cluster members\n",
    "    \n",
    "By the end of this course, you'll be working on a **Learn-by-Building** module to create a data exploratory analysis project to apply what you have learned on provided dataset and attempt to answer all the given questions. This final part is considered as a Graded Assignment so make sure you do well on the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going into our main topic for this coursebook, let's import the packages we are going to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "### Theory\n",
    "Logistic regression is a classification algorithm used to fit a regression curve, $y = f(x)$, where $y$ is a categorical variable. When $y$ is binary (1 for spam, 0 for not-spam) we also call the model **binomial logistic regression** where in cases of $y$ assuming more than 2 values you'll sometimes hear the model being referred to as a class of **multinomial logistic regression**. We can think of logistic regression as a special case of linear regression (which you've mastered in the previous workshop), except we're using **log of odds** as our target variable. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relation to Probability\n",
    "So it's perhaps important to understand what odds mean. Most of us are familiar with **probabilities**. We understood that the **probability** of an event is the proportion of times it will occur divided by the total number of trials. If an event occurs 1 out of 5 times, then the probability (`p`) would be 1 out of 5, or 0.2. \n",
    "\n",
    "Odds are defined as the probability that an event will occur (`p`) divided by the probability than the event will not occur (we'll call it `q`, which is the same as `1-p`). If p is 0.2, we will see that q is 0.8. Expressed in a formula, odds can then be defined as:  \n",
    "$\\frac{p}{(1-p)}$\n",
    "\n",
    "Let's use a fun and real-life example. Supposed we were playing black jack (assuming the casino uses two decks on black jack) and the first card dealt is an Ace, the probability of the next card dealt to the dealer is a Ten is 31.07% (32 possible Tens out of 103 possibility). If we have to express it in odds and define p as 0.31, then our odds of the dealer being dealt a Blackjack (Ace + a Ten) is 0.31/(1-0.31), which brings it to 0.45 to 1. \n",
    "\n",
    "Note that if we have defined `p` as the probability of the Dealer **not having a Blackjack**, our odds would instead be 0.69/0.31, which brings us to 2.23 to 1. We can interpret this as \"for every 2.23 times the dealer didn't get a blackjack, she would get 1 blackjack\". Odds, as we so far understand it, refers to the ratio of favorable event (dealer doesn't get a blackjack) to the unfavorable event. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz 1: Odds of flying on time vs suffering a departure delay**\n",
    "\n",
    "Now to a less-fun but no less important example: airport delays. If I tell you that the probability of a minor departure delay occurring at a particularly busy airport (Soekarno-Hatta) on a festive holiday season is 0.2, what are the chances (expressed in odds) of you departing on time versus that of a departure delay. Recall the formula: \n",
    "\n",
    "$Odds = \\frac{No-delay}{Delay}$\n",
    "\n",
    "I hope you arrived at the right answer of 4 to 1, and intuitively interpret the situation as \"we are 4 times more likely to depart on time than to be delayed\". \n",
    "\n",
    "Odds are rather commonly used in some industry and in sports. In football and in horse racing, you'll often see betting odds expressed as fractions (e.g. 3/1 for a Germany win). In some academic writing or journalistic reporting, you may also see odds being expressed such as this: \"the relative risk of a credit event with Financial Product A over Product B is 1.125\". If you think about it, this is the same concept we've been talking about: odds. \n",
    "\n",
    "If it wasn't immediately clear, consider assigning some numbers to the above example:  \n",
    "- Financial Product A has a 0.45 empirical probability of incurring a credit event  \n",
    "- Financial Product B has a 0.4 empirical probability of incurring a credit event  \n",
    "\n",
    "The odds is hence 0.45/0.4, or 1.125:1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding log of odds\n",
    "When we have a probability $p$, the log of odds (sometimes called the \"log-odds\") is simply the log of the odds ratio, which is:  \n",
    "$log(p/(1-p))$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Odds ratios as we observe above, are just an alternate way of expressing probabilities. Let's say we have the probability of success as 0.8, then the probability of failure is 1 - 0.8 = 0.2. The odds of success are defined as the probability of success over the probability of failure, in our case the odds would be .8/.2 = 4. We can also say that the odds of success is hence 4 to 1. If the probability of success is .5, i.e 50-50, our odds of success is 1 to 1.\n",
    "\n",
    "The transformation from probability to odds is a monotonic transformation, so the odds increases as the probability increase (however note that odds take a range of 0 to infinity):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(0.01, 0.99, 100)\n",
    "y = x/(1-x)\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.ylabel('Odds')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylim(0,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we have an **odd of 1 when our p is 0.5**, and our odd is 4, when p is 0.8, just as we learned from the earlier example (50:50 -> odds of 1, success rate of 0.8 -> odds of 4). \n",
    "\n",
    "Now that we've understood the transformation from probability to odds, let's understand the transformation from odds to logs of odds. \n",
    "\n",
    "Log of odds are:\n",
    "$logit(p) = log(\\frac{p}{1-p})$\n",
    "\n",
    "Almost same code for the above curve, except this time we plot the curve of `log(x/(1-x))` instead of `(x/(1-x))`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5, 5)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(0.01, 0.99, 100)\n",
    "odds = x/(1-x)\n",
    "y = np.log(odds)\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.ylabel('Log of Odds')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylim(-5,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change `x` below from 0.5 to 1, and then to 0 to verify that the log of odds can take any positive or negative value (which is to say, its range is -Inf to Inf). A linear model can produce any value of log of odds and they would be acceptable as a prediction as the range is -Inf to Inf. That is not the case if a linear model has to produce a prediction that is a valid value of \"probability\", because a probability only takes a range of 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 0.5\n",
    "math.log(x/ (1-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the transformation of odds to log of odds is a monotonic one. The greater the odds, the greater the log of odds. However, recall that the probability of .5 will yield us a log-odds of 0. This is because the logit (log of odds) function takes values on [min, max] and transforms them to span [-Inf, Inf]. 5 is our median number and hence it's value on the log of odds scale is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding logit function\n",
    "\n",
    "In the case of a p=0.5 on a scale of 0 to 1, our *p* would then be p = ( 0.5 - 0 ) / (1 - 0) = 0.5; In the case of a p=30 on a scale of 1 to 100, our *p* would subsequently take on the value of (30-1)/(100-1) = 0.292929293.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29292929292929293"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(30-1)/99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.8811993779249543"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.log(0.2929293/(1-0.2929293))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, however, that the logit function puts our probability on the x-axis instead of the y-axis and we can *invert* both axes also called the Sigmoidal **logistic function**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code plot sigmoid curve using invers logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could be wondering by now why we're concerned with understanding these underlying concepts? It turns out that the reason is surprisingly straightforward if we approach it from our prior knowledge of linear regression models.  \n",
    "\n",
    "Recall that with linear regression, we are used to representing our hypothesis in the following form:\n",
    "\n",
    "$\\hat{y} = \\beta_0 + \\beta_1x_1 + ... + \\beta_mx_m$  \n",
    "Where m is the number of predictors\n",
    "\n",
    "But with that hypothesis, our value $\\hat{y}$ could take on any value from *-Inf* to *Inf*. This is obviously not very helpful for our classification task. Ideally, we want:\n",
    "\n",
    "$0 \\leq \\hat{y} \\leq 1$  \n",
    "\n",
    "This is because we can then set a threshold value, say 0.5, and classify any examples above 0.5 as a \"positive\" and any value below it as a \"negative\". Turns out, we can transform a simple linear regression model $\\hat{y} = \\beta_0 + \\beta_1x_1$ by applying the sigmoid function, also known as the logistic function so we would end up with a hypothesis that bound our value to the range of 0 to 1:\n",
    "$\\hat{y}  = sigmoid( \\beta_0 + \\beta_1x_1)$\n",
    "- where $\\hat{y}$ = estimated probability that y=1 on input x.  \n",
    "\n",
    "More formally:\n",
    "$\\hat{y} = P(y=1 | x;\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional] Extra Proof: Intuition behind the sigmoid function  \n",
    "This sub-chapter sheds light on another perspective behind the sigmoid function, in the hope of helping you make sense of the sigmoid function a little more.\n",
    "\n",
    "Starting from a simple linear regression example with an independent variable called \"Age\" (imagine predicting income based on age), we would have the following hypothesis:\n",
    "$\\hat{y} = \\beta_0 + \\beta_{Age}$\n",
    "\n",
    "In logistic regression, since we are only concerned about the probability of our outcome (target), we need our hypothesis to be between 0 and 1:\n",
    "$0 \\leq \\hat{y} \\leq 1$\n",
    "\n",
    "Recall that we can think of $\\hat{y}$ simply as a probability of y being 1, we can denote it as $p$ for the purpose of convenience. Since probability must always be positive, we put this linear equation in exponential form, such that for any value of slope and dependent variable, exponent of this equation will never be negative:\n",
    "$p = exp(\\beta_0 + \\beta_{Age}) = e^{(\\beta_0 + \\beta_{Age})}$\n",
    "\n",
    "Exponenting something would make it an always positive value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.315287191035679e-07"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.exp(-14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've made the range our $p$ can take on 0 to positive infinity; We still have one task to do - we need to make our probability assume a range smaller than 1, essentially making it take on the range of 0 to 1. To make the probability lesser than 1, we will divide p by a number greater than p. \n",
    "\n",
    "> Divide 4 by 5 and get 0.8; or 4 by 20 and get 0.2, for an arithmetic proof  \n",
    "\n",
    "So, back to making p lesser than 1:  \n",
    "$p = exp(\\beta_0 + \\beta_{Age}) / exp(\\beta_0 + \\beta_{Age} + 1) )$\n",
    "\n",
    "The above equation is of course equivalent to:\n",
    "$e^{(\\beta_0 + \\beta_{Age})} / e^{\\beta_0 + \\beta_{Age}+ 1)}$\n",
    "\n",
    "Putting all of these together, we can now rewrite the probability as:\n",
    "p = e^z / (1 + e^z)\n",
    "\n",
    "Where p is the probability of success (y=1) and `z` is the placeholder for $\\beta_0 + \\beta_{Age}$. `q`, the probability of failure, will then be:\n",
    "q = (1 - p) = 1 - ( e^z / (1 + e^z ) )\n",
    "\n",
    "Recalling what we know about *odds*, we can now define our odds as:\n",
    "$\\frac{p}{1-p}$  \n",
    "\n",
    "Let's expand from the above equation:  \n",
    "$\\frac{p}{1-p}$  = $p * \\frac{1}{(1-p)}$  \n",
    "                 = $\\frac{e^z}{1+e^z} * \\frac{1}{1-\\frac{e^z}{1+e^z}}$  \n",
    "                 = $\\frac{e^z}{(1+e^z) - (\\frac{e^z * (1+e^z)}{1+e^z})}$  \n",
    "                 = $\\frac{e^z}{(1+e^z) - e^z}$  \n",
    "                 = $\\frac{e^z}{1}$  \n",
    "\n",
    "So from the above odds equation $\\frac{p}{1-p} = e^z$, we can take the log on both sides and obtain:  \n",
    "$log(\\frac{p}{1-p}) = z$\n",
    "\n",
    "After substituting z for the actual hypothesis in our earlier linear regression example, we arrive at:\n",
    "$log(\\frac{p}{1-p}) = \\beta_0 + \\beta(Age)$\n",
    "\n",
    "This, we learned earlier, is the equation used in logistic regression. It turns out that we arrive at the log of odds which we've studied in the previous section! \n",
    "\n",
    "Another important observation: realize that regardless of what value x takes, our probability of success (y=1) will always be on the range of 0 to 1. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Assumptions of Logistic Regression  \n",
    "Many of the key assumptions of linear regression do not hold true with logistic regression. We've learned about the linearity assumption, normality of residuals, and homoskedasticity assumptions in our regression models class - they do not apply in the case of logistic regression.\n",
    "\n",
    "Logistic regression **does not** require a linear relationship between the dependent and independent variables - it also does not assume normality of residuals nor is it concerned with the problem of heteroskedasticity the way that linear regressions are.\n",
    "\n",
    "However, a few of the assumptions do apply:  \n",
    "- Multicollinearity: Just as with the case of linear regression, logistic regression assumes little to no multicollinearity among the independent variables (recall how we used VIF to identify highly correlated variables in the last workshop)  \n",
    "- Independence of Observations: The observations should not come from repeated measurements and are independent from each other  \n",
    "- Linearity of predictor and log odds: While logistic regressions do not assume linearity between the dependent and independent variables, it does assume that the independent variables (predictors) are linearly related to the log odds.  \n",
    "\n",
    "The first two points are rather self-explanatory, and the third will be illustrated to you in an example later (flight delay prediction). If put slightly differently, the third point stresses that a logistic regression models the logit-transformed probability as a linear relationship with the predictor variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Logistic Regression\n",
    "Supposed you work in an education institution and are put in charge to evaluate the likelihood of a student graduating with a honors degree given their academic scores in a reading test, writing test and mathematics test.  \n",
    "\n",
    "This dataset has four features: `female`, `read`, `write`, `math` and the target variable is `hon`, a binary feature with 1 indicating that the student is in fact in an honors class and 0 indicating otherwise. The dataset is credited to the UCLA: Statistical Consulting Group (see credits for link and details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>female</th>\n",
       "      <th>read</th>\n",
       "      <th>write</th>\n",
       "      <th>math</th>\n",
       "      <th>hon</th>\n",
       "      <th>femalexmath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>52</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>68</td>\n",
       "      <td>59</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>33</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>44</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>52</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   female  read  write  math  hon  femalexmath\n",
       "0       0    57     52    41    0            0\n",
       "1       1    68     59    53    0           53\n",
       "2       0    44     33    54    0            0\n",
       "3       0    63     44    47    0            0\n",
       "4       0    47     52    57    0            0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "honor = pd.read_csv(\"data_input/sample.csv\")\n",
    "honor.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with Discrete Predictor Variables\n",
    "\n",
    "To fully understand logistic regression, let's begin by looking at our `honor` proportion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    151\n",
       "1     49\n",
       "Name: hon, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "honor.hon.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our odds ratio, without the influence of any predictor variable, is 49 out of 200 (49 in honors classes vs 151 not), so that give us a probability of 49/200, p = 0.245. Our odds ratio is therefore 0.245/(1-0.245) = 0.3245033\n",
    "\n",
    "Before we attempt to interpret the parameters estimated from our model above, let's examine the odds ratio of a female being in a honors class as we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>female</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hon</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>74</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "female   0   1\n",
       "hon           \n",
       "0       74  77\n",
       "1       17  32"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(honor.hon, honor.female)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for logistic regression hon~female"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we attempt to interpret the parameters estimated from our model above, let's examine the odds ratio of a female being in a honors class as we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for odds ratio of female"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For males: odds of being in honors class = (17/91)/(74/91) = 0.2297297  \n",
    "- For females: odds of being in honors class = (32/109)/(77/109) = 0.4155844  \n",
    "- The ratio of the odds for female vs ratio of the odds for male = .42/.23 = 1.809, which is to say that the odds for female being in an honors class are about 81% more than that of their male counterpart  \n",
    "\n",
    "Let's now relate the odds ratio to the output from the logistic regression model with our `female` predictor variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code output summary model logreg hon~female"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intercept of **-1.4709** is the log odds for males since male is the reference group (**female** = 0). If we have wanted to confirm this, we can manually calculate this using the odds ratio for the male group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to get log of odds intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficient for **female** is the log of odds ratio between the female group and the male group, which can be manually calculated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to get log of odds female (coeff female)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using what we've learned earlier, we also know how easy it would be for us to calculate the odds ratio from the output of the model's summary: we simply have to exponentiate the coefficient it gives us for female. \n",
    "\n",
    "And if we were to relate this back to the original equation:\n",
    "$logit(p) = \\beta_0 + \\beta_1 * female$\n",
    "\n",
    "- For a male (female = 0): we would substitute the values into the equation and arrive at logit(p) = -1.4709  \n",
    "- For a female (female = 1): we would instead get logit(p) = -1.4709 + (0.5928*1) = -0.8781  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to get odds ratio of female (exp(-0.8781)/exp(-1.4709))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The ratio of the odds for female vs ratio of the odds for male = .42/.23 = 1.809, which is to say that the odds for female being in an honors class are about 81% more than that of their male counterpart  \n",
    "\n",
    "Notice how this is the same answer we derive from our manual calculation even before looking at the output of our logistic regression model. In fact, we could as well have taken the **estimated coefficient** value for `female`, which the output says is 0.5928, and get its exponent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code using exp (coef )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression with one continuous predictor variable\n",
    "Let's try another exercise, this time using the `math` score (continuous variable) such that the equation for our model is formally described as:\n",
    "\n",
    "$logit(p) = \\beta_0 + \\beta_1 * math$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code glm between hon~math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice in the case of a continuous variable such as the math score, our estimated coefficient for the intercept is the log odds of a student with a math score of zero being in an honors class. If we mentally visualize a plot with both x and y axis, this makes intuitive sense: the intercept points to the value of y **when our x feature = 0**. By taking the exponent of this value, we then know the odds of such student being in an honors class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code value of exp intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These odds are very low, and a peek at the distribution for the variable math will reveal that no one in the sample has a math score lower than 30 (mean of 53 in fact), which tells us that the intercept in this model corresponds to the log odds of being in an honors class when math is at the hypothetical value of zero.\n",
    "\n",
    "How do we interpret the coefficient for math? Recall our equation:\n",
    "\n",
    "$logit(p) = log(p/(1-p)) = \\beta_0 + \\beta_1 * math$\n",
    "\n",
    "With the substituted values:\n",
    "logit(p) = -9.79394 + 0.15634 * math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code hist of math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The median of math is ~52. Let's assume a `math` value of 52:\n",
    "logit(p) = -9.79394 + 0.15634 * 52 = -1.66426\n",
    "\n",
    "Examine the effect of a one-unit increase in math score, at 53:\n",
    "logit(p) = -9.79394 + 0.15634 * 53 = -1.50792\n",
    "\n",
    "Taking the difference:\n",
    "-1.50792 - (-1.66426) = 0.15634"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code of math coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and it is! So the coefficient for `math` is in fact the difference in the log odds for one unit of increment in that variable (math score of 53 vs 52). In simpler words, for one-unit increase in the math score, the expected change in log odds is 0.15634.\n",
    "\n",
    "Like the earlier example, we could also translate this change in log odds to the change in odds by exponentiating the log-odds:\n",
    "\n",
    "Change in Odds  = odds(math=53) / odds(math=52)  \n",
    "                = exp(-1.50792) / exp(-1.66426)  \n",
    "                = odds (difference in one-unit increase)  \n",
    "                = exp(0.15634)  \n",
    "                = 1.169224  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code exp of math coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interpret this as: for a one-unit increase in math score, we expect to see ~17% increase in the odds of being in an honors class. This 17% does not depend on the value that math is held at. It's also important to note that a 17% increase in odds is not the same as a 17% increase in probability. All it is saying that compared to a score of 52, scoring 53 will improve the odds of being in an honors class by 1.17 times.\n",
    "\n",
    "### Logistic regression with multiple predictor variables and no interaction terms\n",
    "In general, we can have multiple predictor variables in a logistic regression model:\n",
    "logit(p)        = log(p/(1-p))  \n",
    "                = $\\beta_0 + \\beta1 * x1 + ... + \\beta_k *xk$  \n",
    "                \n",
    "Applying such a model to our example dataset, each estimated coefficient is the expected change in the log odds of being in an honors class **for a one-unit increase in the corresponding predictor variable** holding the other variables constant at a certain value. Each exponentiated coefficient is the ratio of two odds, or the change in odds in the multiplicative scale for a one-unit increase in the corresponding predictor variable holding other variables at a certain value. Let's look at the following equation:\n",
    "\n",
    "$logit(p) = \\beta_0 + \\beta_1 * math + \\beta_2 * female + \\beta_3 * read$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code of glm hon~math+female+read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficient for *math* tells us that, holding *female* and *reading* at a fixed value, we will see a 13% increase in the odds of graduating with honors class for a one-unit increase in math score since exp(.12296) = 1.13. \n",
    "\n",
    "Can you attempt to interpret the above model and answer the following question?\n",
    "\n",
    "- Holding Female and Mathematics score constant, a one-unit increase in reading score improves the odds of graduating with honors by how much?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code exp of reading coeff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Example: Predicting Flight Delay\n",
    "Let's take a look at what happened when we try to predict flight delays using a logistic regression models where the predictor variables are `Month`, `DayofMonth`, and `DayofWeek` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>DayofMonth</th>\n",
       "      <th>DayofWeek</th>\n",
       "      <th>Carrier</th>\n",
       "      <th>CRSDepTime</th>\n",
       "      <th>DepDel15</th>\n",
       "      <th>CRSArrTime</th>\n",
       "      <th>OriginState</th>\n",
       "      <th>DestState</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>DL</td>\n",
       "      <td>1539</td>\n",
       "      <td>0</td>\n",
       "      <td>1824</td>\n",
       "      <td>FL</td>\n",
       "      <td>NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>WN</td>\n",
       "      <td>1400</td>\n",
       "      <td>1</td>\n",
       "      <td>1425</td>\n",
       "      <td>PA</td>\n",
       "      <td>IL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>AS</td>\n",
       "      <td>810</td>\n",
       "      <td>0</td>\n",
       "      <td>1614</td>\n",
       "      <td>WA</td>\n",
       "      <td>DC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>OO</td>\n",
       "      <td>804</td>\n",
       "      <td>0</td>\n",
       "      <td>1027</td>\n",
       "      <td>IL</td>\n",
       "      <td>OH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>DL</td>\n",
       "      <td>805</td>\n",
       "      <td>0</td>\n",
       "      <td>1117</td>\n",
       "      <td>NY</td>\n",
       "      <td>FL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year  Month  DayofMonth  DayofWeek Carrier  CRSDepTime  DepDel15  \\\n",
       "0  2013      9          16          1      DL        1539         0   \n",
       "1  2013      9          23          1      WN        1400         1   \n",
       "2  2013      9           7          6      AS         810         0   \n",
       "3  2013      7          15          1      OO         804         0   \n",
       "4  2013      5          16          4      DL         805         0   \n",
       "\n",
       "   CRSArrTime OriginState DestState  \n",
       "0        1824          FL        NY  \n",
       "1        1425          PA        IL  \n",
       "2        1614          WA        DC  \n",
       "3        1027          IL        OH  \n",
       "4        1117          NY        FL  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight = pd.read_csv(\"data_input/flight_sm.csv\")\n",
    "flight.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code build model glm DepDel15 ~ Month + DayofMonth + DayofWeek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a problem with the above logistic regression model: Can you tell which among the three key assumptions did it violate?\n",
    "- Multicollinearity  \n",
    "- Independence of Observations  \n",
    "- Linearity of predictor and log odds  \n",
    "\n",
    "### Application of Logistic Regression\n",
    "In the field of market research where its commonplace for business analysts to try and get as accurate as possible a prediction of a new product launch (success/failure), a new bundle pricing strategy (odds of success / odds of failure), or a new enrollment plan, logistic regression and its accompanying analysis plays a pivotal role. An example of this is the scenario of a company that is estimating the change of probability / odds of customer buy-in for every $1 dollar change in price. Another example of this is in election forecasts: where a campaign manager is trying to determine the odds of a likely voter to vote for a particular candidate, using demographic parameters such as gender, age, and education level. \n",
    "\n",
    "Another common use of logistic regression in business is in building models of customer retention, which can offer incredible insights into why some customers leave and others stay (drivers of customer retention). This is particular important in certain industries, where reducing customer defections by as little as five percent can double profits (Reichheld, 1996[^1])\n",
    "\n",
    "Interesting weekend read: Another interest project that models customer retention using historical data from a database (more than 500,000 clients) of a big mutual fund investment company and logistic regression (Eiben, Euverman, Kowalczyk, Slisser[^2]), which highlight the benefits of an interpretative model like the one we obtain with logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet another example is in Credit Risk Analysis, where machine learning is deployed to estimate probability of defaults (or in the measurement of other types of credit risk). The paper described how loan officers at bank use logistic regression \"to identify characteristics that are indicative of people who are likely to default on loans, and then use those characteristics to discriminate between good and bad credit risks\"[^4]. \n",
    "\n",
    "A quick summary of the findings:  \n",
    "- Number of years at current employment and number of years at current address have negative coefficients, indicating that customers who have spent less time at either their current employer or their current address are more likely to default  \n",
    "- Debt-to-income ratio (`dti`, a measurement we'll use in our project later) and amount of credit card debt both have positive coefficients, indicating that higher dti ratios or higher amounts of credit card debts are both associated with a greater likelihood of loan defaults.  \n",
    "\n",
    "### Credit Risk Analysis / Modeling: Loans from Q4 2017\n",
    "I've prepared the following data originally made available by [LendingClub](https://www.lendingclub). Some preprocessing steps have been applied to save you from the \"data cleansing\" work. We'll read the data into our workspace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>initial_list_status</th>\n",
       "      <th>purpose</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>dti</th>\n",
       "      <th>verification_status</th>\n",
       "      <th>grade</th>\n",
       "      <th>revol_bal</th>\n",
       "      <th>inq_last_12m</th>\n",
       "      <th>delinq_2yrs</th>\n",
       "      <th>home_ownership</th>\n",
       "      <th>not_paid</th>\n",
       "      <th>log_inc</th>\n",
       "      <th>verified</th>\n",
       "      <th>grdCtoA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>w</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>14.08</td>\n",
       "      <td>675.99</td>\n",
       "      <td>156700.0</td>\n",
       "      <td>19.11</td>\n",
       "      <td>Source Verified</td>\n",
       "      <td>C</td>\n",
       "      <td>21936</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>0</td>\n",
       "      <td>11.962088</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>9.44</td>\n",
       "      <td>480.08</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>19.35</td>\n",
       "      <td>Not Verified</td>\n",
       "      <td>B</td>\n",
       "      <td>5457</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>RENT</td>\n",
       "      <td>1</td>\n",
       "      <td>10.819778</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>w</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>28.72</td>\n",
       "      <td>1010.30</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>65.58</td>\n",
       "      <td>Verified</td>\n",
       "      <td>F</td>\n",
       "      <td>23453</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>OWN</td>\n",
       "      <td>1</td>\n",
       "      <td>10.126631</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>w</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>13.59</td>\n",
       "      <td>484.19</td>\n",
       "      <td>175000.0</td>\n",
       "      <td>12.60</td>\n",
       "      <td>Not Verified</td>\n",
       "      <td>C</td>\n",
       "      <td>31740</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>1</td>\n",
       "      <td>12.072541</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>w</td>\n",
       "      <td>major_purchase</td>\n",
       "      <td>15.05</td>\n",
       "      <td>476.33</td>\n",
       "      <td>109992.0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>Not Verified</td>\n",
       "      <td>C</td>\n",
       "      <td>2284</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>0</td>\n",
       "      <td>11.608163</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  initial_list_status             purpose  int_rate  installment  annual_inc  \\\n",
       "0                   w  debt_consolidation     14.08       675.99    156700.0   \n",
       "1                   f  debt_consolidation      9.44       480.08     50000.0   \n",
       "2                   w  debt_consolidation     28.72      1010.30     25000.0   \n",
       "3                   w  debt_consolidation     13.59       484.19    175000.0   \n",
       "4                   w      major_purchase     15.05       476.33    109992.0   \n",
       "\n",
       "     dti verification_status grade  revol_bal  inq_last_12m  delinq_2yrs  \\\n",
       "0  19.11     Source Verified     C      21936             3            0   \n",
       "1  19.35        Not Verified     B       5457             1            1   \n",
       "2  65.58            Verified     F      23453             0            0   \n",
       "3  12.60        Not Verified     C      31740             0            0   \n",
       "4  10.00        Not Verified     C       2284             3            0   \n",
       "\n",
       "  home_ownership  not_paid    log_inc  verified  grdCtoA  \n",
       "0       MORTGAGE         0  11.962088         1        0  \n",
       "1           RENT         1  10.819778         0        1  \n",
       "2            OWN         1  10.126631         1        0  \n",
       "3       MORTGAGE         1  12.072541         0        0  \n",
       "4       MORTGAGE         0  11.608163         0        0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loan = pd.read_csv(\"data_input/loan2017Q4.csv\")\n",
    "loan.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable of interest is the `not_paid` variable, a binary variable that indicate whether a loan is fully paid or not. A loan is considered \"not paid\" (not paid = 1) when it is **Defaulted**, **Charged Off**, or past due date (**Grace Period**). To prevent one class from dominating the other, the data I've prepared here over-sampled more \"bad\" loans so that the underlying characteristics of the empirically minority class is adequately represented.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code table of not paid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's important to note is that logistic regression is not susceptible to a \"class imbalance\" problem per-se, and an unbalanced class representation is for the most part dealt with as sample size grows anyway. That said, in the situation of highly imbalanced class representation, the patterns within the minority class may not be sufficiently \"described\" and in the case of an extreme imbalance you may be better off using an \"anomaly detection\" approach than through a classification approach.\n",
    "\n",
    "In the Unsupervised Machine Learning workshop within the Machine Learning Specialization, I will delve into the specific details of anomaly detection algorithms with far greater depth so let's stay on track and study the dataset we've just read into our environment:  \n",
    "- `initial_list_status`: Either `w` (whole) or `f` (fractional). This variable indicates if the loan was a whole loan or fractional loan. For background: Some institutional investors have a preference to purchase loans in their entirety to obtain legal and accounting treatment specific to their situation - with the added benefit of \"instant funding\" to borrowers  \n",
    "- `purpose`: Simplified from the original data; One of: `credit_card`, `debt_consolidation`, `home_improvement`, `major_purchase` and `small_business`  \n",
    "- `int_rate`: Interest rate in percentages  \n",
    "- `installment`: Monthly payment owed by the borrower  \n",
    "- `annual_inc`: Self-reported annual income provided by the borrower / co-borrowers during application  \n",
    "- `dti`: A ratio of the borrower's total monthly debt payments on his/her total obligations to the self-reported monthly income  \n",
    "- `verification_status`: is the reported income verified, not verified, or if the income source was verified  \n",
    "- `grade`: software-assigned loan grade  \n",
    "- `revol_bal`: total credit revolving balance (in the case of credit card, it refers to the portion of credit card spending that goes unpaid at the end of a billing cycle)  \n",
    "- `inq_last_12m`: number of credit inquiries in the last 12 months  \n",
    "- `delinq_2yrs`: number of 30+ days past-due incidences of delinquency in the borrower's credit file for the past 2 years  \n",
    "- `home_ownership`: one of `MORTGAGE`, `OWN` and `RENT`  \n",
    "- `not_paid`: 1 for fully-paid loans, 0 for charged-off, past-due / grace period or defaulted  \n",
    "- `log_inc`: log of `annual_inc`  \n",
    "- `verified`: 0 for \"Not verified\" under `verification_status`, 1 otherwise  \n",
    "- `grdCtoA`: 1 for a `grade` of A, B or C, 0 otherwise\n",
    "\n",
    "Before we dive into building our classification model, I'd like to encourage you to spend some time on the \"exploratory phase\". This is the phase where you investigate the relationships and discover rough structures of the data. You can use `summary()`, or `fivenum()`, or even `cor()` - take your time to write a few more lines of code below this chunk and be curious about your data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code of data exploratory of loan data between not_paid column with dti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation and Out-of-Sample Error\n",
    "Before we develop our classification model, I'll introduce you to the idea of estimating the accuracy of our model. Simply put, we are going to:  \n",
    "- Split our dataset into train and test sets  \n",
    "- Build our machine learning model using data **only** from our train set  \n",
    "- Obtain an unbiased measurement of the model's accuracy by predicting on test set  \n",
    "\n",
    "A related idea is known as **cross-validation**, in which we:  \n",
    "- Split our dataset into train, cross-validation, and test sets  \n",
    "- Develop the initial model using our train set  \n",
    "- Evaluate model on cross-validation set(s), returning to the previous step if necessary (say, pick different predictor variables, use a different parameter, or to tune other aspects of the model specification)  \n",
    "- Pick a final model based on an evaluation criteria (Adj.R-squared, accuracy, etc)  \n",
    "- Obtain an unbiased measurement of the model's accuracy by predicting on test set  \n",
    "\n",
    "We can repeat step(2) and step(3) as much as is necessary, testing out different algorithms or model specification, or combinations of predictor variables and pick a final model on which we will obtain our estimated accuracy by testing it on the test set. An important rule on this is that the **test set must not be used in any of the steps before the (5)**, such that the accuracy we obtain is an unbiased measurement of the out-of-sample accuracy of the model. \n",
    "\n",
    "The idea of obtaining an unbiased estimate of our model's out-of-sample performance is an important one as it is often the case that the in-sample error (the error you obtain from running your algorithm on the dataset it was trained on) is optimistic and tuned / adapted in a particular way to minimize the error in the training sample. Therefore - the in-sample error is not a good representation or indication of how our model will perform when it is applied on unseen data. \n",
    "\n",
    "Another way to think about is that our training data has two components to it: signal and noise. The goal of machine learning is to identify the signal but be robust enough to avoid modeling the \"noise\" component of the data. When we build a model, we want to know that our model is not overly adapted to the data set to the point that it captures both the signal and noise, a phenomenon known as \"overfitting\". When our model is guilty of overfitting, the in-sample accuracy will be very high (in some cases ~100%) but fail to perform on unseen data. The idea is to strike the right balance between accuracy (don't underfit) and robustness to noise (don't overfit).  \n",
    "\n",
    "### Predicting Credit Risk from Loans\n",
    "Applying what we've learned above, we'll split our data into the `loans.train` and `loans.test` set. I'll first show you this approach and later show you the cross-validation approach - I encourage you to follow this part of the workshop closely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code cross validation of loan data to be train and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already know how to build a binomial logistic regression and learned the \"manual\" way of obtaining those coefficients in previous sections. Here we'll cut to the chase and use `LogisticRegression()` for our model construction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code of glm not_paid ~ verified + purpose + installment + int_rate + home_ownership + grdCtoA + annual_inc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code of summary model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe from the model summary that holding other variables constant, obtaining an assigned grade of A to C reduce the log-odds (because it's a negative coefficient) of a loan default; Now let's use the `predict()` function, specifying the:  \n",
    "- Model to be used for prediction (`creditrisk`)  \n",
    "- Dataset on which the model should predict (`loans.test`)\n",
    "- A response type. The default `link` is on the scale of the linear predictors (log-odds) but we'll specify `response` so the prediction is on the scale of the response variable (which means: probabilities). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code of predict value from model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a look at the `pred.Risk` variable we appended to our `loans.test` dataframe: we can therefore set a \"risk\" threshold, say, at 0.5 and predict any loans that exceed that threshold as a \"default=1\". 0.5 may not always be the right threshold setting and we'll discuss that later in the section describing \"precision\" vs \"recall\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code of loans.test[1:10, 15:17]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Prediction Output\n",
    "As an exercise, are you able to append yet another variable (column) to the above dataframe. Name it `pred.not_paid` and make sure it's a binary (0 or 1). You can use `ifelse` for this task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code of if else condition when pred.Risk > 0.5 will classify in 1 else 0\n",
    "# code cross tabulation between predicted and actual data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table above is also known as the **confusion matrix**. \n",
    "\n",
    "Observe from the confusion matrix that: \n",
    "- Out of the 151 actual defaults we classified 97 of them correctly  \n",
    "- Out of the 161 fully-paid loans we classified 93 of them correctly  \n",
    "- Out of the 312 cases of loans in our test set, we classified 190 of them correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Classifiers: Sensitivity, Specificity and Precision\n",
    "Sensitivity and specificity are metrics commonly used to measures the performance of a binary classification.  \n",
    "\n",
    "- Sensitivity (also called the true positive rate, the **recall**, or probability of detection in some fields) measures the proportion of positives that are correctly identified as such (cancer cell detection, email spam, insurance fraud etc)  \n",
    "- Specificity (also called the true negative rate) measures the proportion of negatives that are correctly identified as such (e.g. the percentage of healthy people who are correctly identified as not having the condition, legitimate emails identified as such, legitimate insurance claims)  \n",
    "- Precision: Proportion of correctly identified positives from all classified as such  \n",
    "- Accuracy: Proportion of correctly identified cases from all cases \n",
    "\n",
    "![Source: Wikipedia](assets/sensitivity.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the confusion matrix, can you describe the precision, recall, and accuracy of our model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to find sensitivity, specificity, precision, and accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, you'll also find machine learning applications that uses the notion of a baseline measure in their model evaluation phase. The baseline performance is used to quantify the improvement of an applied solution to the problem and a **base rate** is just the accuracy of trivially predicting the most-frequent (or majority) class. An implementation of this is the ZeroR Classifier found in many data mining applications or related domains: since it ignores all predictors, ZeroR ends us classifying according to the prior probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code of prop table of not paid in test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our baseline rate is 0.51 - a classifier that does no better than 0.51 is not useful because we might as well have classify every class to the majority! \n",
    "\n",
    "False negatives and false positives are rarely equally costly to a business (or really, to any domain). For an insurance company, a false negative on an insurance payout is likely to cost the company more than a false positive for example. Finding the right precision-recall tradeoff comes with domain expertise - and let's make all of these more concrete by extending our credit risk example above.\n",
    "\n",
    "Say the bank's credit department would rather sacrifice some level of specificity or precision in favor of higher recall (or sensitivity). In simpler words, we want to be more sensitive to \"loan defaults\", how would you go about doing that? Try and think critically of the problem before scrolling down to the proposed solution.\n",
    "\n",
    "Well, one thing we can do is to set the threshold to be more sensitive to \"positive cases\": Let's see what happen if we were to predict a \"default\" when the probability exceed 0.4 (20% more sensitive than our previous classifier): \n",
    "\n",
    "```{r}\n",
    "table(\"predicted\"=as.numeric(loans.test$pred.Risk>=0.4), \"actual\"=loans.test$not_paid)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code fot table confusion matrix when pred.Risk >= 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We increased our Sensitivity or Recall rate from 0.64 to above 0.85! What is the cost of such an adjustment?\n",
    "\n",
    "### Performance evaluation and model selection\n",
    "On top of what we've learned so far, there are other tools we can use to evaluate and compare between the performances of our regression models:\n",
    "\n",
    "**1. AIC (Akaike Information Criteria)**  \n",
    "Like R-squared, AIC is a statistical measure of how close the data are to the fitted regression line. It gives us a measure of fit: we'll therefore choose the model with the lowest AIC value, as it helps us minimize residual error in our model:\n",
    "\n",
    "However, unlike adjusted R-squared, the number itself is not meaningful. If you have more than one similar candidate models (where all of the variables of the simpler model occur in the more complex models), then you should select the model that has the smallest AIC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for find AIC from model1 to model4 in honor data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Null Deviance and Residual Deviance** \n",
    "Null deviance indicates how well the response variable is predicted by a model that includes only the intercept (grand mean). The residual deviance shows how well the response variable is predicted by the model when all predictors are included.\n",
    "\n",
    "Intuitively, this means our first model - with no predictor variables **`glm(hon ~ 1)`** should see the same null deviance and residual deviance, and it is (table below)! As we add more / subtract predictor variables, notice how our residual deviance change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to find null deviance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice in the last model `honors.logit5`, we obtain an unrealistically small residual deviance and coefficients that are confusingly large. This is indicative of the \"Hauck Donner effect\": This is when the fitted probabilities are extremely close to zero or one. Consider a medical diagnosis problem with thousands of cases and around 50 binary explanatory variable (which may arise from coding fewer categorical variables); one of these indicators is rarely true but always indicates that the disease is present. Then the fitted probabilities of cases with that indicator should be one, which can only be achieved by taking $\\beta_i$ = Inf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code of exp coeff honor.logit[5]\n",
    "# Infinity minus any arbitarily large numbers still exceed 0.5\n",
    "# Inf-1000000 > 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More intuition:**\n",
    "This phenomenon where one or more predictors take on a coefficient value of infinity is sometimes referred to as _perfect separation_. Another example: imagine the scenario where reading >= 43 will perfectly predict honors=TRUE and reading < 43 will perfectly predict honors=FALSE, or where stock_opening >= 34 will perfectly predict end_high = TRUE and vice versa, then we can imagine that the probabilities where such cases do happen must be 1, and that is achieved by setting the coefficient to infinity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbour Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The k-nearest neighbor algorithm gets it name from the fact that it uses information about an example's k-nearest neighbors to classify unlabeled examples. Upon choosing _k_, the algorithm requires a training dataset made up of examples that have been classified into several categories, as labeled by a nominal variable. Then, for each unlabeled record in the test dataset, k-NN identifies _k_ records in the training data that are the \"nearest\" in similarity. The unlabeled test instance is assigned the class of the majority of the k-nearest neighbors.  \n",
    "\n",
    "Supposed we pick k=1, then the * in the following feature space will be assigned the square class, but if k=5, then the majority class of the five nearest point will be assigned to that point and our point will be classified as a round instead.\n",
    "\n",
    "![ ](assets/knn.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivational Example: Is Tomato a fruit?\n",
    "Suppose that prior to a blind tasting (or blind dining) experience, we created a dataset in which we recorded our impressions of a number of ingredients. For each ingredient, we rated its `sweetness` and `crunchiness` and then labeled them as one of the three types of food: *fruits*, *vegetables*, or *proteins*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code of create data set of food\n",
    "# food <- data.frame(list(c(\"apple\", \"bacon\", \"banana\", \"carrot\",\"celery\", \"cheese\",\"cucumber\", \"fish\", \"grape\", \"green bean\", \"lettuce\", \"nuts\", \"pear\", \"shrimp\",\"orange\"), c(10,1,10,6,3,1,2,3,10,3,1,3,10,2,9), c(9,4,1,10,10,1,8,2,5,7,10,5,7,2,3), c(\"fruit\", \"protein\", \"fruit\", \"vegetable\", \"vegetable\", \"protein\", \"vegetable\", \"protein\", \"fruit\", \"vegetable\", \"vegetable\", \"proteins\", \"fruit\",\"protein\", \"fruit\")))\n",
    "\n",
    "# Give each feature appropriate names\n",
    "# colnames(food)<- c(\"Ingredient\", \"Sweetness\", \"Crunchiness\", \"Type\")\n",
    "# food"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The k-NN algorithm treats the features as coordinates in a multidimensional feature space. As our dataset includes only two features, the feature space is two-dimensional. We can plot two-dimensional data on a scatterplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code of plotting food data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe here that similar types of food tend to be grouped closely together.\n",
    "\n",
    "Supposed we'd like to decide if tomato is a fruit or a vegetable, we would put tomato onto our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to put tomato in visualization\n",
    "# grob = grobTree(textGrob(\"tomato\", x=0.6, y=0.4, hjust=0, gp=gpar(col=\"darkorange\", fontsize=14)))\n",
    "\n",
    "# plot.fruit + annotation_custom(grob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that we would then use a *distance function* to find tomato's nearest neighbors. Traditionally, the k-NN algorithm assumes *Euclidean distance*, which is the shortest direct route (imagine using a ruler to connect two points). While Euclidean distance function is the most widely used distance metric in k-NN, you will sometimes see the Manhattan distance (which is based on the paths a pedestrian would take by walking city blocks) being used instead [^5]. \n",
    "\n",
    "A few academic papers on this literature may also reference the Minkowsky distance function[^6]:  \n",
    "dist_Minkowsky(A,B) = $(\\sum\\limits^m_{i=1} |x_i -y_i|^r)^{\\frac{1}{r}}$  \n",
    "\n",
    "While these distance functions exist, Euclidean distance is far more often seen in industrial applications and is therefore the focus of this chapter. As a side note, Minkowsky distance is typically used with _r_ being 1 or 2, where the former is equivalent to the Manhattan distance while the latter is the Euclidean distance.  \n",
    "\n",
    "### Euclidean Distance  \n",
    "Let A and B be represented by feature vectors A = ($x_1, x_2, …, x_m$) and B = ($y_1, y_2, …, y_m$), where _m_ is the dimensionality of the feature space. To calculate the distance between A and B, the Euclidean Distance formula can be represented as such:\n",
    "\n",
    "dist(A, B) = $\\sqrt{\\sum\\limits^{m}_{i=1}(x_i-y_i)^2}$\n",
    "\n",
    "Applying the above formula on our blind-tasting example, we can calculate the distance between:  \n",
    "- tomato (sweet: 6, crunchy: 4)  \n",
    "- green bean (sweet: 3, crunchy: 7)\n",
    "\n",
    "dist(tomato, greenbean) = `sqrt((6-3)^2 + (4-7)^2))`, which is 4.24\n",
    "\n",
    "Similarly, we can calculate the distance between the tomato and several of its closest neighbors. Supposed we've done that and choose to assign tomato the food type of its nearest neighbor, which in our case is the orange (distance: 1.4), we are doing what is formally a 1-NN classification. Under 1-NN then the orange would be classified as a fruit.\n",
    "\n",
    "Had we use the k-NN with 3 nearest neighbor instead: orange, grape, and nuts, the majority vote (2 fruits vs 1 protein) would again classify the tomato as a fruit. \n",
    "\n",
    "### Choosing an appropriate *k*\n",
    "The decision of how many neighbors to use for k-NN determines how well the model will generalize to future data. The balance between overfitting and underfitting the training data is a problem known as **bias-variance tradeoff**. Choosing a large k reduces the impact or variance caused by noisy data, but can bias the learner so that it runs the risk of ignoring small, but important patterns.\n",
    "\n",
    "If we use a very large *k*, say, a *k* value as large as the total number of observations in the training data, this would lead to the always predicting the majority class (ZeroR classifier), which we've learned about in the previous chapter.\n",
    "\n",
    "On the opposite extreme, using a single nearest neighbor allows the noisy data or outliers to unduly influence the classification of examples. If one of our training examples were accidentally mislabeled and happens to be a neighboring data point, choosing a k=1 will have resulted in a misclassification, even if the nine other nearest neighbors would have voted differently.\n",
    "\n",
    "In practice, one common strategy is to begin with *k* equal to the square root of the number of training examples. Another strategy is to choose a larger k but apply a weighted voting process in which the vote of the closer neighbors is considered more authoritative than the vote of the farther away neighbors.\n",
    "\n",
    "### Features rescaling\n",
    "Supposed, in addition to Sweetness and Crunchiness, we add a new feature \"Spiciness\" which is measured on a scale of 0 to 10,000. This range, or difference in scale, will allow the spice level of a food to have an amplified impact on the distance function. In fact, it's enlarged contribution to the distance function may end up being the singular decisive feature! \n",
    "\n",
    "We solve this by rescaling the features, i.e shrinking or expanding their range so that each feature's contribution to the distance formula is equally weighed. We want spiciness to be measured on the same scale as sweetness and crunchiness, which is a scale from 1 to 10. The two methods of rescaling features are: \n",
    "\n",
    "- Mix-Max normalization  \n",
    "- z-score standardization  \n",
    "\n",
    "**Min-max normalization** works by transforming a feature such that its values fall into a range of 0 to 1. \n",
    "\n",
    "The formula: $x_{new}$ = `(x-min(x)) / (max(x) - min(x))`  \n",
    "\n",
    "- Which essentially subtracts the min of feature *x* from each value and divides by the range of *x*.\n",
    "\n",
    "Normalized feature's values effectively communicates how far, in percentage terms, the original value fell along the range of all values of feature *x*.\n",
    "\n",
    "**z-score standardization** on the other hand subtracts the mean value of feature *x* and divides the outcome by the standard deviation of *x*.  \n",
    "\n",
    "The formula: $x_{new}$ = `(x-mean(x))/sd(x)`  \n",
    "\n",
    "Standardization rescales each of the feature's values in terms of how many standard deviations they fall above or below the mean values. The resulting value is called a *z-score*. Z-scores has no predefined bounds (minimum and maximum) and may be negative or positive numbers. A more detailed discussion of this is in the Practical Statistics coursebook you have received in an earlier workshop.\n",
    "\n",
    "### Characteristics of k-NN\n",
    "Classification methods using k-NN are called 'lazy learners'. Lazy learners do not build a model; There is no abstraction or generalization process -- compare this to the logistic regression method we've learned earlier to have an intuition of what 'building a model' means. More technically, we say that no 'parameters' are learned about the data.\n",
    "\n",
    "Let's summarize the process that goes into prediction with a k-NN classifier:  \n",
    "- Scaling (putting the variables on a same scale to avoid one variable overpowering the others)  \n",
    "- Select a positive integer *k*  \n",
    "- Select the _k_ nearest neighbor for each \"test\" sample  \n",
    "- Classify based on majority class\n",
    "\n",
    "Because k-NN makes prediction in a manner that is \"just-in-time\" by calculating the similarity between each input sample and the other training samples in the vector space, this method may be computationally expensive on dataset with high dimensionality (high memory requirement and constantly calculating \"distances\" over and over again). If we pick a small *k* value, our algorithm may also be vulnerable to the \"noise\" in our data. On its own, it is also sensitive to the \"scale\" of our data. \n",
    "\n",
    "Despite the limitations, k-NN is incredibly powerful and versatile. In fact some of its weaknesses (such as the outlier and scales) can be adequately mitigated with the scaling strategy we've learned in the earlier section. It is also generally insensitive to outlier and noise when an appropriate *k* value is picked. Unlike logistic regression or linear regression, it works well on non-linear data because k-NN does not make assumption about the data.  \n",
    "\n",
    "Under specific settings and requirements, k-NN is some of the most extensively used algorithms and have impressive accuracy. \n",
    "\n",
    "An example of Nearest Neighbor being used in performance benchmarking by the Microsoft's Kinect team:\n",
    "![Real-Time Human Pose Recognition in Parts from Single Depth Images](assets/kinect.png)\n",
    "Read: http://research.microsoft.com/pubs/145347/BodyPartRecognition.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a k-NN from scratch: Classifying customers by industry segment  \n",
    "Both in the regression models class and in our logistic regression classes, we've learned how to obtain the coefficients and constructing the model manually (from mathematical principles / without the use of \"libraries\"). In this section, I'd like to demonstrate how we can also develop our own classifier from the mathematical principles behind the k-NN algorithm. \n",
    "\n",
    "Imagine you're employed at a particular conglomerate distributing FMCG goods through a distribution network consisting of hotel, restaurant, cafes, and all variety of retail outlets. Our CRM system collected the annual spending in each of the product category for each of the customer, and we'd like to build an algorithm that automatically sort our customers into one of two segments:  \n",
    "- Horeca: Short for Hotel, Restaurant and Cafe  \n",
    "- Retail: Retail industry  \n",
    "\n",
    "You are provided some training datasets as part of the task. We would borrow from a dataset prepared by Margarida Cardoso and [available on the UCI Machine Learning repository](https://archive.ics.uci.edu/ml/datasets/Wholesale+customers):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel</th>\n",
       "      <th>Region</th>\n",
       "      <th>Fresh</th>\n",
       "      <th>Milk</th>\n",
       "      <th>Grocery</th>\n",
       "      <th>Frozen</th>\n",
       "      <th>Detergents_Paper</th>\n",
       "      <th>Delicassen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>12669</td>\n",
       "      <td>9656</td>\n",
       "      <td>7561</td>\n",
       "      <td>214</td>\n",
       "      <td>2674</td>\n",
       "      <td>1338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7057</td>\n",
       "      <td>9810</td>\n",
       "      <td>9568</td>\n",
       "      <td>1762</td>\n",
       "      <td>3293</td>\n",
       "      <td>1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6353</td>\n",
       "      <td>8808</td>\n",
       "      <td>7684</td>\n",
       "      <td>2405</td>\n",
       "      <td>3516</td>\n",
       "      <td>7844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>13265</td>\n",
       "      <td>1196</td>\n",
       "      <td>4221</td>\n",
       "      <td>6404</td>\n",
       "      <td>507</td>\n",
       "      <td>1788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>22615</td>\n",
       "      <td>5410</td>\n",
       "      <td>7198</td>\n",
       "      <td>3915</td>\n",
       "      <td>1777</td>\n",
       "      <td>5185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Channel  Region  Fresh  Milk  Grocery  Frozen  Detergents_Paper  Delicassen\n",
       "0        2       3  12669  9656     7561     214              2674        1338\n",
       "1        2       3   7057  9810     9568    1762              3293        1776\n",
       "2        2       3   6353  8808     7684    2405              3516        7844\n",
       "3        1       3  13265  1196     4221    6404               507        1788\n",
       "4        2       3  22615  5410     7198    3915              1777        5185"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wholesale = pd.read_csv(\"data_input/wholesale.csv\")\n",
    "wholesale.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert the 'Channel' make it a factor and change the labels of the levels into \"horeca\" and \"retail\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code change label of levels in channel column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice here that, unlike the credit risk analysis example, we do not have a balanced dataset. The prior or baseline accuracy for predicting the majority class would be 67.7%. \n",
    "\n",
    "Normalization to z-score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to scale predictor variable\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the dataset into train and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for cross validation\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to classify our target variable, we use a k-NN implementation from the `sklearn.neighbors` library. The `KNeighborsClassifier` function in the sklearn.neighbors library will go through each observation in our `wholesale_train` dataset, and identify the k-Nearest neighbors using Euclidean distance. Each test instance is then assigned the class of the majority of the neighbors - a tie vote is broken at random.\n",
    "\n",
    "\n",
    "We use `k=19` because it's the closest whole number to the square root of our 352, the number of our training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# code for build knn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From those model we can get our prediction to predict the `wholesale_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for predict wholesale_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the model performance, we can see our confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "# code for confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn-by-building Module  \n",
    "As this is a graded task for our students, completion of the task is not optional and count towards your final score.\n",
    "\n",
    "Applying what you’ve learned, present a simple Markdown document in which you demonstrate the use of logistic regression on the `lbb_loans.csv` dataset for a credit risk case or `wholesale.csv` dataset for customer segment prediction case. Explain your findings wherever necessary and show the necessary data preparation steps. To help you through the exercise, consider the following questions throughout the document:  \n",
    "\n",
    "- If you use a logistic regression, how do we correctly interpret the negative coefficients obtained from your logistic regression?  \n",
    "- How do we know which of the variables are more statistically significant as predictors in your logistic regression model? \n",
    "- What is your accuracy? Was the logistic regression better than kNN in terms of accuracy? (recall the lesson on obtaining an unbiased estimate of the model's accuracy)  \n",
    "- Was the logistic regression better than our kNN model at explaining which of the variables are good predictors?\n",
    "- What are some strategies to improve your model? \n",
    "- List down 1 disadvantage and 1 strength of each of the approach (kNN and logistic regression)  \n",
    "\n",
    "Students should be awarded the full points if:  \n",
    "1. The preprocessing steps are done, and the student show an understanding of holding out a test / cross validation set for an estimate of the model’s performance on unseen data\n",
    "2. Student document their analysis on how to improve both of their model. For logistic regression, students are expected to write the model / coefficient interpretation of the probability and for K-nn, students are expected to elaborate the process of finding optimum K.\n",
    "3. The model’s performance is sufficiently explained (accuracy may not be the most helpful metric here! Recall about what you’ve learned confusion matrix and its various metrics)  \n",
    "\n",
    "Student should receive 1 point for each of the above requirements, for a total of (3) points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Objectives\n",
    "In this workshop we'll focus our study on a set of widely-used unsupervised learning methods ranging from PCA (Principal Component Analysis), to Clustering, and other pattern discovery approaches where the target variable is not known or defined. Our goal is to develop a solid intuition behind the problem of dimensionality, the mechanism that is at our disposal, and finally solidify our understanding by working on two of the most common real-life business scenarios.  \n",
    "\n",
    "- **Unsupervised Learning Algorithms**\n",
    "- Clustering Methods\n",
    "- k-means  \n",
    "- Combining PCA with k-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning\n",
    "Throughout the Machine Learning Specialization, we've been learning about algorithms that are greatly useful in situations of regression and classification. More generally, we learn to find the parameters for X1, X2 ... Xn to explain or predict a \"target\" response Y. \n",
    "\n",
    "In the case of unsupervised learning, the situation differs in that there is no such a response Y but rather, we're interested in discovering the structure between X1, X2, to Xn - possibly to identify opportunities for dimensionality reduction or for clustering. Some people have likened unsupervised learning to an exploratory process because it is difficult or impossible to know if the model or any formulation is the \"right\" one since we don't have a \"ground truth\" that we use as a measuring stick. Techniques such as cross-validation and AUC do not apply due to the lack of a \"ground truth\" label. \n",
    "\n",
    "With that said, unsupervised learning methods can still be very powerful especially in the field of clustering and dimensionality reduction. In this workshop, we'll take an in-depth look at unsupervised algorithms such as k-means - and see why unsupervised methods such as these are great tools to add to your toolbox."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means Clustering\n",
    "Clustering refers to the practice of finding meaningful ways to group data (or create subgroups) within a dataset - and the resulting groups are usually called clusters. The objective is to have a number of partitions where the observations that fall into each partition are similar to others in that group, while the partitions are distinctive from one another. \n",
    "\n",
    "K-means is a centroid-based clustering algorithm that follows a simple procedure of classifying a given dataset into a pre-determined number of clusters, denoted as \"k\". This procedure is essentially a series of iterations where we:  \n",
    "1. Find cluster centers  \n",
    "2. Compute distances between each point to each cluster centers  \n",
    "3. Assign / re-assign cluster membership  \n",
    "\n",
    "A few technicality: Instead of saying \"cluster centers\", we'll call them \"centroids\"; Also, in the first iteration of the above procedure, because there are clusters in our feature space, we can't yet compute any centroids so in the first \"iteration\" we'll randomly assign our centroids. It turns out, with enough iteration, that the procedure can usually converge at a reasonably well solution, giving us very reasonable k centroids (remember: we define k, just as in the k-NN algorithm we learned) that we can use for clustering task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the image below:   \n",
    "\n",
    "![ ](assets/centroids.png)\n",
    "\n",
    "If we choose _k_ to be 2, these are the steps that a k-means algorithm take in assigning the original data (green dots) to two clusters:  \n",
    "Step (a): Our data on a two-dimensional space  \n",
    "Step (b): Iteration 1 - Randomly initialize our cluster centroids  \n",
    "Step (c): Iteration 1 - Assigning cluster membership based on a distance function  \n",
    "Step (d): Iteration 2 - Move cluster centroids to be at the center of clusters  \n",
    "Step (e): Iteration 2 - Re-Assigning cluster membership based on a distance function  \n",
    "Step (f): Iteration 3 ...\n",
    "\n",
    "We'll get into the mathematical details a bit later; For now, let's take a look at how we can use R's `kmeans()` function to solve a clustering problem in the absence of a target predictor.\n",
    "\n",
    "### Cluster-based Whisky Recommendation\n",
    "The data we'll be reading in is from Dr.Wisehart (University of St. Andrews), and comprise of 86 distilleries that produce malt whiskies. Each of the whiskies were scored between 0-4 under 12 different taste categories including `Body`, `Sweetness`, `Smoky`, `Medicinal`, `Tobacco`, `Honey`, `Nutty`, `Floral` etc. The original motivation also notes that \"by using correlation data it may be possible to provide whisky recommendations based upon an individual's particular preferences\"[]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowID</th>\n",
       "      <th>Distillery</th>\n",
       "      <th>Body</th>\n",
       "      <th>Sweetness</th>\n",
       "      <th>Smoky</th>\n",
       "      <th>Medicinal</th>\n",
       "      <th>Tobacco</th>\n",
       "      <th>Honey</th>\n",
       "      <th>Spicy</th>\n",
       "      <th>Winey</th>\n",
       "      <th>Nutty</th>\n",
       "      <th>Malty</th>\n",
       "      <th>Fruity</th>\n",
       "      <th>Floral</th>\n",
       "      <th>Postcode</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Aberfeldy</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>\\tPH15 2EB</td>\n",
       "      <td>286580</td>\n",
       "      <td>749680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Aberlour</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>\\tAB38 9PJ</td>\n",
       "      <td>326340</td>\n",
       "      <td>842570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>AnCnoc</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>\\tAB5 5LI</td>\n",
       "      <td>352960</td>\n",
       "      <td>839320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Ardbeg</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>\\tPA42 7EB</td>\n",
       "      <td>141560</td>\n",
       "      <td>646220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Ardmore</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>\\tAB54 4NH</td>\n",
       "      <td>355350</td>\n",
       "      <td>829140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowID Distillery  Body  Sweetness  Smoky  Medicinal  Tobacco  Honey  Spicy  \\\n",
       "0      1  Aberfeldy     2          2      2          0        0      2      1   \n",
       "1      2   Aberlour     3          3      1          0        0      4      3   \n",
       "2      3     AnCnoc     1          3      2          0        0      2      0   \n",
       "3      4     Ardbeg     4          1      4          4        0      0      2   \n",
       "4      5    Ardmore     2          2      2          0        0      1      1   \n",
       "\n",
       "   Winey  Nutty  Malty  Fruity  Floral     Postcode   Latitude   Longitude  \n",
       "0      2      2      2       2       2   \\tPH15 2EB     286580      749680  \n",
       "1      2      2      3       3       2   \\tAB38 9PJ     326340      842570  \n",
       "2      0      2      2       3       2    \\tAB5 5LI     352960      839320  \n",
       "3      0      1      2       1       0   \\tPA42 7EB     141560      646220  \n",
       "4      1      2      3       1       1   \\tAB54 4NH     355350      829140  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whiskies = pd.read_csv(\"data_input/whiskies.txt\")\n",
    "whiskies.head()\n",
    "# Distillery column is the name of each whisky\n",
    "# remove RowID, Postcode, Latitude and Longitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll perform scaling, just as we did in the PCA analyses earlier. With this dataset - scaling is rather arbitary and optional because all measurements assume the same range (0 to 4), but with most data this won't be the case. Recall that the `kmeans` procedure compute a distance (typically Euclidean distance) and as we've learned in the k-NN section of this Specialization, failing to scale may cause our model to perform adequately with the algorithm favoring variables on higher scales. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will have to set a seed for reproducibility because in the first iteration, our centroids are randomly picked on the feature space - thus each time we run the `KMeans()` we are bound to get a slightly different result. Given the objective of **minimizing the within-cluster sum of squared** the k-means algorithm is guaranteed to converge but is not guaranteed to a global optima - a point I'll illustrate later through some code experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# uild kmeans with center 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we use `$iter`, we see that k-means take only 3 iterations to converge, stopping at the third iteration: it already identified 4 sufficiently distinct clusters and further iteration wouldn't improve it any further. The objective has been satisfied. The original algorithm by Lloyd uses this as the objective (minimizing the within-cluster sum of squares):  \n",
    "\n",
    "$\\sum\\limits^k_{i=1}\\sum\\limits_{x_j \\in S_i} (x_j - \\mu_i)^2$\n",
    "\n",
    "Where $\\mu_i$ is the mean of all the points in cluster $S_i$\n",
    "\n",
    "Now let's make this whole idea a lot more concrete by working with some simulated data in code.\n",
    "\n",
    "### Dive Deeper: Understanding k-means\n",
    "In the following experiment, we'll observe how the initialization may not converge to the global optima by simulating some data and changing the seed number iteratively. Here's the code to generate some random data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code randomize x1,y1,x2,y2, and save it in a object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting `x` and `y` from a data will yield the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code plot a$x and a$y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By eye-balling the data, one may make a reasonable argument that there can be three clusters. I'm going to run `KMeans()` on the simulated data, specifying 3 so the k-means algorithm would use 3 number of clusters. Later, you may want to change the seed from 50 to 100 so you can get a visual idea of how the initialization of our centroids will lead to a rather different outcome:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code build kmeans clustering using k=3, print centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the kmeans model `a_k`, we'll now plot our clusters (square) and map the color of our points to the assigned clusters from our **`a_k$cluster`**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code plot x and y and adding center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `kmeans` model we obtained (we named it `a_k`) also has methods that output these two figures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to print within ss and tot of within ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we print `a_k` we will see \"between SS / total SS = 67.8%\", this can serve as another indicator for the goodness of fit. When we compute the sum of squared distances of each point to the global sample mean, we get the total sum of squares. That's also computed for us by `kmeans()` and accessible via `$totss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code of tot ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, when we choose to compute one per cluster (we have 3 clusters) instead of using the global sample mean - and then find the sum of squared distances of these three means to the global mean, we get the between sum of squares (`$betweenss`). When we do this, we also multiply the squared distance of each mean to the global mean by the number of data points it represents:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code of between ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the ratio of `betweenss` and `totss` we get 67.8% which is the same as what the output of `a_k` gives us when we print `a_k`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code of between ss devided by totss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We said earlier that this can be taken as a goodness of the clustering model our k-means has found. It can be thought of as the decomposition of deviance in deviance \"between\" and deviance \"within\" - we want a clustering model that has strong properties of internal cohesion and maximal external separation and so the between sum of squares and total sum of squares ratio as close to 1 as possible indicates a good fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that however, we can arbitrarily improve the \"goodness\" of model by just increasing *k*, so the quality as I've mentioned above is purely mathematical and may not reflect the user's requirement. Often times, as with the case of whiskies clustering, we want to consider external information when picking a good value of k.\n",
    "\n",
    "Run the function below and observe that our within sum of squares decrease as we naively increase the number of clusters - what we're looking for is a point where diminishing returns start to kick in (an elbow) and we start to lose substantial gains: we'll use that point as the number of clusters (*k*) for our kmeans model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for make wss plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing k=4, we'll create our kmeans model (`whi_km`) and we will append the cluster to our original data in a variable named `clust`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code build kmeans using k=4\n",
    "# put cluster centers in whiskies data and name the column with `clust`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:**\n",
    "Now assuming a long-time customer of ours reveal that him (and his spouse) enjoy Laphroig the most, what other whiskies can we recommend?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code whiskies `Laphroig`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn-by-building\n",
    "Using any of kmeans algorithms you've learned, produce a simple markdown document where you demonstrate an exercise of either clustering on one of either the `wholesale.csv` or the `nyc` dataset. \n",
    "\n",
    "Explain your choice of parameters (how you choose *k* for k-means clustering)from the original data. What are some business utility for the unsupervised model you've developed? The Markdown document should be not longer than 4 paragraph, and contain one or two visualization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tokopedia] *",
   "language": "python",
   "name": "conda-env-tokopedia-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
