{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coursebook: Classification Model**\n",
    "- Part 5 of Python Fundamental Course\n",
    "- Course Length: 24 Hours\n",
    "- Last Updated: July 2019\n",
    "\n",
    "___\n",
    "\n",
    "- Developed by [Algoritma](https://algorit.ma)'s product division and instructors team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "The coursebook is part of the **Python Fundamentals Course** prepared by [Algoritma](https://algorit.ma). The coursebook is intended for a restricted audience only, i.e. the individuals and organizations having received this coursebook directly from the training organization. It may not be reproduced, distributed, translated or adapted in any form outside these individuals and organizations without permission.\n",
    "\n",
    "Algoritma is a data science education center based in Jakarta. We organize workshops and training programs to help working professionals and students gain mastery in various data science sub-fields: data visualization, machine learning, data modeling, statistical inference etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification in Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Objectives\n",
    "In this workshop, we'll extend our understanding of regression algorithms and see what we've learned in the previous workshop can be extended to solve a different kind of problems: classification problems. More specifically, we'll learn to solve binary and multi-class classification models using machine learning algorithms that are easily understood and in the case of logistic regression, readily interpretable. \n",
    "\n",
    "You will learn to develop classification algorithms from scratch, and investigate the mathematical foundations underpinning logistic regressions and nearest neighbors algorithms. My objective is to deliver a 9-hour session that is packed with the depth to help you develop, apply, score and evaluate two of the most highly versatile algorithms widely used today.\n",
    "\n",
    "- **Logistic Regression**\n",
    "    - Understanding Odds  \n",
    "    - Log of Odds  \n",
    "    - Logistic Regression in Practice  \n",
    "    - Assumption and Limitation\n",
    "- **Nearest Neighbors Prediction**\n",
    "    - Calculating distance  \n",
    "    - KNN from Scratch\n",
    "    - Non parametric model\n",
    "- **Model Evaluation**  \n",
    "    - Cross Validation  \n",
    "    - Bias variance trade-off  \n",
    "    - Confusion matrix  \n",
    "    \n",
    "By the end of this course, you'll be working on a **Learn-by-Building** module to create a data exploratory analysis project to apply what you have learned on provided dataset and attempt to answer all the given questions. This final part is considered as a Graded Assignment so make sure you do well on the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going into our main topic for this coursebook, let's import the packages we are going to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "### Theory\n",
    "Logistic regression is a classification algorithm used to fit a regression curve, $y = f(x)$, where $y$ is a categorical variable. When $y$ is binary (1 for spam, 0 for not-spam) we also call the model **binomial logistic regression** where in cases of $y$ assuming more than 2 values you'll sometimes hear the model being referred to as a class of **multinomial logistic regression**. We can think of logistic regression as a special case of linear regression (which you've mastered in the previous workshop), except we're using **log of odds** as our target variable. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relation to Probability\n",
    "So it's perhaps important to understand what odds mean. Most of us are familiar with **probabilities**. We understood that the **probability** of an event is the proportion of times it will occur divided by the total number of trials. If an event occurs 1 out of 5 times, then the probability (`p`) would be 1 out of 5, or 0.2. \n",
    "\n",
    "Odds are defined as the probability that an event will occur (`p`) divided by the probability than the event will not occur (we'll call it `q`, which is the same as `1-p`). If p is 0.2, we will see that q is 0.8. Expressed in a formula, odds can then be defined as:  \n",
    "$\\frac{p}{(1-p)}$\n",
    "\n",
    "Let's use a fun and real-life example. Supposed we were playing black jack (assuming the casino uses two decks on black jack) and the first card dealt is an Ace, the probability of the next card dealt to the dealer is a Ten is 31.07% (32 possible Tens out of 103 possibility). If we have to express it in odds and define p as 0.31, then our odds of the dealer being dealt a Blackjack (Ace + a Ten) is 0.31/(1-0.31), which brings it to 0.45 to 1. \n",
    "\n",
    "Note that if we have defined `p` as the probability of the Dealer **not having a Blackjack**, our odds would instead be 0.69/0.31, which brings us to 2.23 to 1. We can interpret this as \"for every 2.23 times the dealer didn't get a blackjack, she would get 1 blackjack\". Odds, as we so far understand it, refers to the ratio of favorable event (dealer doesn't get a blackjack) to the unfavorable event. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz 1: Odds of flying on time vs suffering a departure delay**\n",
    "\n",
    "Now to a less-fun but no less important example: airport delays. If I tell you that the probability of a minor departure delay occurring at a particularly busy airport (Soekarno-Hatta) on a festive holiday season is 0.2, what are the chances (expressed in odds) of you departing on time versus that of a departure delay. Recall the formula: \n",
    "\n",
    "$Odds = \\frac{No-delay}{Delay}$\n",
    "\n",
    "I hope you arrived at the right answer of 4 to 1, and intuitively interpret the situation as \"we are 4 times more likely to depart on time than to be delayed\". \n",
    "\n",
    "Odds are rather commonly used in some industry and in sports. In football and in horse racing, you'll often see betting odds expressed as fractions (e.g. 3/1 for a Germany win). In some academic writing or journalistic reporting, you may also see odds being expressed such as this: \"the relative risk of a credit event with Financial Product A over Product B is 1.125\". If you think about it, this is the same concept we've been talking about: odds. \n",
    "\n",
    "If it wasn't immediately clear, consider assigning some numbers to the above example:  \n",
    "- Financial Product A has a 0.45 empirical probability of incurring a credit event  \n",
    "- Financial Product B has a 0.4 empirical probability of incurring a credit event  \n",
    "\n",
    "The odds is hence 0.45/0.4, or 1.125:1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding log of odds\n",
    "When we have a probability $p$, the log of odds (sometimes called the \"log-odds\") is simply the log of the odds ratio, which is:  \n",
    "$log(p/(1-p))$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Odds ratios as we observe above, are just an alternate way of expressing probabilities. Let's say we have the probability of success as 0.8, then the probability of failure is 1 - 0.8 = 0.2. The odds of success are defined as the probability of success over the probability of failure, in our case the odds would be .8/.2 = 4. We can also say that the odds of success is hence 4 to 1. If the probability of success is .5, i.e 50-50, our odds of success is 1 to 1.\n",
    "\n",
    "The transformation from probability to odds is a monotonic transformation, so the odds increases as the probability increase (however note that odds take a range of 0 to infinity):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(0.01, 0.99, 100)\n",
    "y = x/(1-x)\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.ylabel('Odds')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylim(0,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we have an **odd of 1 when our p is 0.5**, and our odd is 4, when p is 0.8, just as we learned from the earlier example (50:50 -> odds of 1, success rate of 0.8 -> odds of 4). \n",
    "\n",
    "Now that we've understood the transformation from probability to odds, let's understand the transformation from odds to logs of odds. \n",
    "\n",
    "Log of odds are:\n",
    "$logit(p) = log(\\frac{p}{1-p})$\n",
    "\n",
    "Almost same code for the above curve, except this time we plot the curve of `log(x/(1-x))` instead of `(x/(1-x))`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5, 5)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(0.01, 0.99, 100)\n",
    "odds = x/(1-x)\n",
    "y = np.log(odds)\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.ylabel('Log of Odds')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylim(-5,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change `x` below from 0.5 to 1, and then to 0 to verify that the log of odds can take any positive or negative value (which is to say, its range is -Inf to Inf). A linear model can produce any value of log of odds and they would be acceptable as a prediction as the range is -Inf to Inf. That is not the case if a linear model has to produce a prediction that is a valid value of \"probability\", because a probability only takes a range of 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 0.5\n",
    "math.log(x/ (1-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the transformation of odds to log of odds is a monotonic one. The greater the odds, the greater the log of odds. However, recall that the probability of .5 will yield us a log-odds of 0. This is because the logit (log of odds) function takes values on [min, max] and transforms them to span [-Inf, Inf]. 5 is our median number and hence it's value on the log of odds scale is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding logit function\n",
    "\n",
    "In the case of a p=0.5 on a scale of 0 to 1, our *p* would then be p = ( 0.5 - 0 ) / (1 - 0) = 0.5; In the case of a p=30 on a scale of 1 to 100, our *p* would subsequently take on the value of (30-1)/(100-1) = 0.292929293.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29292929292929293"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(30-1)/99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.8811993779249543"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.log(0.2929293/(1-0.2929293))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, however, that the logit function puts our probability on the x-axis instead of the y-axis and we can *invert* both axes also called the Sigmoidal **logistic function**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(0.01, 0.99, 100)\n",
    "odds = x/(1-x)\n",
    "y = np.log(odds)\n",
    "\n",
    "plt.plot(y,x)\n",
    "plt.ylabel('f(x)')\n",
    "plt.xlabel('x')\n",
    "plt.ylim(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could be wondering by now why we're concerned with understanding these underlying concepts? It turns out that the reason is surprisingly straightforward if we approach it from our prior knowledge of linear regression models.  \n",
    "\n",
    "Recall that with linear regression, we are used to representing our hypothesis in the following form:\n",
    "\n",
    "$\\hat{y} = \\beta_0 + \\beta_1x_1 + ... + \\beta_mx_m$  \n",
    "Where m is the number of predictors\n",
    "\n",
    "But with that hypothesis, our value $\\hat{y}$ could take on any value from *-Inf* to *Inf*. This is obviously not very helpful for our classification task. Ideally, we want:\n",
    "\n",
    "$0 \\leq \\hat{y} \\leq 1$  \n",
    "\n",
    "This is because we can then set a threshold value, say 0.5, and classify any examples above 0.5 as a \"positive\" and any value below it as a \"negative\". Turns out, we can transform a simple linear regression model $\\hat{y} = \\beta_0 + \\beta_1x_1$ by applying the sigmoid function, also known as the logistic function so we would end up with a hypothesis that bound our value to the range of 0 to 1:\n",
    "$\\hat{y}  = sigmoid( \\beta_0 + \\beta_1x_1)$\n",
    "- where $\\hat{y}$ = estimated probability that y=1 on input x.  \n",
    "\n",
    "More formally:\n",
    "$\\hat{y} = P(y=1 | x;\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional] Extra Proof: Intuition behind the sigmoid function  \n",
    "This sub-chapter sheds light on another perspective behind the sigmoid function, in the hope of helping you make sense of the sigmoid function a little more.\n",
    "\n",
    "Starting from a simple linear regression example with an independent variable called \"Age\" (imagine predicting income based on age), we would have the following hypothesis:\n",
    "$\\hat{y} = \\beta_0 + \\beta_{Age}$\n",
    "\n",
    "In logistic regression, since we are only concerned about the probability of our outcome (target), we need our hypothesis to be between 0 and 1:\n",
    "$0 \\leq \\hat{y} \\leq 1$\n",
    "\n",
    "Recall that we can think of $\\hat{y}$ simply as a probability of y being 1, we can denote it as $p$ for the purpose of convenience. Since probability must always be positive, we put this linear equation in exponential form, such that for any value of slope and dependent variable, exponent of this equation will never be negative:\n",
    "$p = exp(\\beta_0 + \\beta_{Age}) = e^{(\\beta_0 + \\beta_{Age})}$\n",
    "\n",
    "Exponenting something would make it an always positive value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.315287191035679e-07"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.exp(-14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've made the range our $p$ can take on 0 to positive infinity; We still have one task to do - we need to make our probability assume a range smaller than 1, essentially making it take on the range of 0 to 1. To make the probability lesser than 1, we will divide p by a number greater than p. \n",
    "\n",
    "> Divide 4 by 5 and get 0.8; or 4 by 20 and get 0.2, for an arithmetic proof  \n",
    "\n",
    "So, back to making p lesser than 1:  \n",
    "$p = \\frac{exp(\\beta_0 + \\beta_{Age})} {exp(\\beta_0 + \\beta_{Age} + 1) }$\n",
    "\n",
    "The above equation is of course equivalent to:\n",
    "$\\frac{e^{(\\beta_0 + \\beta_{Age})}}{e^{\\beta_0 + \\beta_{Age}+ 1)}}$\n",
    "\n",
    "Putting all of these together, we can now rewrite the probability as:\n",
    "$p = \\frac{e^z}{(1 + e^z)}$\n",
    "\n",
    "Where p is the probability of success (y=1) and `z` is the placeholder for $\\beta_0 + \\beta_{Age}$. `q`, the probability of failure, will then be:\n",
    "$q = (1 - p) = 1 - \\frac {e^z} {(1 + e^z )} $\n",
    "\n",
    "Recalling what we know about *odds*, we can now define our odds as:\n",
    "$\\frac{p}{1-p}$  \n",
    "\n",
    "Let's expand from the above equation:  \n",
    "$\\frac{p}{1-p}$  = $p * \\frac{1}{(1-p)}$  \n",
    "                 = $\\frac{e^z}{1+e^z} * \\frac{1}{1-\\frac{e^z}{1+e^z}}$  \n",
    "                 = $\\frac{e^z}{(1+e^z) - (\\frac{e^z * (1+e^z)}{1+e^z})}$  \n",
    "                 = $\\frac{e^z}{(1+e^z) - e^z}$  \n",
    "                 = $\\frac{e^z}{1}$  \n",
    "\n",
    "So from the above odds equation $\\frac{p}{1-p} = e^z$, we can take the log on both sides and obtain:  \n",
    "$log(\\frac{p}{1-p}) = z$\n",
    "\n",
    "After substituting z for the actual hypothesis in our earlier linear regression example, we arrive at:\n",
    "$log(\\frac{p}{1-p}) = \\beta_0 + \\beta(Age)$\n",
    "\n",
    "This, we learned earlier, is the equation used in logistic regression. It turns out that we arrive at the log of odds which we've studied in the previous section! \n",
    "\n",
    "Another important observation: realize that regardless of what value x takes, our probability of success (y=1) will always be on the range of 0 to 1. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Assumptions of Logistic Regression  \n",
    "Many of the key assumptions of linear regression do not hold true with logistic regression. We've learned about the linearity assumption, normality of residuals, and homoskedasticity assumptions in our regression models class - they do not apply in the case of logistic regression.\n",
    "\n",
    "Logistic regression **does not** require a linear relationship between the dependent and independent variables - it also does not assume normality of residuals nor is it concerned with the problem of heteroskedasticity the way that linear regressions are.\n",
    "\n",
    "However, a few of the assumptions do apply:  \n",
    "- **Multicollinearity**: Just as with the case of linear regression, logistic regression assumes little to no multicollinearity among the independent variables (recall how we used VIF to identify highly correlated variables in the last workshop)  \n",
    "- **Independence of Observations**: The observations should not come from repeated measurements and are independent from each other  \n",
    "- **Linearity of predictor and log odds**: While logistic regressions do not assume linearity between the dependent and independent variables, it does assume that the independent variables (predictors) are linearly related to the log odds.  \n",
    "\n",
    "The first two points are rather self-explanatory, and the third will be illustrated to you in an example later (flight delay prediction). If put slightly differently, the third point stresses that a logistic regression models the logit-transformed probability as a linear relationship with the predictor variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Logistic Regression\n",
    "Supposed you work in an education institution and are put in charge to evaluate the likelihood of a student graduating with a honors degree given their academic scores in a reading test, writing test and mathematics test.  \n",
    "\n",
    "This dataset has four features: `female`, `read`, `write`, `math` and the target variable is `hon`, a binary feature with 1 indicating that the student is in fact in an honors class and 0 indicating otherwise. The dataset is credited to the UCLA: Statistical Consulting Group (see credits for link and details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>female</th>\n",
       "      <th>read</th>\n",
       "      <th>write</th>\n",
       "      <th>math</th>\n",
       "      <th>hon</th>\n",
       "      <th>femalexmath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>52</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>68</td>\n",
       "      <td>59</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>33</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>44</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>52</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   female  read  write  math  hon  femalexmath\n",
       "0       0    57     52    41    0            0\n",
       "1       1    68     59    53    0           53\n",
       "2       0    44     33    54    0            0\n",
       "3       0    63     44    47    0            0\n",
       "4       0    47     52    57    0            0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "honor = pd.read_csv(\"data_input/sample.csv\")\n",
    "honor.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with Discrete Predictor Variables\n",
    "\n",
    "To fully understand logistic regression, let's begin by looking at our `honor` proportion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    151\n",
       "1     49\n",
       "Name: hon, dtype: int64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "honor.hon.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our odds ratio, without the influence of any predictor variable, is 49 out of 200 (49 in honors classes vs 151 not), so that give us a probability of 49/200, p = 0.245. Our odds ratio is therefore 0.245/(1-0.245) = 0.3245033\n",
    "\n",
    "Before we attempt to interpret the parameters estimated from our model above, let's examine the odds ratio of a female being in a honors class as we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>female</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hon</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>74</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "female   0   1\n",
       "hon           \n",
       "0       74  77\n",
       "1       17  32"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(honor.hon, honor.female)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For males: odds of being in honors class = (17/91)/(74/91) = 0.2297297  \n",
    "- For females: odds of being in honors class = (32/109)/(77/109) = 0.4155844  \n",
    "- The ratio of the odds for female vs ratio of the odds for male = .42/.23 = 1.809, which is to say that the odds for female being in an honors class are about 81% more than that of their male counterpart  \n",
    "\n",
    "Let's now add one binary predictor variable, **female** to the model, such that the equation for our model is formally described as:\n",
    "$logit(p) = \\beta_0 + \\beta_1 * female$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.549016\n",
      "         Iterations 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>hon</td>       <th>  No. Observations:  </th>  <td>   200</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>   198</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     1</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Thu, 25 Jul 2019</td> <th>  Pseudo R-squ.:     </th>  <td>0.01394</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>10:26:36</td>     <th>  Log-Likelihood:    </th> <td> -109.80</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -111.36</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>  <td>0.07811</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>female</th>    <td>    0.5928</td> <td>    0.341</td> <td>    1.736</td> <td> 0.083</td> <td>   -0.076</td> <td>    1.262</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>   -1.4709</td> <td>    0.269</td> <td>   -5.469</td> <td> 0.000</td> <td>   -1.998</td> <td>   -0.944</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                    hon   No. Observations:                  200\n",
       "Model:                          Logit   Df Residuals:                      198\n",
       "Method:                           MLE   Df Model:                            1\n",
       "Date:                Thu, 25 Jul 2019   Pseudo R-squ.:                 0.01394\n",
       "Time:                        10:26:36   Log-Likelihood:                -109.80\n",
       "converged:                       True   LL-Null:                       -111.36\n",
       "Covariance Type:            nonrobust   LLR p-value:                   0.07811\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "female         0.5928      0.341      1.736      0.083      -0.076       1.262\n",
       "intercept     -1.4709      0.269     -5.469      0.000      -1.998      -0.944\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "honor['intercept'] = 1\n",
    "\n",
    "logit_model_female = sm.Logit(honor.hon, honor[['female','intercept']]).fit()\n",
    "logit_model_female.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now relate the odds ratio to the output from the logistic regression model with our `female` predictor variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2297186453419476"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.exp(-1.4709)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intercept of **-1.4709** is the log odds for males since male is the reference group (**female** = 0). If we have wanted to confirm this, we can manually calculate this using the odds ratio for the male group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.4709"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(math.log((17/91)/(74/91)),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficient for **female** is the log of odds ratio between the female group and the male group, which can be manually calculated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5928"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odds_males = (17/91)/(74/91)\n",
    "odds_females = (32/109)/(77/109)\n",
    "odds_ratio = odds_females/odds_males\n",
    "\n",
    "round(math.log(odds_ratio),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using what we've learned earlier, we also know how easy it would be for us to calculate the odds ratio from the output of the model's summary: we simply have to exponentiate the coefficient it gives us for female. \n",
    "\n",
    "And if we were to relate this back to the original equation:\n",
    "$logit(p) = \\beta_0 + \\beta_1 * female$\n",
    "\n",
    "- For a male (female = 0): we would substitute the values into the equation and arrive at logit(p) = -1.4709  \n",
    "- For a female (female = 1): we would instead get logit(p) = -1.4709 + (0.5928*1) = -0.8781  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The ratio of the odds for female vs ratio of the odds for male = .42/.23 = 1.809, which is to say that the odds for female being in an honors class are about 81% more than that of their male counterpart  \n",
    "\n",
    "Notice how this is the same answer we derive from our manual calculation even before looking at the output of our logistic regression model. In fact, we could as well have taken the **estimated coefficient** value for `female`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression with one continuous predictor variable\n",
    "Let's try another exercise, this time using the `math` score (continuous variable) such that the equation for our model is formally described as:\n",
    "\n",
    "$logit(p) = \\beta_0 + \\beta_1 * math$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.417683\n",
      "         Iterations 7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>hon</td>       <th>  No. Observations:  </th>  <td>   200</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>   198</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     1</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Thu, 25 Jul 2019</td> <th>  Pseudo R-squ.:     </th>  <td>0.2498</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>10:26:51</td>     <th>  Log-Likelihood:    </th> <td> -83.537</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -111.36</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>8.718e-14</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>math</th>      <td>    0.1563</td> <td>    0.026</td> <td>    6.105</td> <td> 0.000</td> <td>    0.106</td> <td>    0.207</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>   -9.7939</td> <td>    1.482</td> <td>   -6.610</td> <td> 0.000</td> <td>  -12.698</td> <td>   -6.890</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                    hon   No. Observations:                  200\n",
       "Model:                          Logit   Df Residuals:                      198\n",
       "Method:                           MLE   Df Model:                            1\n",
       "Date:                Thu, 25 Jul 2019   Pseudo R-squ.:                  0.2498\n",
       "Time:                        10:26:51   Log-Likelihood:                -83.537\n",
       "converged:                       True   LL-Null:                       -111.36\n",
       "Covariance Type:            nonrobust   LLR p-value:                 8.718e-14\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "math           0.1563      0.026      6.105      0.000       0.106       0.207\n",
       "intercept     -9.7939      1.482     -6.610      0.000     -12.698      -6.890\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_model_math = sm.Logit(honor.hon, honor[['math','intercept']]).fit()\n",
    "logit_model_math.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice in the case of a continuous variable such as the math score, our estimated coefficient for the intercept is the log odds of a student with a math score of zero being in an honors class. If we mentally visualize a plot with both x and y axis, this makes intuitive sense: the intercept points to the value of y **when our x feature = 0**. By taking the exponent of this value, we then know the odds of such student being in an honors class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.579088796666687e-05"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.exp(-9.7939)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These odds are very low, and a peek at the distribution for the variable math will reveal that no one in the sample has a math score lower than 30 (mean of 53 in fact), which tells us that the intercept in this model corresponds to the log odds of being in an honors class when math is at the hypothetical value of zero.\n",
    "\n",
    "How do we interpret the coefficient for math? Recall our equation:\n",
    "\n",
    "$logit(p) = log(p/(1-p)) = \\beta_0 + \\beta_1 * math$\n",
    "\n",
    "With the substituted values:\n",
    "logit(p) = -9.7939 + 0.1563 * math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    200.000000\n",
       "mean      52.645000\n",
       "std        9.368448\n",
       "min       33.000000\n",
       "25%       45.000000\n",
       "50%       52.000000\n",
       "75%       59.000000\n",
       "max       75.000000\n",
       "Name: math, dtype: float64"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "honor['math'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The median of math is 52. Let's assume a `math` value of 52:\n",
    "logit(p) = -9.7939 + 0.1563 * 52 = -1.66426\n",
    "\n",
    "Examine the effect of a one-unit increase in math score, at 53:\n",
    "logit(p) = -9.7939 + 0.1563 * 53 = -1.50792\n",
    "\n",
    "Taking the difference:\n",
    "-1.50792 - (-1.66426) = 0.15634"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15634"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(-1.50792 -(-1.66426),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15634"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(logit_model_math.params['math'], 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and it is! So the coefficient for `math` is in fact the difference in the log odds for one unit of increment in that variable (math score of 53 vs 52). In simpler words, for one-unit increase in the math score, the expected change in log odds is 0.15634.\n",
    "\n",
    "Like the earlier example, we could also translate this change in log odds to the change in odds by exponentiating the log-odds:\n",
    "\n",
    "Change in Odds  = odds(math=53) / odds(math=52)  \n",
    "                = exp(-1.50792) / exp(-1.66426)  \n",
    "                = odds (difference in one-unit increase)  \n",
    "                = exp(0.15634)  \n",
    "                = 1.169224  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1692236715647317"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.exp(-1.50792)/math.exp(-1.66426)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1692240873242836"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.exp(logit_model_math.params['math'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interpret this as: for a one-unit increase in math score, we expect to see ~17% increase in the odds of being in an honors class. This 17% does not depend on the value that math is held at. It's also important to note that a 17% increase in odds is not the same as a 17% increase in probability. All it is saying that compared to a score of 52, scoring 53 will improve the odds of being in an honors class by 1.17 times.\n",
    "\n",
    "### Logistic regression with multiple predictor variables and no interaction terms\n",
    "In general, we can have multiple predictor variables in a logistic regression model:\n",
    "logit(p)        = log(p/(1-p))  \n",
    "                = $\\beta_0 + \\beta1 * x1 + ... + \\beta_k *xk$  \n",
    "                \n",
    "Applying such a model to our example dataset, each estimated coefficient is the expected change in the log odds of being in an honors class **for a one-unit increase in the corresponding predictor variable** holding the other variables constant at a certain value. Each exponentiated coefficient is the ratio of two odds, or the change in odds in the multiplicative scale for a one-unit increase in the corresponding predictor variable holding other variables at a certain value. Let's look at the following equation:\n",
    "\n",
    "$logit(p) = \\beta_0 + \\beta_1 * math + \\beta_2 * female + \\beta_3 * read$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.390424\n",
      "         Iterations 7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>hon</td>       <th>  No. Observations:  </th>  <td>   200</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>   196</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     3</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Thu, 25 Jul 2019</td> <th>  Pseudo R-squ.:     </th>  <td>0.2988</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>10:34:24</td>     <th>  Log-Likelihood:    </th> <td> -78.085</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -111.36</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>2.348e-14</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>female</th>    <td>    0.9799</td> <td>    0.422</td> <td>    2.324</td> <td> 0.020</td> <td>    0.154</td> <td>    1.806</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>math</th>      <td>    0.1230</td> <td>    0.031</td> <td>    3.931</td> <td> 0.000</td> <td>    0.062</td> <td>    0.184</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>read</th>      <td>    0.0591</td> <td>    0.027</td> <td>    2.224</td> <td> 0.026</td> <td>    0.007</td> <td>    0.111</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>  -11.7702</td> <td>    1.711</td> <td>   -6.880</td> <td> 0.000</td> <td>  -15.123</td> <td>   -8.417</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                    hon   No. Observations:                  200\n",
       "Model:                          Logit   Df Residuals:                      196\n",
       "Method:                           MLE   Df Model:                            3\n",
       "Date:                Thu, 25 Jul 2019   Pseudo R-squ.:                  0.2988\n",
       "Time:                        10:34:24   Log-Likelihood:                -78.085\n",
       "converged:                       True   LL-Null:                       -111.36\n",
       "Covariance Type:            nonrobust   LLR p-value:                 2.348e-14\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "female         0.9799      0.422      2.324      0.020       0.154       1.806\n",
       "math           0.1230      0.031      3.931      0.000       0.062       0.184\n",
       "read           0.0591      0.027      2.224      0.026       0.007       0.111\n",
       "intercept    -11.7702      1.711     -6.880      0.000     -15.123      -8.417\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_logit_multi = sm.Logit(honor.hon, honor[['female', 'math', 'read','intercept']]).fit()\n",
    "model_logit_multi.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficient for *math* tells us that, holding *female* and *reading* at a fixed value, we will see a 13% increase in the odds of graduating with honors class for a one-unit increase in math score since exp(.12296) = 1.13. \n",
    "\n",
    "**Discussion:**\n",
    "\n",
    "Can you attempt to interpret the above model and answer the following question?\n",
    "\n",
    "- Holding Female and Mathematics score constant, a one-unit increase in reading score improves the odds of graduating with honors by how much?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Example: Predicting Flight Delay\n",
    "Let's take a look at what happened when we try to predict flight delays using a logistic regression models where the predictor variables are `Month`, `DayofMonth`, and `DayofWeek` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>DayofMonth</th>\n",
       "      <th>DayofWeek</th>\n",
       "      <th>Carrier</th>\n",
       "      <th>CRSDepTime</th>\n",
       "      <th>DepDel15</th>\n",
       "      <th>CRSArrTime</th>\n",
       "      <th>OriginState</th>\n",
       "      <th>DestState</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>DL</td>\n",
       "      <td>1539</td>\n",
       "      <td>0</td>\n",
       "      <td>1824</td>\n",
       "      <td>FL</td>\n",
       "      <td>NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>WN</td>\n",
       "      <td>1400</td>\n",
       "      <td>1</td>\n",
       "      <td>1425</td>\n",
       "      <td>PA</td>\n",
       "      <td>IL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>AS</td>\n",
       "      <td>810</td>\n",
       "      <td>0</td>\n",
       "      <td>1614</td>\n",
       "      <td>WA</td>\n",
       "      <td>DC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>OO</td>\n",
       "      <td>804</td>\n",
       "      <td>0</td>\n",
       "      <td>1027</td>\n",
       "      <td>IL</td>\n",
       "      <td>OH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>DL</td>\n",
       "      <td>805</td>\n",
       "      <td>0</td>\n",
       "      <td>1117</td>\n",
       "      <td>NY</td>\n",
       "      <td>FL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year  Month  DayofMonth  DayofWeek Carrier  CRSDepTime  DepDel15  \\\n",
       "0  2013      9          16          1      DL        1539         0   \n",
       "1  2013      9          23          1      WN        1400         1   \n",
       "2  2013      9           7          6      AS         810         0   \n",
       "3  2013      7          15          1      OO         804         0   \n",
       "4  2013      5          16          4      DL         805         0   \n",
       "\n",
       "   CRSArrTime OriginState DestState  \n",
       "0        1824          FL        NY  \n",
       "1        1425          PA        IL  \n",
       "2        1614          WA        DC  \n",
       "3        1027          IL        OH  \n",
       "4        1117          NY        FL  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight = pd.read_csv(\"data_input/flight_sm.csv\")\n",
    "flight.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.501818\n",
      "         Iterations 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>DepDel15</td>     <th>  No. Observations:  </th>   <td>538363</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td>538359</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>     3</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Thu, 25 Jul 2019</td> <th>  Pseudo R-squ.:     </th>  <td>0.002427</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>10:37:01</td>     <th>  Log-Likelihood:    </th> <td>-2.7016e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td>-2.7082e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>1.116e-284</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Month</th>      <td>   -0.0610</td> <td>    0.002</td> <td>  -35.512</td> <td> 0.000</td> <td>   -0.064</td> <td>   -0.058</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>DayofMonth</th> <td>    0.0026</td> <td>    0.000</td> <td>    6.668</td> <td> 0.000</td> <td>    0.002</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>DayofWeek</th>  <td>   -0.0048</td> <td>    0.002</td> <td>   -2.824</td> <td> 0.005</td> <td>   -0.008</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th>  <td>   -0.9747</td> <td>    0.015</td> <td>  -64.570</td> <td> 0.000</td> <td>   -1.004</td> <td>   -0.945</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:               DepDel15   No. Observations:               538363\n",
       "Model:                          Logit   Df Residuals:                   538359\n",
       "Method:                           MLE   Df Model:                            3\n",
       "Date:                Thu, 25 Jul 2019   Pseudo R-squ.:                0.002427\n",
       "Time:                        10:37:01   Log-Likelihood:            -2.7016e+05\n",
       "converged:                       True   LL-Null:                   -2.7082e+05\n",
       "Covariance Type:            nonrobust   LLR p-value:                1.116e-284\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Month         -0.0610      0.002    -35.512      0.000      -0.064      -0.058\n",
       "DayofMonth     0.0026      0.000      6.668      0.000       0.002       0.003\n",
       "DayofWeek     -0.0048      0.002     -2.824      0.005      -0.008      -0.001\n",
       "intercept     -0.9747      0.015    -64.570      0.000      -1.004      -0.945\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight['intercept'] = 1\n",
    "\n",
    "logit_flight = sm.Logit(flight.DepDel15, flight[['Month', 'DayofMonth', 'DayofWeek', 'intercept']]).fit()\n",
    "logit_flight.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a problem with the above logistic regression model: Can you tell which among the three key assumptions did it violate?\n",
    "- Multicollinearity  \n",
    "- Independence of Observations  \n",
    "- Linearity of predictor and log odds  \n",
    "\n",
    "### Application of Logistic Regression\n",
    "In the field of market research where its commonplace for business analysts to try and get as accurate as possible a prediction of a new product launch (success/failure), a new bundle pricing strategy (odds of success / odds of failure), or a new enrollment plan, logistic regression and its accompanying analysis plays a pivotal role. An example of this is the scenario of a company that is estimating the change of probability / odds of customer buy-in for every $1 dollar change in price. Another example of this is in election forecasts: where a campaign manager is trying to determine the odds of a likely voter to vote for a particular candidate, using demographic parameters such as gender, age, and education level. \n",
    "\n",
    "Another common use of logistic regression in business is in building models of customer retention, which can offer incredible insights into why some customers leave and others stay (drivers of customer retention). This is particular important in certain industries, where reducing customer defections by as little as five percent can double profits (Reichheld, 1996)\n",
    "\n",
    "Interesting weekend read: Another interest project that models customer retention using historical data from a database (more than 500,000 clients) of a big mutual fund investment company and logistic regression (Eiben, Euverman, Kowalczyk, Slisser), which highlight the benefits of an interpretative model like the one we obtain with logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet another example is in Credit Risk Analysis, where machine learning is deployed to estimate probability of defaults (or in the measurement of other types of credit risk). The paper described how loan officers at bank use logistic regression \"to identify characteristics that are indicative of people who are likely to default on loans, and then use those characteristics to discriminate between good and bad credit risks\".\n",
    "\n",
    "A quick summary of the findings:  \n",
    "- Number of years at current employment and number of years at current address have negative coefficients, indicating that customers who have spent less time at either their current employer or their current address are more likely to default  \n",
    "- Debt-to-income ratio (`dti`, a measurement we'll use in our project later) and amount of credit card debt both have positive coefficients, indicating that higher dti ratios or higher amounts of credit card debts are both associated with a greater likelihood of loan defaults.  \n",
    "\n",
    "[Reichheld, F.F. (1996)., Learning from Customer Defections, in Harvard Business Review,march-april](https://hbr.org/1996/03/learning-from-customer-defections)\n",
    "[Modelling Customer Retention with Statistical Techniques, Rough Data Models, and Genetic Programming.](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.55.7177&rep=rep1&type=pdf)\n",
    "[Modelling telecom customer attrition using logistic regression](https://academicjournals.org/article/article1379926496_Oghojafor%20et%20al.pdf)\n",
    "[Credit Risk Analysis Using Logistic Regression Modeling](http://smartdrill.com/pdf/Credit%20Risk%20Analysis.pdf)\n",
    "\n",
    "### Credit Risk Analysis / Modeling: Loans from Q4 2017\n",
    "I've prepared the following data originally made available by [LendingClub](https://www.lendingclub). Some preprocessing steps have been applied to save you from the \"data cleansing\" work. We'll read the data into our workspace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>initial_list_status</th>\n",
       "      <th>purpose</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>dti</th>\n",
       "      <th>verification_status</th>\n",
       "      <th>grade</th>\n",
       "      <th>revol_bal</th>\n",
       "      <th>inq_last_12m</th>\n",
       "      <th>delinq_2yrs</th>\n",
       "      <th>home_ownership</th>\n",
       "      <th>not_paid</th>\n",
       "      <th>log_inc</th>\n",
       "      <th>verified</th>\n",
       "      <th>grdCtoA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>w</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>14.08</td>\n",
       "      <td>675.99</td>\n",
       "      <td>156700.0</td>\n",
       "      <td>19.11</td>\n",
       "      <td>Source Verified</td>\n",
       "      <td>C</td>\n",
       "      <td>21936</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>0</td>\n",
       "      <td>11.962088</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>9.44</td>\n",
       "      <td>480.08</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>19.35</td>\n",
       "      <td>Not Verified</td>\n",
       "      <td>B</td>\n",
       "      <td>5457</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>RENT</td>\n",
       "      <td>1</td>\n",
       "      <td>10.819778</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>w</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>28.72</td>\n",
       "      <td>1010.30</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>65.58</td>\n",
       "      <td>Verified</td>\n",
       "      <td>F</td>\n",
       "      <td>23453</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>OWN</td>\n",
       "      <td>1</td>\n",
       "      <td>10.126631</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>w</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>13.59</td>\n",
       "      <td>484.19</td>\n",
       "      <td>175000.0</td>\n",
       "      <td>12.60</td>\n",
       "      <td>Not Verified</td>\n",
       "      <td>C</td>\n",
       "      <td>31740</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>1</td>\n",
       "      <td>12.072541</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>w</td>\n",
       "      <td>major_purchase</td>\n",
       "      <td>15.05</td>\n",
       "      <td>476.33</td>\n",
       "      <td>109992.0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>Not Verified</td>\n",
       "      <td>C</td>\n",
       "      <td>2284</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>0</td>\n",
       "      <td>11.608163</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  initial_list_status             purpose  int_rate  installment  annual_inc  \\\n",
       "0                   w  debt_consolidation     14.08       675.99    156700.0   \n",
       "1                   f  debt_consolidation      9.44       480.08     50000.0   \n",
       "2                   w  debt_consolidation     28.72      1010.30     25000.0   \n",
       "3                   w  debt_consolidation     13.59       484.19    175000.0   \n",
       "4                   w      major_purchase     15.05       476.33    109992.0   \n",
       "\n",
       "     dti verification_status grade  revol_bal  inq_last_12m  delinq_2yrs  \\\n",
       "0  19.11     Source Verified     C      21936             3            0   \n",
       "1  19.35        Not Verified     B       5457             1            1   \n",
       "2  65.58            Verified     F      23453             0            0   \n",
       "3  12.60        Not Verified     C      31740             0            0   \n",
       "4  10.00        Not Verified     C       2284             3            0   \n",
       "\n",
       "  home_ownership  not_paid    log_inc  verified  grdCtoA  \n",
       "0       MORTGAGE         0  11.962088         1        0  \n",
       "1           RENT         1  10.819778         0        1  \n",
       "2            OWN         1  10.126631         1        0  \n",
       "3       MORTGAGE         1  12.072541         0        0  \n",
       "4       MORTGAGE         0  11.608163         0        0  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loan = pd.read_csv(\"data_input/loan2017Q4.csv\")\n",
    "loan.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable of interest is the `not_paid` variable, a binary variable that indicate whether a loan is fully paid or not. A loan is considered \"not paid\" (not paid = 1) when it is **Defaulted**, **Charged Off**, or past due date (**Grace Period**). To prevent one class from dominating the other, the data I've prepared here over-sampled more \"bad\" loans so that the underlying characteristics of the empirically minority class is adequately represented.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    778\n",
       "0    778\n",
       "Name: not_paid, dtype: int64"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loan.not_paid.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's important to note is that logistic regression is not susceptible to a \"class imbalance\" problem per-se, and an unbalanced class representation is for the most part dealt with as sample size grows anyway. That said, in the situation of highly imbalanced class representation, the patterns within the minority class may not be sufficiently \"described\" and in the case of an extreme imbalance you may be better off using an \"anomaly detection\" approach than through a classification approach.\n",
    "\n",
    "Let's study the dataset we've just read into our environment:  \n",
    "- `initial_list_status`: Either `w` (whole) or `f` (fractional). This variable indicates if the loan was a whole loan or fractional loan. For background: Some institutional investors have a preference to purchase loans in their entirety to obtain legal and accounting treatment specific to their situation - with the added benefit of \"instant funding\" to borrowers  \n",
    "- `purpose`: Simplified from the original data; One of: `credit_card`, `debt_consolidation`, `home_improvement`, `major_purchase` and `small_business`  \n",
    "- `int_rate`: Interest rate in percentages  \n",
    "- `installment`: Monthly payment owed by the borrower  \n",
    "- `annual_inc`: Self-reported annual income provided by the borrower / co-borrowers during application  \n",
    "- `dti`: A ratio of the borrower's total monthly debt payments on his/her total obligations to the self-reported monthly income  \n",
    "- `verification_status`: is the reported income verified, not verified, or if the income source was verified  \n",
    "- `grade`: software-assigned loan grade  \n",
    "- `revol_bal`: total credit revolving balance (in the case of credit card, it refers to the portion of credit card spending that goes unpaid at the end of a billing cycle)  \n",
    "- `inq_last_12m`: number of credit inquiries in the last 12 months  \n",
    "- `delinq_2yrs`: number of 30+ days past-due incidences of delinquency in the borrower's credit file for the past 2 years  \n",
    "- `home_ownership`: one of `MORTGAGE`, `OWN` and `RENT`  \n",
    "- `not_paid`: 1 for fully-paid loans, 0 for charged-off, past-due / grace period or defaulted  \n",
    "- `log_inc`: log of `annual_inc`  \n",
    "- `verified`: 0 for \"Not verified\" under `verification_status`, 1 otherwise  \n",
    "- `grdCtoA`: 1 for a `grade` of A, B or C, 0 otherwise\n",
    "\n",
    "Before we dive into building our classification model, I'd like to encourage you to spend some time on the \"exploratory phase\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dive Deeper:**\n",
    "\n",
    "This is the phase where you investigate the relationships and discover rough structures of the data. You can use pivot tables, histogram, and any of the techniques we have learned in the previous weeks to get a better sense about your data. Take your time to write a few more lines and be always be curious about your data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation and Out-of-Sample Error\n",
    "Before we develop our classification model, I'll introduce you to the idea of estimating the accuracy of our model. Simply put, we are going to:  \n",
    "- Split our dataset into train and test sets  \n",
    "- Build our machine learning model using data **only** from our train set  \n",
    "- Obtain an unbiased measurement of the model's accuracy by predicting on test set  \n",
    "\n",
    "A related idea is known as **cross-validation**, in which we:  \n",
    "- Split our dataset into train, cross-validation, and test sets  \n",
    "- Develop the initial model using our train set  \n",
    "- Evaluate model on cross-validation set(s), returning to the previous step if necessary (say, pick different predictor variables, use a different parameter, or to tune other aspects of the model specification)  \n",
    "- Pick a final model based on an evaluation criteria (Adj.R-squared, accuracy, etc)  \n",
    "- Obtain an unbiased measurement of the model's accuracy by predicting on test set  \n",
    "\n",
    "We can repeat step(2) and step(3) as much as is necessary, testing out different algorithms or model specification, or combinations of predictor variables and pick a final model on which we will obtain our estimated accuracy by testing it on the test set. An important rule on this is that the **test set must not be used in any of the steps before the (5)**, such that the accuracy we obtain is an unbiased measurement of the out-of-sample accuracy of the model. \n",
    "\n",
    "The idea of obtaining an unbiased estimate of our model's out-of-sample performance is an important one as it is often the case that the in-sample error (the error you obtain from running your algorithm on the dataset it was trained on) is optimistic and tuned / adapted in a particular way to minimize the error in the training sample. Therefore - the in-sample error is not a good representation or indication of how our model will perform when it is applied on unseen data. \n",
    "\n",
    "Another way to think about is that our training data has two components to it: signal and noise. The goal of machine learning is to identify the signal but be robust enough to avoid modeling the \"noise\" component of the data. When we build a model, we want to know that our model is not overly adapted to the data set to the point that it captures both the signal and noise, a phenomenon known as \"overfitting\". When our model is guilty of overfitting, the in-sample accuracy will be very high (in some cases ~100%) but fail to perform on unseen data. The idea is to strike the right balance between accuracy (don't underfit) and robustness to noise (don't overfit).  \n",
    "\n",
    "To continue let's talk about one more important thing: data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "verified            int64\n",
       "purpose            object\n",
       "installment       float64\n",
       "int_rate          float64\n",
       "home_ownership     object\n",
       "grdCtoA             int64\n",
       "annual_inc        float64\n",
       "not_paid            int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loan_selected = loan[['verified', 'purpose', 'installment', 'int_rate', 'home_ownership', 'grdCtoA', 'annual_inc', 'not_paid']]\n",
    "\n",
    "loan_selected.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, purpose, and home_ownership is stored as an `object` types since both are a categorical value. We need to transform it into a dummy variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = ['purpose', 'home_ownership']\n",
    "\n",
    "loan_dummy = loan_selected.copy()\n",
    "for col in cat_columns:\n",
    "    cat_list = pd.get_dummies(loan_dummy[col], prefix=col)\n",
    "    loan_dummy = loan_dummy.join(cat_list)\n",
    "\n",
    "loan_selected = loan_dummy.select_dtypes(include=['number'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's proceed to splitting our data into a train and test set. To do that we can use `pandas` useful `train_test_split`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1244, 13)\n",
      "(1244,)\n",
      "(312, 13)\n",
      "(312,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = loan_selected.drop(['not_paid'], axis=1)\n",
    "y = loan_selected['not_paid']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Credit Risk from Loans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already know how to build a binomial logistic regression and learned the \"manual\" way of obtaining those coefficients in previous sections. In this section we'll cut into the chase and proceed using `Logit()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.658551\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>not_paid</td>     <th>  No. Observations:  </th>  <td>  1244</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>  1232</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>    11</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Thu, 25 Jul 2019</td> <th>  Pseudo R-squ.:     </th>  <td>0.04977</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>11:23:25</td>     <th>  Log-Likelihood:    </th> <td> -819.24</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -862.14</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>1.097e-13</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "               <td></td>                 <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>verified</th>                   <td>    0.1692</td> <td>    0.125</td> <td>    1.348</td> <td> 0.178</td> <td>   -0.077</td> <td>    0.415</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>installment</th>                <td>    0.0011</td> <td>    0.000</td> <td>    4.935</td> <td> 0.000</td> <td>    0.001</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>int_rate</th>                   <td>    0.0155</td> <td>    0.016</td> <td>    0.985</td> <td> 0.325</td> <td>   -0.015</td> <td>    0.046</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grdCtoA</th>                    <td>   -0.4187</td> <td>    0.181</td> <td>   -2.312</td> <td> 0.021</td> <td>   -0.774</td> <td>   -0.064</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>annual_inc</th>                 <td>-3.485e-06</td> <td> 1.16e-06</td> <td>   -3.001</td> <td> 0.003</td> <td>-5.76e-06</td> <td>-1.21e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>purpose_credit_card</th>        <td>   -0.3850</td> <td> 6.61e+06</td> <td>-5.82e-08</td> <td> 1.000</td> <td> -1.3e+07</td> <td>  1.3e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>purpose_debt_consolidation</th> <td>   -0.1569</td> <td> 6.61e+06</td> <td>-2.37e-08</td> <td> 1.000</td> <td> -1.3e+07</td> <td>  1.3e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>purpose_home_improvement</th>   <td>   -0.1895</td> <td> 6.61e+06</td> <td>-2.87e-08</td> <td> 1.000</td> <td> -1.3e+07</td> <td>  1.3e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>purpose_major_purchase</th>     <td>    0.0350</td> <td> 6.61e+06</td> <td> 5.29e-09</td> <td> 1.000</td> <td> -1.3e+07</td> <td>  1.3e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>purpose_small_business</th>     <td>    0.3203</td> <td> 6.61e+06</td> <td> 4.84e-08</td> <td> 1.000</td> <td> -1.3e+07</td> <td>  1.3e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>home_ownership_MORTGAGE</th>    <td>   -0.3290</td> <td> 6.61e+06</td> <td>-4.97e-08</td> <td> 1.000</td> <td> -1.3e+07</td> <td>  1.3e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>home_ownership_OWN</th>         <td>    0.0742</td> <td> 6.61e+06</td> <td> 1.12e-08</td> <td> 1.000</td> <td> -1.3e+07</td> <td>  1.3e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>home_ownership_RENT</th>        <td>   -0.1214</td> <td> 6.61e+06</td> <td>-1.83e-08</td> <td> 1.000</td> <td> -1.3e+07</td> <td>  1.3e+07</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:               not_paid   No. Observations:                 1244\n",
       "Model:                          Logit   Df Residuals:                     1232\n",
       "Method:                           MLE   Df Model:                           11\n",
       "Date:                Thu, 25 Jul 2019   Pseudo R-squ.:                 0.04977\n",
       "Time:                        11:23:25   Log-Likelihood:                -819.24\n",
       "converged:                       True   LL-Null:                       -862.14\n",
       "Covariance Type:            nonrobust   LLR p-value:                 1.097e-13\n",
       "==============================================================================================\n",
       "                                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "----------------------------------------------------------------------------------------------\n",
       "verified                       0.1692      0.125      1.348      0.178      -0.077       0.415\n",
       "installment                    0.0011      0.000      4.935      0.000       0.001       0.002\n",
       "int_rate                       0.0155      0.016      0.985      0.325      -0.015       0.046\n",
       "grdCtoA                       -0.4187      0.181     -2.312      0.021      -0.774      -0.064\n",
       "annual_inc                 -3.485e-06   1.16e-06     -3.001      0.003   -5.76e-06   -1.21e-06\n",
       "purpose_credit_card           -0.3850   6.61e+06  -5.82e-08      1.000    -1.3e+07     1.3e+07\n",
       "purpose_debt_consolidation    -0.1569   6.61e+06  -2.37e-08      1.000    -1.3e+07     1.3e+07\n",
       "purpose_home_improvement      -0.1895   6.61e+06  -2.87e-08      1.000    -1.3e+07     1.3e+07\n",
       "purpose_major_purchase         0.0350   6.61e+06   5.29e-09      1.000    -1.3e+07     1.3e+07\n",
       "purpose_small_business         0.3203   6.61e+06   4.84e-08      1.000    -1.3e+07     1.3e+07\n",
       "home_ownership_MORTGAGE       -0.3290   6.61e+06  -4.97e-08      1.000    -1.3e+07     1.3e+07\n",
       "home_ownership_OWN             0.0742   6.61e+06   1.12e-08      1.000    -1.3e+07     1.3e+07\n",
       "home_ownership_RENT           -0.1214   6.61e+06  -1.83e-08      1.000    -1.3e+07     1.3e+07\n",
       "==============================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_loan = sm.Logit(y_train, x_train).fit()\n",
    "logit_loan.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe from the model summary that holding other variables constant, obtaining an assigned grade of A to C reduce the log-odds (because it's a negative coefficient) of a loan default; Now let's use the `predict()` function, specifying the:  \n",
    "- Model to be used for prediction (`logit_loan`)  \n",
    "- Dataset on which the model should predict (`x_test`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    312.000000\n",
       "mean       0.491621\n",
       "std        0.126984\n",
       "min        0.091605\n",
       "25%        0.396420\n",
       "50%        0.488220\n",
       "75%        0.582834\n",
       "max        0.826921\n",
       "dtype: float64"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code of predict value from model\n",
    "prediction = logit_loan.predict(x_test)\n",
    "\n",
    "prediction.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction results range from 0 to 1 describing the probability of someone not paying back a loan for each test data. We can therefore set a \"risk\" threshold, say, at 0.5 and predict any loans that exceed that threshold as a \"default=1\". 0.5 may not always be the right threshold setting and we'll discuss that later in the section describing \"precision\" vs \"recall\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>not_paid</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>91</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>74</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "not_paid   0   1\n",
       "row_0           \n",
       "0.0       91  69\n",
       "1.0       74  78"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(round(prediction), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table above is also known as the **confusion matrix**. \n",
    "\n",
    "Observe from the confusion matrix that: \n",
    "- Out of the 147 actual defaults we classified 91 of them correctly  \n",
    "- Out of the 165 fully-paid loans we classified 78 of them correctly  \n",
    "- Out of the 312 cases of loans in our test set, we classified 169 of them correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Prediction Output\n",
    "As an exercise, are you able to append yet another variable (column) to the above `x_test`. Name it `prediction` and make sure it's a binary (0 or 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Classifiers: Sensitivity, Specificity and Precision\n",
    "Sensitivity and specificity are metrics commonly used to measures the performance of a binary classification.  \n",
    "\n",
    "- Sensitivity (also called the true positive rate, the **recall**, or probability of detection in some fields) measures the proportion of positives that are correctly identified as such (cancer cell detection, email spam, insurance fraud etc)  \n",
    "- Specificity (also called the true negative rate) measures the proportion of negatives that are correctly identified as such (e.g. the percentage of healthy people who are correctly identified as not having the condition, legitimate emails identified as such, legitimate insurance claims)  \n",
    "- Precision: Proportion of correctly identified positives from all classified as such  \n",
    "- Accuracy: Proportion of correctly identified cases from all cases \n",
    "\n",
    "![Source: Wikipedia](assets/sensitivity.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:**\n",
    "\n",
    "Given the confusion matrix, can you describe the precision, recall, and accuracy of our model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>not_paid</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>91</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>74</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "not_paid   0   1\n",
       "row_0           \n",
       "0.0       91  69\n",
       "1.0       74  78"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(round(prediction), y_test)\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5306122448979592"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "78/(78+69)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use `scikit-learn` to help us calculate it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision score: 0.5131578947368421\n",
      "Recall score: 0.5306122448979592\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score, precision_score\n",
    "\n",
    "print(f'Precision score: {precision_score(y_test, round(prediction))}')\n",
    "print(f'Recall score: {recall_score(y_test, round(prediction))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, you'll also find machine learning applications that uses the notion of a baseline measure in their model evaluation phase. The baseline performance is used to quantify the improvement of an applied solution to the problem and a **base rate** is just the accuracy of trivially predicting the most-frequent (or majority) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    778\n",
       "0    778\n",
       "Name: not_paid, dtype: int64"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loan_selected.not_paid.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our initial proportion is balanced - a classifier that does no better than 0.5 is not useful because we might as well have classify every class to the majority! \n",
    "\n",
    "False negatives and false positives are rarely equally costly to a business (or really, to any domain). For an insurance company, a false negative on an insurance payout is likely to cost the company more than a false positive for example. Finding the right precision-recall tradeoff comes with domain expertise - and let's make all of these more concrete by extending our credit risk example above.\n",
    "\n",
    "Say the bank's credit department would rather sacrifice some level of specificity or precision in favor of higher recall (or sensitivity). In simpler words, we want to be more sensitive to \"loan defaults\", how would you go about doing that? Try and think critically of the problem before scrolling down to the proposed solution.\n",
    "\n",
    "Well, one thing we can do is to set the threshold to be more sensitive to \"positive cases\": Let's see what happen if we were to predict a \"default\" when the probability exceed 0.4 (20% more sensitive than our previous classifier): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>not_paid</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>112</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "not_paid    0    1\n",
       "row_0             \n",
       "0          53   27\n",
       "1         112  120"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_threshold = prediction.apply(lambda x: 1 if x > 0.4 else 0)\n",
    "\n",
    "pd.crosstab(prediction_threshold, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision score: 0.5172413793103449\n",
      "Recall score: 0.8163265306122449\n"
     ]
    }
   ],
   "source": [
    "print(f'Precision score: {precision_score(y_test, prediction_threshold)}')\n",
    "print(f'Recall score: {recall_score(y_test, prediction_threshold)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We increased our Sensitivity or Recall rate from 0.53 to above 0.8! What is the cost of such an adjustment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbour Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The k-nearest neighbor algorithm gets it name from the fact that it uses information about an example's k-nearest neighbors to classify unlabeled examples. Upon choosing _k_, the algorithm requires a training dataset made up of examples that have been classified into several categories, as labeled by a nominal variable. Then, for each unlabeled record in the test dataset, k-NN identifies _k_ records in the training data that are the \"nearest\" in similarity. The unlabeled test instance is assigned the class of the majority of the k-nearest neighbors.  \n",
    "\n",
    "Supposed we pick k=1, then the * in the following feature space will be assigned the square class, but if k=5, then the majority class of the five nearest point will be assigned to that point and our point will be classified as a round instead.\n",
    "\n",
    "![ ](assets/knn.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that we would then use a *distance function* to find data point's nearest neighbors. Traditionally, the k-NN algorithm assumes *Euclidean distance*, which is the shortest direct route (imagine using a ruler to connect two points). While Euclidean distance function is the most widely used distance metric in k-NN, you will sometimes see the Manhattan distance (which is based on the paths a pedestrian would take by walking city blocks) being used instead [^5]. \n",
    "\n",
    "While these distance functions exist, Euclidean distance is far more often seen in industrial applications and is therefore the focus of this chapter.\n",
    "\n",
    "### Euclidean Distance  \n",
    "Let A and B be represented by feature vectors A = ($x_1, x_2, …, x_m$) and B = ($y_1, y_2, …, y_m$), where _m_ is the dimensionality of the feature space. To calculate the distance between A and B, the Euclidean Distance formula can be represented as such:\n",
    "\n",
    "dist(A, B) = $\\sqrt{\\sum\\limits^{m}_{i=1}(x_i-y_i)^2}$\n",
    "\n",
    "Applying the above formula on our blind-tasting example, we can calculate the distance between:  \n",
    "- tomato (sweet: 6, crunchy: 4)  \n",
    "- green bean (sweet: 3, crunchy: 7)\n",
    "\n",
    "dist(tomato, greenbean) = `sqrt((6-3)^2 + (4-7)^2))`, which is 4.24\n",
    "\n",
    "\n",
    "### Choosing an appropriate *k*\n",
    "The decision of how many neighbors to use for k-NN determines how well the model will generalize to future data. The balance between overfitting and underfitting the training data is a problem known as **bias-variance tradeoff**. Choosing a large k reduces the impact or variance caused by noisy data, but can bias the learner so that it runs the risk of ignoring small, but important patterns.\n",
    "\n",
    "If we use a very large k, say, a k value as large as the total number of observations in the training data, this would lead to the always predicting the majority class, which we've learned about in the previous chapter.\n",
    "\n",
    "On the opposite extreme, using a single nearest neighbor allows the noisy data or outliers to unduly influence the classification of examples. If one of our training examples were accidentally mislabeled and happens to be a neighboring data point, choosing a k=1 will have resulted in a misclassification, even if the nine other nearest neighbors would have voted differently.\n",
    "\n",
    "![](assets/biasvariance.png)\n",
    "\n",
    "In practice, one common strategy is to begin with *k* equal to the square root of the number of training examples. Another strategy is to choose a larger k but apply a weighted voting process in which the vote of the closer neighbors is considered more authoritative than the vote of the farther away neighbors.\n",
    "\n",
    "### Features rescaling\n",
    "Supposed, in addition to Sweetness and Crunchiness, we add a new feature \"Spiciness\" which is measured on a scale of 0 to 10,000. This range, or difference in scale, will allow the spice level of a food to have an amplified impact on the distance function. In fact, it's enlarged contribution to the distance function may end up being the singular decisive feature! \n",
    "\n",
    "We solve this by rescaling the features, i.e shrinking or expanding their range so that each feature's contribution to the distance formula is equally weighed. We want spiciness to be measured on the same scale as sweetness and crunchiness, which is a scale from 1 to 10. The two methods of rescaling features are: \n",
    "\n",
    "- Mix-Max normalization  \n",
    "- z-score standardization  \n",
    "\n",
    "**Min-max normalization** works by transforming a feature such that its values fall into a range of 0 to 1. \n",
    "\n",
    "The formula: $x_{new} = \\frac{(x-min(x))} {(max(x) - min(x))}$\n",
    "\n",
    "- Which essentially subtracts the min of feature *x* from each value and divides by the range of *x*.\n",
    "\n",
    "Normalized feature's values effectively communicates how far, in percentage terms, the original value fell along the range of all values of feature *x*.\n",
    "\n",
    "**z-score standardization** on the other hand subtracts the mean value of feature *x* and divides the outcome by the standard deviation of *x*.  \n",
    "\n",
    "The formula: $x_{new} = \\frac{(x-\\bar x)}{std(x)}$  \n",
    "\n",
    "Standardization rescales each of the feature's values in terms of how many standard deviations they fall above or below the mean values. The resulting value is called a *z-score*. Z-scores has no predefined bounds (minimum and maximum) and may be negative or positive numbers. A more detailed discussion of this is in the Practical Statistics coursebook you have received in an earlier workshop.\n",
    "\n",
    "### Characteristics of k-NN\n",
    "Classification methods using k-NN are called 'lazy learners'. Lazy learners do not build a model; There is no abstraction or generalization process -- compare this to the logistic regression method we've learned earlier to have an intuition of what 'building a model' means. More technically, we say that no 'parameters' are learned about the data.\n",
    "\n",
    "Let's summarize the process that goes into prediction with a k-NN classifier:  \n",
    "- Scaling (putting the variables on a same scale to avoid one variable overpowering the others)  \n",
    "- Select a positive integer *k*  \n",
    "- Select the _k_ nearest neighbor for each \"test\" sample  \n",
    "- Classify based on majority class\n",
    "\n",
    "Because k-NN makes prediction in a manner that is \"just-in-time\" by calculating the similarity between each input sample and the other training samples in the vector space, this method may be computationally expensive on dataset with high dimensionality (high memory requirement and constantly calculating \"distances\" over and over again). If we pick a small *k* value, our algorithm may also be vulnerable to the \"noise\" in our data. On its own, it is also sensitive to the \"scale\" of our data. \n",
    "\n",
    "Despite the limitations, k-NN is incredibly powerful and versatile. In fact some of its weaknesses (such as the outlier and scales) can be adequately mitigated with the scaling strategy we've learned in the earlier section. It is also generally insensitive to outlier and noise when an appropriate *k* value is picked. Unlike logistic regression or linear regression, it works well on non-linear data because k-NN does not make assumption about the data.  \n",
    "\n",
    "Under specific settings and requirements, k-NN is some of the most extensively used algorithms and have impressive accuracy. \n",
    "\n",
    "An example of Nearest Neighbor being used in performance benchmarking by the Microsoft's Kinect team:\n",
    "![Real-Time Human Pose Recognition in Parts from Single Depth Images](assets/kinect.png)\n",
    "Read: http://research.microsoft.com/pubs/145347/BodyPartRecognition.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Customer's Segment\n",
    "\n",
    "Both in the regression models class and in our logistic regression classes, we’ve learned how to obtain the coefficients and constructing the model manually (from mathematical principles / without the use of “libraries”). In this section, I’d like to demonstrate how we can also develop our own classifier from the mathematical principles behind the k-NN algorithm.\n",
    "\n",
    "Imagine you’re employed at a particular conglomerate distributing FMCG goods through a distribution network consisting of hotel, restaurant, cafes, and all variety of retail outlets. Our CRM system collected the annual spending in each of the product category for each of the customer, and we’d like to build an algorithm that automatically sort our customers into one of two segments:\n",
    "- Horeca: Short for Hotel, Restaurant and Cafe\n",
    "- Retail: Retail industry\n",
    "\n",
    "You are provided some training datasets as part of the task. We would borrow from a dataset prepared by Margarida Cardoso and available on the [UCI Machine Learning repository](https://archive.ics.uci.edu/ml/datasets/Wholesale+customers):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel</th>\n",
       "      <th>Region</th>\n",
       "      <th>Fresh</th>\n",
       "      <th>Milk</th>\n",
       "      <th>Grocery</th>\n",
       "      <th>Frozen</th>\n",
       "      <th>Detergents_Paper</th>\n",
       "      <th>Delicassen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>12669</td>\n",
       "      <td>9656</td>\n",
       "      <td>7561</td>\n",
       "      <td>214</td>\n",
       "      <td>2674</td>\n",
       "      <td>1338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7057</td>\n",
       "      <td>9810</td>\n",
       "      <td>9568</td>\n",
       "      <td>1762</td>\n",
       "      <td>3293</td>\n",
       "      <td>1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6353</td>\n",
       "      <td>8808</td>\n",
       "      <td>7684</td>\n",
       "      <td>2405</td>\n",
       "      <td>3516</td>\n",
       "      <td>7844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>13265</td>\n",
       "      <td>1196</td>\n",
       "      <td>4221</td>\n",
       "      <td>6404</td>\n",
       "      <td>507</td>\n",
       "      <td>1788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>22615</td>\n",
       "      <td>5410</td>\n",
       "      <td>7198</td>\n",
       "      <td>3915</td>\n",
       "      <td>1777</td>\n",
       "      <td>5185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Channel  Region  Fresh  Milk  Grocery  Frozen  Detergents_Paper  Delicassen\n",
       "0        2       3  12669  9656     7561     214              2674        1338\n",
       "1        2       3   7057  9810     9568    1762              3293        1776\n",
       "2        2       3   6353  8808     7684    2405              3516        7844\n",
       "3        1       3  13265  1196     4221    6404               507        1788\n",
       "4        2       3  22615  5410     7198    3915              1777        5185"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wholesale = pd.read_csv(\"data_input/wholesale.csv\")\n",
    "wholesale.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop some unwanted columns and convert the 'Channel' make it a factor and change the labels of the levels into \"horeca\" and \"retail\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "horeca    298\n",
       "retail    142\n",
       "Name: Channel, dtype: int64"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wholesale = wholesale.drop(columns=['Region'], axis=1)\n",
    "di = {'1' : 'horeca', '2' : 'retail'}\n",
    "wholesale['Channel'] = wholesale['Channel'].astype('str').map(di)\n",
    "\n",
    "wholesale['Channel'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice here that, unlike the credit risk analysis example, we do not have a balanced dataset. The prior or baseline accuracy for predicting the majority class would be 67.7%. \n",
    "\n",
    "Normalization to z-score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "cols = wholesale.select_dtypes('number').columns\n",
    "\n",
    "wholesale[cols] = scaler.fit_transform(wholesale[cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the dataset into train and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(wholesale[cols], wholesale['Channel'], test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to classify our target variable, we use a k-NN implementation from the `sklearn.neighbors` library. The `KNeighborsClassifier` function in the sklearn.neighbors library will go through each observation in our `wholesale_train` dataset, and identify the k-Nearest neighbors using Euclidean distance. Each test instance is then assigned the class of the majority of the neighbors - a tie vote is broken at random.\n",
    "\n",
    "\n",
    "We use `k=19` because it's the closest whole number to the square root of our 352, the number of our training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# code for build knn model\n",
    "\n",
    "knn_wholesale = KNeighborsClassifier(n_neighbors=19)\n",
    "knn_wholesale.fit(x_train, y_train) \n",
    "\n",
    "channel_prediction = knn_wholesale.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the model performance, we can see our confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>horeca</th>\n",
       "      <th>retail</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Channel</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>horeca</th>\n",
       "      <td>53</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>retail</th>\n",
       "      <td>6</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0    horeca  retail\n",
       "Channel                \n",
       "horeca       53       3\n",
       "retail        6      26"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(y_test, channel_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.896551724137931"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_test, channel_prediction, pos_label='retail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8125"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test, channel_prediction, pos_label='retail')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
