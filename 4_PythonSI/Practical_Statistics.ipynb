{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coursebook: Inferential Statistics**\n",
    "- Part 1 of _Python Fundamental Course_\n",
    "- Course Length: 9 Hours\n",
    "- Last Updated: July 2019\n",
    "\n",
    "___\n",
    "\n",
    "- Developed by [Algoritma](https://algorit.ma)'s product division and instructors team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coursebook is part of the **Python Fundamental Course** prepared by [Algoritma](https://algorit.ma). The coursebook is intended for a restricted audience only, i.e. the individuals and organizations having received this coursebook directly from the training organization. It may not be reproduced, distributed, translated or adapted in any form outside these individuals and organizations without permission.\n",
    "\n",
    "Algoritma is a data science education center based in Jakarta. We organize workshops and training programs to help working professionals and students gain mastery in various data science sub-fields: data visualization, machine learning, data modeling, statistical inference etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Objectives\n",
    "\n",
    "On the first section of this course is to pave the statistical foundation for more advance implementation and machine learning implementation. There will be 2 main objectives:\n",
    "\n",
    "- **Descriptive Statistics**\n",
    "- Understanding 5 number summary\n",
    "- Central tendency measure\n",
    "- Variability measure\n",
    "- Z Score and Central limit theorem\n",
    "- **Inferential Statistics**\n",
    "- Probability density function\n",
    "- Confidence intervals\n",
    "- Hypothesis test\n",
    "- Power analysis\n",
    "- Designing A/B test\n",
    "\n",
    "At the end of this course, we’ll be working with Learn by Building module as your graded assignment. You’ll be working in groups and design an A/B testing of your choice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Statistics\n",
    "\n",
    "Statisticians and data scientists use descriptive statistics to summarize and describe a large number of measurements. Many times, this task is accompanied with graphs and plots that help describe the numerical summary of data. When data science is applied in the business context, an example of descriptive statistic is the average number of transactions per month. Another example is the percentage of e-commerce transactions with a voucher code applied. The simple rule is that descriptive statistics do not involve generalizing beyond the data we have obtained, and are merely descriptive of what we have at hand. The branch of statistics that deal with drawing inferences about the larger population is called inferential statistics.\n",
    "\n",
    "In describing data, we are typically concerned with the task of quantifying and comparing central tendency, variability, and the shape of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "product_title      object\n",
       "location           object\n",
       "account_seller     object\n",
       "price             float64\n",
       "review              int64\n",
       "installment       float64\n",
       "rating            float64\n",
       "brand_hp           object\n",
       "dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "hp = pd.read_csv('tokped_hp.csv', encoding='latin8')\n",
    "hp.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measures of Central Tendency\n",
    "\n",
    "Often times in the exploratory data analysis phase, we want to get a sense of what the **most representative score** of a particular measurement is. We often simplify this idea by referring to it as the “average”, but there are in fact, three measures of central tendency that you need to have in your statistical toolset.\n",
    "\n",
    "The most popular measure of central tendency is the **mean**, which is sometimes represented as $\\bar{x}$ when computed on a **sample** and represented as $\\mu$ when computed on a population. Mean is really the sum of all your measurements, divided by the number of measurements, and works best on data that has an even distribution or a normal distribution (don’t worry if the idea of a normal distribution isn’t clear - we’ll get to that in a while!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2,616,352.82'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'{:,.2f}'.format(hp.price.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **median** is the point of value that cuts the distribution into two equal halves such that 50% of the observations are below it. To find this value, we would order the observations and find the middle value that separates the distribution into two equal halves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2,000,000.00'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'{:,.2f}'.format(hp.price.median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For data with odd number of observations, the median is the middle value but for data with an even number of observations we would instead use the average of the two middle scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1185000.0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odds_price = hp.iloc[0:9,:]\n",
    "odds_price.price.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1185000.0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odds_price.sort_values('price').price.tolist()[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1190350.0"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odds_price = hp.iloc[0:10,:]\n",
    "odds_price.price.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1190350.0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fourth = odds_price.sort_values('price').price.tolist()[4]\n",
    "fifth = odds_price.sort_values('price').price.tolist()[5]\n",
    "\n",
    "(fourth + fifth)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to be cautious when applying the mean on data with a skewed distribution because the mean may not be the best candidate for a most representative score compared to other measures of central tendency. For example, a company surveys its employees household income and posted the following monthly household income (IDR, in Mil):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the median puts that figure at about 7.25, the mean is about 2.67 times higher and is not truly representative of the actual household income. While most of the employees have a combined household earning of less than 8 mil, the mean value of our household income would have believe that the average household income of our employees is in fact more than 20 mil IDR.\n",
    "\n",
    "The median in this case is a better measure of centrality because it is not sensitive to the outlier data.\n",
    "\n",
    "If we are in fact, *required* to compute the mean on data with skewed distribution, another technique to reduce the influence of outlier data is to use a slight variation of the mean, called the Trimmed Mean. The trimmed mean removes a small designated percentage of the largest and smallest values before computing the mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When there are discreet values fora variable, the **mode** refers to the value that occurs most frequently. This statistic is rarely used in practice. Say we are surveying the number of times a customer place a booking in our travel booking app over the period of a year and collected the following sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measures of Spread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measures of spread measures the extent to which **value in a distribution differ from each other.** In practice, it is far easier to compute the distance between the values to their mean and when we square each one of these distances and add them all up the average1 of that result is known as **variance**. Taking the square root of the variance will result in the **standard deviation**. Just like the mean, standard deviation is the “expected value” of how far the scores deviate from the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5851682988361.099"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp.price.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And taking the square root of variance yields the standard deviation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2419025.2144946936"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp.price.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance and standard deviation are always positive when the values are not identical. When there’s no variability, the variance is 0. Because variance and standard deviation are sensitive to every value, they may not be the most “representative” measurement for skewed data.\n",
    "\n",
    "Other measurements of the spread are the **range** and the **interquartile range**. The range is the distance from our smallest measurement to the largest one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20350000.0"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp.price.max() - hp.price.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interquartile range is the range computed for the middle 50% of the distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2050000.0"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp.price.quantile(0.75) - hp.price.quantile(0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall how we can call `describe()` to create a summarized numerical information for each column? It gives us a quick access to what we have learned in this section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    4.920000e+03\n",
       "mean     2.616353e+06\n",
       "std      2.419025e+06\n",
       "min      0.000000e+00\n",
       "25%      1.200000e+06\n",
       "50%      2.000000e+06\n",
       "75%      3.250000e+06\n",
       "max      2.035000e+07\n",
       "Name: price, dtype: float64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp.price.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:**\n",
    "\n",
    "Which financial assets has more votality in their annual price?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_coins = [1.4, 0.4, 0.8, 1.1, 1.8, 2.2, 2.3, 1.2]\n",
    "price_oil = [1.6, 1.2, 1.9, 0.8, 0.6, 1.5, 2.1, 1.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary measure of votality used by stock traders and financial analysts is standard deviation, and recall that this metric reflects the average amount of an item’s price over a period of time. While the price for our fictional “oil” asset and “coins” asset averaged out to be USD 1.4 over time, which of these two present a higher votality than the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common way to quickly inspect a data is by using visualization techniques. Let's recall back to last week course using Bokeh. The following codes is used to create a *box plot*. This plot provides you a five number summary and is really useful for you to understand the distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance and Correlation\n",
    "\n",
    "When we have two samples, X and Y, of the same size, then the **covariance** is an estimate of how **variation in X is related to the variation in Y**. Covariance measures how two variables *covary* and is represented as:\n",
    "\n",
    "$Cov(X,Y) = \\frac{1}{n-1} \\sum_{i=1}^{n}(X_i - \\mu_X)(Y_i - \\mu_Y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>review</th>\n",
       "      <th>installment</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>price</th>\n",
       "      <td>5.851683e+12</td>\n",
       "      <td>-3.626288e+07</td>\n",
       "      <td>2.438004e+11</td>\n",
       "      <td>66207.455167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review</th>\n",
       "      <td>-3.626288e+07</td>\n",
       "      <td>3.081258e+04</td>\n",
       "      <td>-1.511326e+06</td>\n",
       "      <td>-1.744481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>installment</th>\n",
       "      <td>2.438004e+11</td>\n",
       "      <td>-1.511326e+06</td>\n",
       "      <td>1.015783e+10</td>\n",
       "      <td>2759.214768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rating</th>\n",
       "      <td>6.620746e+04</td>\n",
       "      <td>-1.744481e+00</td>\n",
       "      <td>2.759215e+03</td>\n",
       "      <td>0.035065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    price        review   installment        rating\n",
       "price        5.851683e+12 -3.626288e+07  2.438004e+11  66207.455167\n",
       "review      -3.626288e+07  3.081258e+04 -1.511326e+06     -1.744481\n",
       "installment  2.438004e+11 -1.511326e+06  1.015783e+10   2759.214768\n",
       "rating       6.620746e+04 -1.744481e+00  2.759215e+03      0.035065"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp.cov()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting a negative covariance means that smaller X tends to be associated with larger Y (and vice versa). The covariance of any variable with itself is its variance2. Notice also that $Cov(X,Y) = Cov(Y,X)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correlation**, unlike covariance, is not sensitive to the units in which our variables X and Y are measured and hence more useful for determining how strong the relationship is between variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>review</th>\n",
       "      <th>installment</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>price</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.085400</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.146161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review</th>\n",
       "      <td>-0.085400</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.085427</td>\n",
       "      <td>-0.053072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>installment</th>\n",
       "      <td>0.999985</td>\n",
       "      <td>-0.085427</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.146201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rating</th>\n",
       "      <td>0.146161</td>\n",
       "      <td>-0.053072</td>\n",
       "      <td>0.146201</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                price    review  installment    rating\n",
       "price        1.000000 -0.085400     0.999985  0.146161\n",
       "review      -0.085400  1.000000    -0.085427 -0.053072\n",
       "installment  0.999985 -0.085427     1.000000  0.146201\n",
       "rating       0.146161 -0.053072     0.146201  1.000000"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some facts about correlation:  \n",
    "- $Cor(X,Y) == Cor(Y,X) $ \n",
    "- $-1 <= Cor(X,Y) <= 1 $ \n",
    "- $Cor(X,Y)$ is 1 or -1 only when the X and Y observations fall perfectly on a positive or negatively sloped line  \n",
    "- $Cor(X,Y) = 0$ implies no linear relationship  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say you had a rather large amount of variables in your hand. While python can handle large numbers well, a visualization technique called correlation plot can save you some time in inspecting your data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Normal Distribution\n",
    "\n",
    "Another way of studying the central tendency and spread of data is through a curve: a curve is often used to represent a distribution and the most famous of all distributions is the normal curve.\n",
    "\n",
    "A normal distribution with a mean of 0 and standard deviation of 1 is called a **standard normal curve** and can be plotted with while specifying the limits for our x-axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a measurement follows a standard normal distribution, then the assumptions of a normal distribution can be applied to the data and these assumptions can be completely specified by two parameters, which are the mean and standard deviation. The empirical rule of a standard normal gives us the following:\n",
    "\n",
    "- 68% of data will fall within 1 standard deviation of the mean  \n",
    "- 95% of data will fall within 2 standard deviations of the mean  \n",
    "- 99.7% of data will fall within 3 standard deviations of the mean  \n",
    "\n",
    "Scroll back to the normal curve we plotted above, observe:  \n",
    "- It is perfectly symmetrical   \n",
    "- It is unimodal (has only a single mode)  \n",
    "- Area under curve is 1  \n",
    "\n",
    "One relating idea that gives the normal distribution such significance is known as the **Central limit theorem**: it says that when we have many independent variables generated by all kinds of distributions, the aggregate of those variables will tend toward a normal distribution assuming of course the lack of any extraordinary intervention. This universality is observed across different domains making the normal distribution a core centerpiece in applied statistics and mathematics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an exercise, I’d like you to generate 50 random numbers using `numpy` indicating mean of 0 and standard deviation of 1. Now use the `density()` function I created and create a plot based on the generated 50 random numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.6491357826359128"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.normal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supposed you were to change 50 to 500, and then to 5000 and even 50000, what did you observe? The key takeaway here is that as the number of sample approach infinity this plot will eventually converge in distribution to the standard normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferential Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Density Function and Probability Mass Function\n",
    "\n",
    "When we’re thinking about continuous random variables (blood sugar level, height, rainfall amount), it is important to realize that this variable has an uncountable number of possible values, even between two real intervals. The resulting probability distribution of the variable can be described by a probability density, where the probability is found by taking the area under the curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Add normal distribution pdf here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discrete random variables (number of player injury, amount of defaulted loans, travel bookings per customer), on the other hand, can be described using a probability mass function, which maps each value of the random variable to a probability:  \n",
    "- p(0 bookings) = 0.28  \n",
    "- p(1 booking) = 0.09  \n",
    "- …  \n",
    "- p(6 bookings) = 0.004  \n",
    "\n",
    "Because they are probabilities, these individual probabilities have to sum up to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going into probability density function and mass function, it is important to understand that the function is estimated by binning the distribution values. Let's take a look at the following histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram is commonly combined with either probability density or mass function to generates the following insight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamentals of Hypothesis Testing\n",
    "\n",
    "Going back to the standard normal distribution - you may be asking by now how any of what you’re learning in the past few chapters are useful. To answer the question, I feel it is only appropriate we solidify these intuition with a few concrete examples. Consider the following scenario:\n",
    "\n",
    "The height of men in Indonesia is normally distributed with a mean of 160cm and a standard deviation of 7cm. What is the probability of a randomly selected man being taller than 175cm?\n",
    "\n",
    "The solution: 175cm is 15cm above the mean, and dividing that by the standard deviation of 7cm, we get 2.143. We refer to this as the **z-score**. The probability of an Indonesian men being taller than 175cm is P(Z > 2.143)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:**\n",
    "\n",
    "Supposed we have a population mean of 180cm and standard deviation of 8cm, what is the probability that a randomly picked person being shorter than 174cm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Z-scores we used above is useful when relating different measurement distributions to each acting as a common denominator. Essentially, a z-score gives us a “standardized” unit that measure how many standard deviations is a particular statistic away from the mean. This property, as we’ll see, is paramount to many statistical hypothesis tests, performance evaluation (more of that when we get to the Machine Learning courses), and in the construction of confidence or prediction intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Error and Confidence Intervals\n",
    "\n",
    "Standard error can be thought as a related concept to standard deviation. It is a measure that estimates how close a calculated mean (sample) is likely to be to the true mean of that population; It is calculated by dividing the standard deviation of the sample data by the square root of the number of observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a normal distribution (we’ll get to this in a moment), 67% of the time the true population mean will lie within the range of +- 1 SE. In other words, if we can establish a normal distribution, we can theorize about the true value of the population mean with a range. Because of the formula:\n",
    "\n",
    "$SE = \\frac{\\sigma}{\\sqrt{}n}$\n",
    "\n",
    "The larger our sample size n gets, the smaller SE will be: and hence we are less uncertain about our estimation of the true population mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We often begin our estimation with a point estimate, using for example the sample mean $\\bar{x}$ as a point estimate of the population mean $\\mu$. We can then construct confidence intervals around our point estimates so we have an interval that may contain the true value of the parameter. When statisticians say a “95% confidence interval”, what they mean is that if we create 100 confidence intervals of the same size from a given population, we expect to find the true parameter (let’s say the population’s net savings per household) in 95 of them.\n",
    "\n",
    "We construct a confidence interval by taking the point estimate +/- margin of error, where margin of error is computed as:  \n",
    "\n",
    "$E=Z_{a/2} \\times SE$\n",
    "\n",
    "Because confidence intervals are two-sided the level of significance we chose (alpha) has to be divided into halves. When we compute by finding the z-score associated with a value of 2.5% (0.025) on each end, we end up looking at the middle 95% of the area under the curve. We can use `ppf()` from `scipy` package to help us find the z-score associated with a 95% confidence interval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6448536269514722"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "norm.ppf(.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let’s put together a scenario to make all of this more concrete. Say we want to know the average annual dividend payout in a particular industry, and had known through an earlier study that this figure resembles a normal distribution with a population standard deviation of 2.4%. We looked at the public books of these 81 companies in said industry and attain a sample mean of 11.8% (that is, the average company from this group of 81 companies pay 11.8% of profit to their shareholders annually). We want to construct a 95% confidence interval for the μ, the mean dividend payout.\n",
    "\n",
    "Solution: \n",
    "- Z-score associated with a 95% confidence interval is 1.96  \n",
    "- The standard error of the mean (SE) is $\\frac{2.4}{\\sqrt{81}} = 0.267$   \n",
    "- The margin of error (E) is $1.96 \\times SE = 0.524$  \n",
    "- The confidence interval is 11.8% +- 0.524%  \n",
    "\n",
    "And so we can say that the 95% confidence interval for the mean dividend payout in this industry is (11.28%, 12.32%). We can be 95% confident that this interval will contain the mean dividend payout for this particular industry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### p-value\n",
    "\n",
    "In your day to day data science work, you will often be required to explain the model’s reliability and uncertainty, and the formal process of such is specified as something called the **test of significance**. Statistical significance often reference the **p-value**, a measure of the probability of obtaining a result equal to or more extreme than what was actually observed, assuming the null hypothesis is true.\n",
    "\n",
    "Imagine a scenario where you’re assigned to consult on Quicker, a startup that simplify and automate government grants application for newly incorporated startups. Through public announcements and official records, you find that the average duration for a newly incorporated startup to get its first government grant or financial funding is 215 days (with a population standard variance of 24 days, again through official records). Of the 35 entrepreneurs using Quicker platform, the average time is 178 days.\n",
    "\n",
    "**Discussion:**\n",
    "Does this observation (178 days) deviate away from the population enough for it to be statistically significant? Use a 95% confidence interval\n",
    "\n",
    "$H_0: \\mu = \\mu_0$  \n",
    "$H_A: \\mu < \\mu_0$\n",
    "\n",
    "The null hypothesis is that our new mean equals to the original mean, whereas the alternative hypothesis states that the mean of Quicker customers **is lower** than the original mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution:\n",
    "\n",
    "Recall what we’ve learned about the z-score, we can compute the p-value as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we can reject the null hypothesis at the 95% confidence level if our p-value is <= alpha (0.05), this is not the case here as our p-value is actually 0.0616. In this scenario, we fail to reject the null hypothesis that the Quicker platform did in fact led to a statistically significant reduction in government funding time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-Test\n",
    "\n",
    "Generally, z-tests are used when we have a large enough sample size (rule of thumb is n >= 30) and when the population standard deviation is known. If the above conditions aren’t met, we can instead use a statistical measurement known as the Student’s t-test. Say grant automation platform has 10 users so far and that the population standard deviation (sigma) is not known.\n",
    "\n",
    "To perform a t-test we can use **PYTHON FUNCTION HERE** and pass in an optional alternative hypothesis. Here we’re interested in finding out whether we can reject the null hypothesis (that says there is no difference in government funding time for Quicker startups and other startups) in favor of the alternative hypothesis (that Quicker startup spend less time on average to acquire their first grant):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power Analysis\n",
    "\n",
    "In designing a hypothesis testing, or popularly known as A/B testing, one of its critical part is: determining the size of your sample. While significance test is widely used in the industry, namely a few:  \n",
    "- Testing new features that creates more engagement  \n",
    "- Selecting effective marketing strategy to generate more conversion  \n",
    "- Finding promotion best practice to increase revenue  \n",
    "\n",
    "The cost of evaluating these experiments is expensive for both time and resource. The role of data scientist, is to make sure the proposed design of an A/B testing has considered this resource constraint and also generate a conclusive result.\n",
    "\n",
    "In some cases, drawing conclusion can lead to a fatal error due to wrong choice of sampling decision. Now let me introduce you to **2 types of error**:  \n",
    "- Type 1 error: false positive  \n",
    "- Type 2 error: false negative  \n",
    "\n",
    "In hypothesis test context, type 1 error occur when we reject a true null hypothesis while type 2 error occur when we fail to reject a false null hypothesis (a similar concept later on machine learning course!). Now what do we do with this information? Well a **higher statistical power** will result in a lower probability of failing to reject a false null hypothesis. In practice, a lower power will lead to wrong conclusion as stated in the following formula:  \n",
    "\n",
    "$Power = P(reject H_0 | H_A True)$  \n",
    "$Power = 1 - P(Error_2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **EFFECT SIZE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider this:\n",
    "\n",
    "You are designing an experiment for Tokopedia new homepage, you'd like to see if a certain promotional banner is placed in the second slot would lead to an ineffective campaign. In this case, say we accept the probability of type 2 error to be 15% and a significance level of 5%. The expected effect size is set to be 10%. Try the below code and see what is the required sample size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1796"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statsmodels.stats.power import TTestIndPower\n",
    "\n",
    "effect_size = 0.1\n",
    "alpha = 0.05\n",
    "power = 1-0.15\n",
    "\n",
    "power_analysis = TTestIndPower()\n",
    "sample_size = power_analysis.solve_power(effect_size = effect_size, \n",
    "                                         power = power, \n",
    "                                         alpha = alpha)\n",
    "\n",
    "int(sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most times, you would generate a visualization to analyze the trade-off in selecting the parameters. Let's take a look at the following plot: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above plot will give you an idea of how would the Power and sample size varies given different set of parameters. The information you gain in this analysis needs to correspond with the cost constraints of your experiment and the risk that came with it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tokopedia",
   "language": "python",
   "name": "tokopedia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
