{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coursebook: Getting Started with Python**\n",
    "- Part 1 of _Python Machine Learning Fundamentals_\n",
    "- Course Length: 24 Hours\n",
    "- Last Updated: July 2019\n",
    "\n",
    "___\n",
    "\n",
    "- Developed by [Algoritma](https://algorit.ma)'s product division and instructors team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "The coursebook is part of the **Python Machine Learning Fundamentals** prepared by [Algoritma](https://algorit.ma). The coursebook is intended for a restricted audience only, i.e. the individuals and organizations having received this coursebook directly from the training organization. It may not be reproduced, distributed, translated or adapted in any form outside these individuals and organizations without permission.\n",
    "\n",
    "Algoritma is a data science education center based in Jakarta. We organize workshops and training programs to help working professionals and students gain mastery in various data science sub-fields: data visualization, machine learning, data modeling, statistical inference etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Objectives\n",
    "\n",
    "On the first section of this **Python Machine Learning Fundamentals**, we'll start by getting used in working with Jupyter Notebook and programming basics. In this coursebook we will cover:\n",
    "\n",
    "- **Regression Model**\n",
    "    - Simple Linear Regression\n",
    "    - Multiple Linear Regression\n",
    "- **Classification**\n",
    "    - Binary Logistic Regression\n",
    "    - K-Nearest Neighbour\n",
    "    - Evaluating classifier model\n",
    "- **Clustering Analysis**\n",
    "\n",
    "By the end of this course, you'll be working on a **Learn-by-Building** module to create a data exploratory analysis project to apply what you have learned on provided dataset and attempt to answer all the given questions. This final part is considered as a Graded Assignment so make sure you do well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning on a very basic level, refers to a sub-field of computer science that “gives computer the ability to learn without being explicitly programmed”. Less-sensationally, it is concerned with the theory and application of statistical and mathematical methods to arrive at a particular objective without following a set of strictly defined and rigid pre-determined rules.\n",
    "\n",
    "The most common machine learning is divided into two types, namely **supervised learning** and **unsupervised learning**. The difference between the two is that when we talk about **supervised learning** then we talk about predictions of a value because in supervised learning the data we will analyze has a variable target.\n",
    "\n",
    "When the prediction value is numerical (think oil prices, rainfall, quarterly sales, blood pressure etc), it is generally referred to as a “regression” problem. This is in contrast with “classification” problems, a general term for when the value we’re trying to predict is categorical (loan defaults, email spam collection, handwriting recognition etc).\n",
    "\n",
    "Whereas if we talk about **unsupervised learning** then we talk about data exploration because the data we want to process doesn't have a variable target. The method usually used to analyze a data using unsupervised learning is clustering or data grouping.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Model\n",
    "\n",
    "First of all we want to learn about machine learning is about regression model. So let’s try and achieve some intuition about this important concept through the following illustration. I’m going to ahead and load some data and create a histogram from the resulting data.\n",
    "\n",
    "It is important here to remind you that regression models are not just used in the machine learning context for numeric prediction. Regression, in fact, represent the “workhorse of data science”1 and is among the most practical and theoretically understood models in statistics. Data scientists well trained with this foundation will be able “to solve an incredible array of problems”2. Because regression models often lead to highly interpretable models, we can (and should) consider them as a handy statistical tool that has its place in some of the most common data science tasks:\n",
    "\n",
    "- **Prediction**: Predict the profitability of a new product category given its pilot launch sales figure\n",
    "- **Statistical Modeling**: Determining a quantitative relationship between price sensitivity and average sales unit\n",
    "- **Covariation**: Determining the (residual) variation in average sales unit that appears unrelated to price levels; and to investigate the impact of other external factors beyond price points in explaining the fluctuation of average sales unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression\n",
    "\n",
    "### Least Square Regression\n",
    "\n",
    "One of the terms you’ll hear a lot in this course is about linear regression. So let’s try and achieve some intuition about this important concept through the following illustration. I’m going to ahead and load some data and create a histogram from the resulting data.\n",
    "\n",
    "The data, as some among us may recall, is from an online retailer that specialize in the trading of office supplies and stationery:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Row.ID</th>\n",
       "      <th>Order.ID</th>\n",
       "      <th>Order.Date</th>\n",
       "      <th>Ship.Date</th>\n",
       "      <th>Ship.Mode</th>\n",
       "      <th>Customer.ID</th>\n",
       "      <th>Segment</th>\n",
       "      <th>Product.ID</th>\n",
       "      <th>Category</th>\n",
       "      <th>Sub.Category</th>\n",
       "      <th>Product.Name</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Discount</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>407</td>\n",
       "      <td>CA-2017-117457</td>\n",
       "      <td>12/8/17</td>\n",
       "      <td>12/12/17</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>KH-16510</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>TEC-CO-10004115</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Copiers</td>\n",
       "      <td>Sharp AL-1530CS Digital Copier</td>\n",
       "      <td>1199.976</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>434.9913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5069</td>\n",
       "      <td>CA-2014-124478</td>\n",
       "      <td>8/8/14</td>\n",
       "      <td>8/12/14</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>MA-17560</td>\n",
       "      <td>Home Office</td>\n",
       "      <td>TEC-CO-10001571</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Copiers</td>\n",
       "      <td>Sharp 1540cs Digital Laser Copier</td>\n",
       "      <td>549.990</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>274.9950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3274</td>\n",
       "      <td>CA-2017-133865</td>\n",
       "      <td>5/8/17</td>\n",
       "      <td>5/12/17</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>PS-19045</td>\n",
       "      <td>Home Office</td>\n",
       "      <td>TEC-CO-10001046</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Copiers</td>\n",
       "      <td>Canon Imageclass D680 Copier / Fax</td>\n",
       "      <td>3359.952</td>\n",
       "      <td>6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1049.9850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6201</td>\n",
       "      <td>CA-2015-146675</td>\n",
       "      <td>4/16/15</td>\n",
       "      <td>4/20/15</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>SB-20185</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>TEC-CO-10001766</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Copiers</td>\n",
       "      <td>Canon PC940 Copier</td>\n",
       "      <td>1439.968</td>\n",
       "      <td>4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>485.9892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7037</td>\n",
       "      <td>CA-2014-124618</td>\n",
       "      <td>5/2/14</td>\n",
       "      <td>5/4/14</td>\n",
       "      <td>Second Class</td>\n",
       "      <td>CS-11860</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>TEC-CO-10004202</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Copiers</td>\n",
       "      <td>Brother DCP1000 Digital 3 in 1 Multifunction M...</td>\n",
       "      <td>479.984</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>89.9970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Row.ID        Order.ID Order.Date Ship.Date       Ship.Mode Customer.ID  \\\n",
       "0     407  CA-2017-117457    12/8/17  12/12/17  Standard Class    KH-16510   \n",
       "1    5069  CA-2014-124478     8/8/14   8/12/14  Standard Class    MA-17560   \n",
       "2    3274  CA-2017-133865     5/8/17   5/12/17  Standard Class    PS-19045   \n",
       "3    6201  CA-2015-146675    4/16/15   4/20/15  Standard Class    SB-20185   \n",
       "4    7037  CA-2014-124618     5/2/14    5/4/14    Second Class    CS-11860   \n",
       "\n",
       "       Segment       Product.ID    Category Sub.Category  \\\n",
       "0     Consumer  TEC-CO-10004115  Technology      Copiers   \n",
       "1  Home Office  TEC-CO-10001571  Technology      Copiers   \n",
       "2  Home Office  TEC-CO-10001046  Technology      Copiers   \n",
       "3     Consumer  TEC-CO-10001766  Technology      Copiers   \n",
       "4     Consumer  TEC-CO-10004202  Technology      Copiers   \n",
       "\n",
       "                                        Product.Name     Sales  Quantity  \\\n",
       "0                     Sharp AL-1530CS Digital Copier  1199.976         3   \n",
       "1                  Sharp 1540cs Digital Laser Copier   549.990         1   \n",
       "2                 Canon Imageclass D680 Copier / Fax  3359.952         6   \n",
       "3                                 Canon PC940 Copier  1439.968         4   \n",
       "4  Brother DCP1000 Digital 3 in 1 Multifunction M...   479.984         2   \n",
       "\n",
       "   Discount     Profit  \n",
       "0       0.2   434.9913  \n",
       "1       0.0   274.9950  \n",
       "2       0.2  1049.9850  \n",
       "3       0.2   485.9892  \n",
       "4       0.2    89.9970  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copymachines = pd.read_csv(\"data_input/copiers.csv\")\n",
    "copymachines = copymachines.sample(frac=1).reset_index(drop=True)\n",
    "copymachines.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the sales variable take on a rather large value (with an outlier at $5000), the idea of a least squares estimate is to identify a point in our data that minimizes the sum of the squared distances between the observed data and itself. We’ll observe later that, with no predictor variables, this least squares estimate is the sample average.\n",
    "\n",
    "Because our estimation model isn’t going to predict every observation perfectly, minimizing the average (which is equivalent to: the sum) of squared errors seem like a reasonable thing to do. If we had minimize the average absolute deviation between the data, it would lead us to the median as the least squares estimate instead of the mean. While this may seem intuitive to some, I am counting on some of you to be skeptical enough as to question me on whether the sample average would in fact lead us to the least squares estimate.\n",
    "\n",
    "Let’s explain the importance of least squares in the context of regression models. Before I create a scatterplot of the sales data, I’d remove the far outlier (the one close to $5000) from our sample data and treat it as noise. Do note that removing outlying data (or in the general treatment of outliers) is not a decision to be taken lightly and generally involve a more methodical and lengthier treatment with respect to its implications. If done poorly, you may even be guilty of “doctoring” the data to fit your pre-determined narrative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to plot Profit and Sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, without diverting too far from the subject matter of this workshop, I think you can agree that the decision is well-justified. Another useful way to think about this particular decision in this case is to consider the following trade-off:\n",
    "\n",
    "> Do we want the presence of a numerically distant observation (potentially < ~1% of total observations) at the expense of “poorer model fit” for the rest (99%) of the observation?\n",
    "\n",
    "To refresh your memory, recall from your Practical Statistics class that the box plot identify an observation as an outlier if it’s positioned above or below either of its “whiskers”. That, we learned is 1.5 times the interquartile range above the upper quartile and bellow the lower quartile. Let’s draw a box plot of our variable of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x198e54d0240>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "copymachines.boxplot(column = ['Sales'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminating the outlier data from our original sample, we can now plot Sales against Profit and attain the following graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code plot of sales without outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a fairly linear relationship between the Sales and Profit variables of our `copymachines` dataset, and the objective of a simple linear regression is concerned with modeling that relationship with a straight line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Linear Regression\n",
    "\n",
    "Create a linear model in Python is as convenient as you’d wish it to be. We will call the `LinearRegression()` function and specify two parameters: the `x_data` for our linear model and `y_data` from which our model is built from.\n",
    "\n",
    "But before we build simple linear regression model, the `copymachines` data will split into train and test data. We given train dataset by 80% of `copymachines` datas and the test dataset given by 20%  of `copymachines` datas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(copymachines.shape[0]*0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45     839.988\n",
       "46     439.992\n",
       "47     879.984\n",
       "48     899.970\n",
       "49    2799.944\n",
       "Name: Sales, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = copymachines.Sales[:50]\n",
    "X_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "sel = round(copymachines.shape[0]*0.8)\n",
    "\n",
    "x_train = copymachines.Sales[:sel]\n",
    "x_test = copymachines.Sales[sel:]\n",
    "\n",
    "y_train = copymachines.Profit[:sel]\n",
    "y_test = copymachines.Profit[sel:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You could also have used several built-in scikit-learn functions for the shuffling task. An example of achieving this effect is the `shuffle()` function:\n",
    "\n",
    "    from sklearn.utils import shuffle\n",
    "    df = shuffle(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1199.976  549.99  3359.952 1439.968  479.984  899.982  719.976 1999.96\n",
      "  639.968 2399.96   439.992  479.984 1199.96   549.99   599.99   959.984\n",
      "  599.98  1199.98  1799.97  2999.95   959.984  479.976  479.984  959.984\n",
      " 2879.952 1439.976 1999.96  2799.96  1119.984  479.984 2399.96   319.984\n",
      " 1199.976 2799.96   999.98  1399.98  1199.976  879.984 4899.93   799.984\n",
      " 1199.976  479.976 1799.97   299.99   559.992  839.988  439.992  879.984\n",
      "  899.97  2799.944]\n",
      "(50,)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(x_train))\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sklearn` expect our x to be a 2-dimensional array and would throw an Exception otherwise, so we'll reshape the data to have two dimension, the second being `1` for the single column feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 1)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.array(x_train).reshape(-1,1)\n",
    "x_test = np.array(x_test).reshape(-1,1)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create our linear regressor object (\"model\") and fit the model to our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = linear_model.LinearRegression()\n",
    "lm.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we’ve saved `lm` as a linear model and we can now use the attributes of `lm`, such as its  coefficients to create our linear model by its `intercept` and `coef_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-91.84350669944604\n",
      "[0.40563133]\n"
     ]
    }
   ],
   "source": [
    "print(lm.intercept_)\n",
    "print(lm.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with that let’s create our plot again, but this time we’ll also add a line that intercepts the y-axis at -91.8435 and have a slope of 0.4056 degree, just as the coefficients of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x198e5544c18>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the coefficients, what does that tell us? Well the size of the coefficients tell us the effect that variable has on our target variable. We observed here that Sales have a coefficient of 0.4056 on Profit, and because 0.4056 is a positive number we know that the effect is positive: the higher the Sales, the higher the Profit. A negative coefficient will indicate the opposite, and an example of that would be Profit vs Market Saturation: the increasingly saturated market leads to decreasing profit.\n",
    "\n",
    "Can you think of another example where we might observe a negative coefficient in a regression model?\n",
    "\n",
    "In addition to the above information, we’ve also derived the profit equation from our linear model directly. It takes the form:\n",
    "\n",
    "$\\hat Y=β_0+β_1X_1$\n",
    "\n",
    "Which in plain English means: Estimated Profit = Intercept + Slope * Sales\n",
    "\n",
    "Substituting the beta coefficients into the formula hence yield: Estimated Profit = -91.8435066994 + 0.40563133 * Sales\n",
    "\n",
    "That tells us that the profit is expected to increase by $0.4056$ when the sales price of our Copiers machine increase by $1$, and decrease by $0.4056$ as the sales price of our Copiers machine decrease by $1$. For a Copiers machine with a listed price of $0$, the profit is predicted to be negative (incurring a loss of approximately $-91.84$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supposed we’re expecting a sales transaction by the end of day amount $1,000$. What would our linear model predict its profit to be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "313.7878233006"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-91.8435066994 + 0.40563133 *1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that our linear model, `lm` would predict a profit of $313.7878$. Not too bad! However, Python has built-in functions such as `*.predict()` that allow us to obtain predictions given some input data. predict expects a machine learning model as its first parameter, and in this case a data frame to predict on. We already have the model object (“lm”) so let’s create the new data for us to work with.\n",
    "\n",
    "We can use the test data to predict Profit by its model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 102.85304241, 1125.03020681, 1043.90799679,  346.22859638,\n",
       "        516.58320927])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = lm.predict(x_test)\n",
    "pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x198e560d630>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R-Squared\n",
    "\n",
    "R squared by definition is the percentage of the total variability that is explained by the linear relationship with the predictor (Regression Variation / Total Variation):\n",
    "\n",
    "$R^2=\\frac {∑ \\limits_{i=1}^n (Y_i−\\bar Y)^2}{∑ \\limits_{i=1}^n(Y_i−\\bar Y)^2}$\n",
    "\n",
    "In other words, R squared can be thought of as a quantity that represents the % of the total variation that’s represented by the model. We simply take the regression variation and divide it by the total variation. In our case, it is the % of the variation in profit that is explained by the regression relationship with sales. Some facts about $R^2$:\n",
    "- $R^2$ is the percentage of variation explained by the regression model\n",
    "- 0 ≤ $R^2$ ≤ 1\n",
    "- If we define R as the sample correlation between the predictor and the outcome, $R^2$ is simply the sample correlation squared\n",
    "\n",
    "Because R-squared is a statistical measure of how close the data are to the fitted line, we want our model to achieve a high R-squared as it means our model has fit the data well (not always the case, but we’ll get to that later).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166.90383503000325"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from math import sqrt\n",
    "\n",
    "sqrt(mean_squared_error(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8858190158603508"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code to get R-Squared\n",
    "r2_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression\n",
    "\n",
    "Let’s build another regression model to solidify our understanding of regression models. Earlier on, we predict a future value of y given one input, x. Recall the y in our first example is the Profit and our x is the number of Sales. \n",
    "\n",
    "Also recall that because the number of profit depend on the number of sales, this y we’re working with is often referred to as dependent variable while the x are referred to as, you guessed it, independent variables. Can a regression model contain more than one dependent variable? Absolutely!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code we take a peek at a dataset used by criminologists to study the effect of punishment regimes on crime rates. We’ll read the dataset and rename the columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>M</th>\n",
       "      <th>So</th>\n",
       "      <th>Ed</th>\n",
       "      <th>Po1</th>\n",
       "      <th>Po2</th>\n",
       "      <th>LF</th>\n",
       "      <th>M.F</th>\n",
       "      <th>Pop</th>\n",
       "      <th>NW</th>\n",
       "      <th>U1</th>\n",
       "      <th>U2</th>\n",
       "      <th>GDP</th>\n",
       "      <th>Ineq</th>\n",
       "      <th>Prob</th>\n",
       "      <th>Time</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>130</td>\n",
       "      <td>0</td>\n",
       "      <td>116</td>\n",
       "      <td>63</td>\n",
       "      <td>57</td>\n",
       "      <td>641</td>\n",
       "      <td>984</td>\n",
       "      <td>14</td>\n",
       "      <td>26</td>\n",
       "      <td>70</td>\n",
       "      <td>21</td>\n",
       "      <td>486</td>\n",
       "      <td>196</td>\n",
       "      <td>0.069197</td>\n",
       "      <td>21.9003</td>\n",
       "      <td>523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33</td>\n",
       "      <td>147</td>\n",
       "      <td>1</td>\n",
       "      <td>104</td>\n",
       "      <td>63</td>\n",
       "      <td>64</td>\n",
       "      <td>560</td>\n",
       "      <td>972</td>\n",
       "      <td>23</td>\n",
       "      <td>95</td>\n",
       "      <td>76</td>\n",
       "      <td>24</td>\n",
       "      <td>462</td>\n",
       "      <td>233</td>\n",
       "      <td>0.049499</td>\n",
       "      <td>25.5005</td>\n",
       "      <td>1072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>142</td>\n",
       "      <td>1</td>\n",
       "      <td>88</td>\n",
       "      <td>81</td>\n",
       "      <td>77</td>\n",
       "      <td>497</td>\n",
       "      <td>956</td>\n",
       "      <td>33</td>\n",
       "      <td>321</td>\n",
       "      <td>116</td>\n",
       "      <td>47</td>\n",
       "      <td>427</td>\n",
       "      <td>247</td>\n",
       "      <td>0.052099</td>\n",
       "      <td>26.0991</td>\n",
       "      <td>946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>143</td>\n",
       "      <td>0</td>\n",
       "      <td>113</td>\n",
       "      <td>103</td>\n",
       "      <td>95</td>\n",
       "      <td>583</td>\n",
       "      <td>1012</td>\n",
       "      <td>13</td>\n",
       "      <td>102</td>\n",
       "      <td>96</td>\n",
       "      <td>36</td>\n",
       "      <td>557</td>\n",
       "      <td>194</td>\n",
       "      <td>0.029599</td>\n",
       "      <td>25.2999</td>\n",
       "      <td>1635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>152</td>\n",
       "      <td>1</td>\n",
       "      <td>87</td>\n",
       "      <td>57</td>\n",
       "      <td>53</td>\n",
       "      <td>530</td>\n",
       "      <td>986</td>\n",
       "      <td>30</td>\n",
       "      <td>72</td>\n",
       "      <td>92</td>\n",
       "      <td>43</td>\n",
       "      <td>405</td>\n",
       "      <td>264</td>\n",
       "      <td>0.069100</td>\n",
       "      <td>22.7008</td>\n",
       "      <td>798</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    M  So   Ed  Po1  Po2   LF   M.F  Pop   NW   U1  U2  GDP  \\\n",
       "0          25  130   0  116   63   57  641   984   14   26   70  21  486   \n",
       "1          33  147   1  104   63   64  560   972   23   95   76  24  462   \n",
       "2          16  142   1   88   81   77  497   956   33  321  116  47  427   \n",
       "3           2  143   0  113  103   95  583  1012   13  102   96  36  557   \n",
       "4          15  152   1   87   57   53  530   986   30   72   92  43  405   \n",
       "\n",
       "   Ineq      Prob     Time     y  \n",
       "0   196  0.069197  21.9003   523  \n",
       "1   233  0.049499  25.5005  1072  \n",
       "2   247  0.052099  26.0991   946  \n",
       "3   194  0.029599  25.2999  1635  \n",
       "4   264  0.069100  22.7008   798  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crime = pd.read_csv(\"crime.csv\")\n",
    "crime = crime.sample(frac=1).reset_index(drop=True)\n",
    "crime.head()\n",
    "\n",
    "# code for rename colnames of crime data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset was collected in 1960 and a full description of the dataset wasn’t conveniently available. I use the description I gathered from the authors of the MASS package. After you rename the dataset (in your coursebook, around line 410 to line 420), the variables are:\n",
    "- `percent_m`: percentage of males aged 14-24 - is_south: whether it is in a Southern state. 1 for Yes, 0 for No.\n",
    "- `mean_education`: mean years of schooling\n",
    "- `police_exp60`: police expenditure in 1960\n",
    "- `police_exp59`: police expenditure in 1959 - labour_participation: labour force participation rate\n",
    "- `m_per1000f`: number of males per 1000 females\n",
    "- `state_pop`: state population\n",
    "- `nonwhites_per1000`: number of non-whites resident per 1000 people\n",
    "- `unemploy_m24`: unemployment rate of urban males aged 14-24\n",
    "- `unemploy_m39`: unemployment rate of urban males aged 35-39\n",
    "- `gdp`: gross domestic product per head\n",
    "- `inequality`: income inequality\n",
    "- `prob_prison`: probability of imprisonment\n",
    "- `time_prison`: avg time served in prisons\n",
    "- `crime_rate`: crime rate in an unspecified category \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   In this part we want to predict `inequality` by `gdp` and `mean_education`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code linear regression of inequality~gdp+mean_education"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusted R-Squared\n",
    "\n",
    "Using R-squared itself can be misleading in our assessment of the model fit and this is due to the one of the key limitation of this metric. R-squared, it turns out increases with every new addition of a predictor variable, even if it turns out that the variable is just completely random number - the R-squared does not decrease. As a result, a model with more independent variables may appear to have a better fit just on the merit of having more terms alone.\n",
    "\n",
    "A model that has too many predictors also tend to overfit and worse, the regression model would “model” the random noise in our data as if they were “features”, hence producing misleading R-squared values.\n",
    "\n",
    "The adjusted R-squared compares the explanatory power of regression models built with different number of predictors, allowing us to compare a crime rate regression model with 4 variables to another one with just 2 variables and find out if the one with 4 has achieved a higher R-squared simply because it has more predictors or if they truly lead to a better fit.\n",
    "\n",
    "Compare the following 3 models and pay attention to it’s Adjusted R-squared value. Model 1 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code lm model ineq~gdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code lm model ineq~~ gdp + labour_participation + m_per1000f + time_prison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code adj r-squared of 3 model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`model1` has one predictor variable and has an R-squared of 0.7815 (adjusted R-squared of 0.7766). We see that by adding three additional predictors (`labor_participation`, `m_per1000f`, `time_prison`) the R-squared of our model increased to 0.7939 (`model3`). Now we say that R-squared indicates the quality of model fit, so does this necessarily means that `model3` is a better model than `model1`? Not really. In fact, by adding three additional parameters, our Adjusted R-Squared has decreased and returned a model that has a lower Adjusted R-squared than the two other models (despite built with more predictor variables). \n",
    "\n",
    "So as a recap, our R-squared value tells us how well our model describes the data. It measures the extent to which the variance in our dependent variable (inequality) can be explained by the independent variables (gdp etc). However, as we increase the number of independent variables our model's R-squared value will also increase as it is incorporate any legitimate information as well as the noise introduced by these extra variables.\n",
    "\n",
    "Adjusted R-squared on the other hand does not increases the way R-squared does because it is adjusted for the number of predictor variables in our model. It increases only when the new variable actually leads to a better prediction. While the mathematical details of the adjusted R-squared formula is beyond the scope of this workshop, I'll give you a quick proof that it does \"penalizes\" the r-squared based on the number of predictors the model contains.\n",
    "\n",
    "The mathematical notation of adjusted R-squared:\n",
    "$R^2_{adj} = 1-(1-R^2)\\frac{n-1}{n-p-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumption of Linear Model\n",
    "\n",
    "In making predictions on the target variable using linear regression, we must fulfill several assumptions as a condition that the regression model that we make is statistically appropriate.\n",
    "Some of the assumptions that must be fulfilled in linear regression models are:\n",
    "\n",
    "1. Linear prediction variables for target variables\n",
    "2. Normal residuals\n",
    "3. Residuals have no heteroscedasticity\n",
    "4. There is no multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linearity\n",
    "\n",
    "We can check our model has a linearity using correlation test of our target variable and predictor variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code cor test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal Residuals\n",
    "\n",
    "To check residuals of our model is normal, we can use histogram of residuals model or we can take a statistics test called shapiro wilk test. \n",
    "\n",
    "In shapiro wilk test we use hypotesist to check our model has normal residual.\n",
    "\n",
    "$H_0$ : residuals model normal\n",
    "\n",
    "$H_1$ : residuals model abnormal\n",
    "\n",
    "Recall our practical statistics material, if we want to get our conclusion of model has normal of residual, we must get the p-value of test bigger than alpha (5%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code of histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code of shapiro-wilk test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Residuals No-Heteroscedasticity\n",
    "\n",
    "Same with snormal residual test, the residual we have must be homogeneous which means the data is spread. If the residual data that we have spreads and does not form a pattern, the linear regression model that we have is good at predicting the target variable. \n",
    "\n",
    "To check our residuals doesn't have pattern, we can make a scatterplot between residual data and target variable or we can use Breuch-Pagan test.\n",
    "\n",
    "In Breuch-Pagan test we have hypothesis that :\n",
    "\n",
    "$H_0$ : residuals model doesn't have pattern (homoscedasticity)\n",
    "\n",
    "$H_1$ : residuals model have pattern (heteroskedastisity)\n",
    "\n",
    "Recall our practical statistics material, if we want to get our conclusion of model has no heteroscedasticity, we must get the p-value of test bigger than alpha (5%).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code of scatter plot between residuals and target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code using breuch pagan test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No-Multicolinearity\n",
    "\n",
    "One of the statistical tool you have at your disposal when assessing multicollinearity is the **Variance Inflation Factor** (VIF) statistic. Put simply, VIF is a way to measure the effect of multicollinearity among the predictors in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code of VIF "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dive Deeper** :\n",
    "\n",
    "I’ve created `divedeeper_lm` in the following code chunk. Before you complete the code to inspect the VIF, what are your suspicions? Do you expect any VIF higher than 10?\n",
    "\n",
    "Now go ahead and calculate the VIF values for the model’s predictors? Were you correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code lm model divedeeper_lm crime_rate ~ police_exp60 + mean_education + m_per1000f + prob_prison + police_exp59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn-by-building Module  \n",
    "As this is a graded task for our students, completion of the task is not optional and count towards your final score\n",
    "\n",
    "Write a regression analysis report applying what you've learned in the workshop. Using the dataset provided by you, write your findings on the different socioeconomic variables most highly correlated to crime rates (`crime_rate`). Explain your recommendations where appropriate. To help you through the exercise, you should ask the following questions of your candidate model:  \n",
    "\n",
    "- Can your model be any more simplified without losing substantial information?  \n",
    "- Have you tried predicting the crime rate given a reasonable set of values for the predictor variable?  \n",
    "- Have you identified any non-random pattern in your residual plot?\n",
    "\n",
    "Students should be awarded the full points if:  \n",
    "1. Student can select variable that is significant to both statistically and business-wise.\n",
    "2. Student perform regression linear assumption checking (Linearity, Multicolinearity, Heteroscedasticity, Normality)\n",
    "3. Student write the findings and explaining recommendations where appropriate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limitations of Regression Models\n",
    "-  Linear regressions are best fitted on data where a linear relationship between the predictor variables and target exist.  \n",
    "- Simple / Multiple regression models can be sensitive to outliers (recall the chapter regarding leverage and power)  \n",
    "- Simple / Multiple regression models assumes that the independent variables are not highly correlated with each other (hence using the `police_exp` to capture the information from both `police_exp60` and `police_exp59`)\n",
    "\n",
    "The limitations of the different machine learning models are something we will revisit soon and again as we progress in the machine learning specialization. Not only will we learn how to identify them early - we'll also learn various techniques to treat them, preventing overfitting / underfitting and making model diagnostic a critical part of your machine learning toolset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification in Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Objectives\n",
    "In this workshop, we'll extend our understanding of regression algorithms and see what we've learned in the previous workshop can be extended to solve a different kind of problems: classification problems. More specifically, we'll learn to solve binary and multi-class classification models using machine learning algorithms that are easily understood and in the case of logistic regression, readily interpretable. \n",
    "\n",
    "You will learn to develop classification algorithms from scratch, and investigate the mathematical foundations underpinning logistic regressions and nearest neighbors algorithms. My objective is to deliver a 9-hour session that is packed with the depth to help you develop, apply, score and evaluate two of the most highly versatile algorithms widely used today.\n",
    "\n",
    "- **Logistic Regression**\n",
    "- Understanding Odds  \n",
    "- Log of Odds  \n",
    "- Sigmoid Curve    \n",
    "- Logistic Regression in Practice  \n",
    "\n",
    "- **Nearest Neighbors Prediction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "### Theory\n",
    "Logistic regression is a classification algorithm used to fit a regression curve, $y = f(x)$, where $y$ is a categorical variable. When $y$ is binary (1 for spam, 0 for not-spam) we also call the model **binomial logistic regression** where in cases of $y$ assuming more than 2 values you'll sometimes hear the model being referred to as a class of **multinomial logistic regression**. We can think of logistic regression as a special case of linear regression (which you've mastered in the previous workshop), except we're using **log of odds** as our target variable. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relation to Probability\n",
    "So it's perhaps important to understand what odds mean. Most of us are familiar with **probabilities**. We understood that the **probability** of an event is the proportion of times it will occur divided by the total number of trials. If an event occurs 1 out of 5 times, then the probability (`p`) would be 1 out of 5, or 0.2. \n",
    "\n",
    "Odds are defined as the probability that an event will occur (`p`) divided by the probability than the event will not occur (we'll call it `q`, which is the same as `1-p`). If p is 0.2, we will see that q is 0.8. Expressed in a formula, odds can then be defined as:  \n",
    "$\\frac{p}{(1-p)}$\n",
    "\n",
    "Let's use a fun and real-life example. Supposed we were playing black jack (assuming the casino uses two decks on black jack) and the first card dealt is an Ace, the probability of the next card dealt to the dealer is a Ten is 31.07% (32 possible Tens out of 103 possibility). If we have to express it in odds and define p as 0.31, then our odds of the dealer being dealt a Blackjack (Ace + a Ten) is 0.31/(1-0.31), which brings it to 0.45 to 1. \n",
    "\n",
    "Note that if we have defined `p` as the probability of the Dealer **not having a Blackjack**, our odds would instead be 0.69/0.31, which brings us to 2.23 to 1. We can interpret this as \"for every 2.23 times the dealer didn't get a blackjack, she would get 1 blackjack\". Odds, as we so far understand it, refers to the ratio of favorable event (dealer doesn't get a blackjack) to the unfavorable event. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz 1: Odds of flying on time vs suffering a departure delay**\n",
    "\n",
    "Now to a less-fun but no less important example: airport delays. If I tell you that the probability of a minor departure delay occurring at a particularly busy airport (Soekarno-Hatta) on a festive holiday season is 0.2, what are the chances (expressed in odds) of you departing on time versus that of a departure delay. Recall the formula: \n",
    "\n",
    "$Odds = \\frac{No-delay}{Delay}$\n",
    "\n",
    "I hope you arrived at the right answer of 4 to 1, and intuitively interpret the situation as \"we are 4 times more likely to depart on time than to be delayed\". \n",
    "\n",
    "Odds are rather commonly used in some industry and in sports. In football and in horse racing, you'll often see betting odds expressed as fractions (e.g. 3/1 for a Germany win). In some academic writing or journalistic reporting, you may also see odds being expressed such as this: \"the relative risk of a credit event with Financial Product A over Product B is 1.125\". If you think about it, this is the same concept we've been talking about: odds. \n",
    "\n",
    "If it wasn't immediately clear, consider assigning some numbers to the above example:  \n",
    "- Financial Product A has a 0.45 empirical probability of incurring a credit event  \n",
    "- Financial Product B has a 0.4 empirical probability of incurring a credit event  \n",
    "\n",
    "The odds is hence 0.45/0.4, or 1.125:1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding log of odds\n",
    "When we have a probability $p$, the log of odds (sometimes called the \"log-odds\") is simply the log of the odds ratio, which is:  \n",
    "$log(p/(1-p))$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code of Menang.M <- 0.8/0.2\n",
    "# Menang.T <- 5/3\n",
    "# odds.ratio <- Menang.M/Menang.T\n",
    "# log(odds.ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Odds ratios as we observe above, are just an alternate way of expressing probabilities. Let's say we have the probability of success as 0.8, then the probability of failure is 1 - 0.8 = 0.2. The odds of success are defined as the probability of success over the probability of failure, in our case the odds would be .8/.2 = 4. We can also say that the odds of success is hence 4 to 1. If the probability of success is .5, i.e 50-50, our odds of success is 1 to 1.\n",
    "\n",
    "The transformation from probability to odds is a monotonic transformation, so the odds increases as the probability increase (however note that odds take a range of 0 to infinity):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code of log of odds curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we have an **odd of 1 when our p is 0.5**, and our odd is 4, when p is 0.8, just as we learned from the earlier example (50:50 -> odds of 1, success rate of 0.8 -> odds of 4). \n",
    "\n",
    "Now that we've understood the transformation from probability to odds, let's understand the transformation from odds to logs of odds. \n",
    "\n",
    "Log of odds are:\n",
    "$logit(p) = log(\\frac{p}{1-p})$\n",
    "\n",
    "Almost same code for the above curve, except this time we plot the curve of `log(x/(1-x))` instead of `(x/(1-x))`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code plot sigmoid curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change `x` below from 0.5 to 1, and then to 0 to verify that the log of odds can take any positive or negative value (which is to say, its range is -Inf to Inf). A linear model can produce any value of log of odds and they would be acceptable as a prediction as the range is -Inf to Inf. That is not the case if a linear model has to produce a prediction that is a valid value of \"probability\", because a probability only takes a range of 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code x <- 0.5\n",
    "# log(x/ (1-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the transformation of odds to log of odds is a monotonic one. The greater the odds, the greater the log of odds. However, recall that the probability of .5 will yield us a log-odds of 0. This is because the logit (log of odds) function takes values on [min, max] and transforms them to span [-Inf, Inf]. 5 is our median number and hence it's value on the log of odds scale is 0.\n",
    "\n",
    "The above sigmoid curve can also be plotted using the logit function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code of logit function or sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding logit function\n",
    "The logit function, formally defined, is expressed as:\n",
    "$y = log(\\frac{p}{1-p})$ where $p = \\frac{x - min}{max-min}$  \n",
    "\n",
    "In the case of a p=0.5 on a scale of 0 to 1, our *p* would then be p = ( 0.5 - 0 ) / (1 - 0) = 0.5; In the case of a p=30 on a scale of 1 to 100, our *p* would subsequently take on the value of (30-1)/(100-1) = 0.292929293.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code example to know about logit function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, however, that the logit function puts our probability on the x-axis instead of the y-axis and we can *invert* both axes also called the Sigmoidal **logistic function**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code plot sigmoid curve using invers logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could be wondering by now why we're concerned with understanding these underlying concepts? It turns out that the reason is surprisingly straightforward if we approach it from our prior knowledge of linear regression models.  \n",
    "\n",
    "Recall that with linear regression, we are used to representing our hypothesis in the following form:\n",
    "\n",
    "$\\hat{y} = \\beta_0 + \\beta_1x_1 + ... + \\beta_mx_m$  \n",
    "Where m is the number of predictors\n",
    "\n",
    "But with that hypothesis, our value $\\hat{y}$ could take on any value from *-Inf* to *Inf*. This is obviously not very helpful for our classification task. Ideally, we want:\n",
    "\n",
    "$0 \\leq \\hat{y} \\leq 1$  \n",
    "\n",
    "This is because we can then set a threshold value, say 0.5, and classify any examples above 0.5 as a \"positive\" and any value below it as a \"negative\". Turns out, we can transform a simple linear regression model $\\hat{y} = \\beta_0 + \\beta_1x_1$ by applying the sigmoid function, also known as the logistic function so we would end up with a hypothesis that bound our value to the range of 0 to 1:\n",
    "$\\hat{y}  = sigmoid( \\beta_0 + \\beta_1x_1)$\n",
    "- where $\\hat{y}$ = estimated probability that y=1 on input x.  \n",
    "\n",
    "More formally:\n",
    "$\\hat{y} = P(y=1 | x;\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional] Extra Proof: Intuition behind the sigmoid function  \n",
    "This sub-chapter sheds light on another perspective behind the sigmoid function, in the hope of helping you make sense of the sigmoid function a little more.\n",
    "\n",
    "Starting from a simple linear regression example with an independent variable called \"Age\" (imagine predicting income based on age), we would have the following hypothesis:\n",
    "$\\hat{y} = \\beta_0 + \\beta_{Age}$\n",
    "\n",
    "In logistic regression, since we are only concerned about the probability of our outcome (target), we need our hypothesis to be between 0 and 1:\n",
    "$0 \\leq \\hat{y} \\leq 1$\n",
    "\n",
    "Recall that we can think of $\\hat{y}$ simply as a probability of y being 1, we can denote it as $p$ for the purpose of convenience. Since probability must always be positive, we put this linear equation in exponential form, such that for any value of slope and dependent variable, exponent of this equation will never be negative:\n",
    "$p = exp(\\beta_0 + \\beta_{Age}) = e^{(\\beta_0 + \\beta_{Age})}$\n",
    "\n",
    "Exponenting something would make it an always positive value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.315287191035679e-07"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.exp(-14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've made the range our $p$ can take on 0 to positive infinity; We still have one task to do - we need to make our probability assume a range smaller than 1, essentially making it take on the range of 0 to 1. To make the probability lesser than 1, we will divide p by a number greater than p. \n",
    "\n",
    "> Divide 4 by 5 and get 0.8; or 4 by 20 and get 0.2, for an arithmetic proof  \n",
    "\n",
    "So, back to making p lesser than 1:  \n",
    "$p = exp(\\beta_0 + \\beta_{Age}) / exp(\\beta_0 + \\beta_{Age} + 1) )$\n",
    "\n",
    "The above equation is of course equivalent to:\n",
    "$e^{(\\beta_0 + \\beta_{Age})} / e^{\\beta_0 + \\beta_{Age}+ 1)}$\n",
    "\n",
    "Putting all of these together, we can now rewrite the probability as:\n",
    "p = e^z / (1 + e^z)\n",
    "\n",
    "Where p is the probability of success (y=1) and `z` is the placeholder for $\\beta_0 + \\beta_{Age}$. `q`, the probability of failure, will then be:\n",
    "q = (1 - p) = 1 - ( e^z / (1 + e^z ) )\n",
    "\n",
    "Recalling what we know about *odds*, we can now define our odds as:\n",
    "$\\frac{p}{1-p}$  \n",
    "\n",
    "Let's expand from the above equation:  \n",
    "$\\frac{p}{1-p}$  = $p * \\frac{1}{(1-p)}$  \n",
    "                 = $\\frac{e^z}{1+e^z} * \\frac{1}{1-\\frac{e^z}{1+e^z}}$  \n",
    "                 = $\\frac{e^z}{(1+e^z) - (\\frac{e^z * (1+e^z)}{1+e^z})}$  \n",
    "                 = $\\frac{e^z}{(1+e^z) - e^z}$  \n",
    "                 = $\\frac{e^z}{1}$  \n",
    "\n",
    "So from the above odds equation $\\frac{p}{1-p} = e^z$, we can take the log on both sides and obtain:  \n",
    "$log(\\frac{p}{1-p}) = z$\n",
    "\n",
    "After substituting z for the actual hypothesis in our earlier linear regression example, we arrive at:\n",
    "$log(\\frac{p}{1-p}) = \\beta_0 + \\beta(Age)$\n",
    "\n",
    "This, we learned earlier, is the equation used in logistic regression. It turns out that we arrive at the log of odds which we've studied in the previous section! \n",
    "\n",
    "Another important observation: realize that regardless of what value x takes, our probability of success (y=1) will always be on the range of 0 to 1. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Assumptions of Logistic Regression  \n",
    "Many of the key assumptions of linear regression do not hold true with logistic regression. We've learned about the linearity assumption, normality of residuals, and homoskedasticity assumptions in our regression models class - they do not apply in the case of logistic regression.\n",
    "\n",
    "Logistic regression **does not** require a linear relationship between the dependent and independent variables - it also does not assume normality of residuals nor is it concerned with the problem of heteroskedasticity the way that linear regressions are.\n",
    "\n",
    "However, a few of the assumptions do apply:  \n",
    "- Multicollinearity: Just as with the case of linear regression, logistic regression assumes little to no multicollinearity among the independent variables (recall how we used VIF to identify highly correlated variables in the last workshop)  \n",
    "- Independence of Observations: The observations should not come from repeated measurements and are independent from each other  \n",
    "- Linearity of predictor and log odds: While logistic regressions do not assume linearity between the dependent and independent variables, it does assume that the independent variables (predictors) are linearly related to the log odds.  \n",
    "\n",
    "The first two points are rather self-explanatory, and the third will be illustrated to you in an example later (flight delay prediction). If put slightly differently, the third point stresses that a logistic regression models the logit-transformed probability as a linear relationship with the predictor variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Logistic Regression\n",
    "Supposed you work in an education institution and are put in charge to evaluate the likelihood of a student graduating with a honors degree given their academic scores in a reading test, writing test and mathematics test.  \n",
    "\n",
    "This dataset has four features: `female`, `read`, `write`, `math` and the target variable is `hon`, a binary feature with 1 indicating that the student is in fact in an honors class and 0 indicating otherwise. The dataset is credited to the UCLA: Statistical Consulting Group (see credits for link and details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>female</th>\n",
       "      <th>read</th>\n",
       "      <th>write</th>\n",
       "      <th>math</th>\n",
       "      <th>hon</th>\n",
       "      <th>femalexmath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>52</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>68</td>\n",
       "      <td>59</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>33</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>44</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>52</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   female  read  write  math  hon  femalexmath\n",
       "0       0    57     52    41    0            0\n",
       "1       1    68     59    53    0           53\n",
       "2       0    44     33    54    0            0\n",
       "3       0    63     44    47    0            0\n",
       "4       0    47     52    57    0            0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "honor = pd.read_csv(\"data_input/sample.csv\")\n",
    "honor.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with no Predictor Variables\n",
    "\n",
    "In Python, fitting a logistic regression is done using the `LogisticRegression()` function. To fully understand logistic regression, let's begin by regressing `hon` with no predictor variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code makin logistic regression using hon~1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code manual to get coefficient log of odds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our odds ratio, without the influence of any predictor variable, is 49 out of 200 (49 in honors classes vs 151 not), so that give us a probability of 49/200, p = 0.245. Our odds ratio is therefore 0.245/(1-0.245) = 0.3245033\n",
    "\n",
    "Taking the log of 0.3245033, our log of odds is therefore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to get oods ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is essentially what our logistic regression model gave us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And... that's 0.245, which is the equivalent of 49/200 (probability of success!)\n",
    "\n",
    "If you need a refresher, recall that:  \n",
    "$p = \\frac{e^z}{1+e^z}$\n",
    "Or scroll back to line 200 to look at the mathematical details of the logit function.\n",
    "\n",
    "\n",
    "### Logistic regression with one discrete predictor variable\n",
    "\n",
    "Let's now add one binary predictor variable, **female** to the model, such that the equation for our model is formally described as:\n",
    "$logit(p) = \\beta_0 + \\beta_1 * female$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for logistic regression hon~female"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we attempt to interpret the parameters estimated from our model above, let's examine the odds ratio of a female being in a honors class as we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for odds ratio of female"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For males: odds of being in honors class = (17/91)/(74/91) = 0.2297297  \n",
    "- For females: odds of being in honors class = (32/109)/(77/109) = 0.4155844  \n",
    "- The ratio of the odds for female vs ratio of the odds for male = .42/.23 = 1.809, which is to say that the odds for female being in an honors class are about 81% more than that of their male counterpart  \n",
    "\n",
    "Let's now relate the odds ratio to the output from the logistic regression model with our `female` predictor variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code output summary model logreg hon~female"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intercept of **-1.4709** is the log odds for males since male is the reference group (**female** = 0). If we have wanted to confirm this, we can manually calculate this using the odds ratio for the male group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to get log of odds intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficient for **female** is the log of odds ratio between the female group and the male group, which can be manually calculated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to get log of odds female (coeff female)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using what we've learned earlier, we also know how easy it would be for us to calculate the odds ratio from the output of the model's summary: we simply have to exponentiate the coefficient it gives us for female. \n",
    "\n",
    "And if we were to relate this back to the original equation:\n",
    "$logit(p) = \\beta_0 + \\beta_1 * female$\n",
    "\n",
    "- For a male (female = 0): we would substitute the values into the equation and arrive at logit(p) = -1.4709  \n",
    "- For a female (female = 1): we would instead get logit(p) = -1.4709 + (0.5928*1) = -0.8781  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to get odds ratio of female (exp(-0.8781)/exp(-1.4709))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The ratio of the odds for female vs ratio of the odds for male = .42/.23 = 1.809, which is to say that the odds for female being in an honors class are about 81% more than that of their male counterpart  \n",
    "\n",
    "Notice how this is the same answer we derive from our manual calculation even before looking at the output of our logistic regression model. In fact, we could as well have taken the **estimated coefficient** value for `female`, which the output says is 0.5928, and get its exponent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code using exp (coef )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression with one continuous predictor variable\n",
    "Let's try another exercise, this time using the `math` score (continuous variable) such that the equation for our model is formally described as:\n",
    "\n",
    "$logit(p) = \\beta_0 + \\beta_1 * math$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code glm between hon~math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice in the case of a continuous variable such as the math score, our estimated coefficient for the intercept is the log odds of a student with a math score of zero being in an honors class. If we mentally visualize a plot with both x and y axis, this makes intuitive sense: the intercept points to the value of y **when our x feature = 0**. By taking the exponent of this value, we then know the odds of such student being in an honors class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code value of exp intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These odds are very low, and a peek at the distribution for the variable math will reveal that no one in the sample has a math score lower than 30 (mean of 53 in fact), which tells us that the intercept in this model corresponds to the log odds of being in an honors class when math is at the hypothetical value of zero.\n",
    "\n",
    "How do we interpret the coefficient for math? Recall our equation:\n",
    "\n",
    "$logit(p) = log(p/(1-p)) = \\beta_0 + \\beta_1 * math$\n",
    "\n",
    "With the substituted values:\n",
    "logit(p) = -9.79394 + 0.15634 * math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code hist of math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The median of math is ~52. Let's assume a `math` value of 52:\n",
    "logit(p) = -9.79394 + 0.15634 * 52 = -1.66426\n",
    "\n",
    "Examine the effect of a one-unit increase in math score, at 53:\n",
    "logit(p) = -9.79394 + 0.15634 * 53 = -1.50792\n",
    "\n",
    "Taking the difference:\n",
    "-1.50792 - (-1.66426) = 0.15634"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code of math coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and it is! So the coefficient for `math` is in fact the difference in the log odds for one unit of increment in that variable (math score of 53 vs 52). In simpler words, for one-unit increase in the math score, the expected change in log odds is 0.15634.\n",
    "\n",
    "Like the earlier example, we could also translate this change in log odds to the change in odds by exponentiating the log-odds:\n",
    "\n",
    "Change in Odds  = odds(math=53) / odds(math=52)  \n",
    "                = exp(-1.50792) / exp(-1.66426)  \n",
    "                = odds (difference in one-unit increase)  \n",
    "                = exp(0.15634)  \n",
    "                = 1.169224  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code exp of math coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interpret this as: for a one-unit increase in math score, we expect to see ~17% increase in the odds of being in an honors class. This 17% does not depend on the value that math is held at. It's also important to note that a 17% increase in odds is not the same as a 17% increase in probability. All it is saying that compared to a score of 52, scoring 53 will improve the odds of being in an honors class by 1.17 times.\n",
    "\n",
    "### Logistic regression with multiple predictor variables and no interaction terms\n",
    "In general, we can have multiple predictor variables in a logistic regression model:\n",
    "logit(p)        = log(p/(1-p))  \n",
    "                = $\\beta_0 + \\beta1 * x1 + ... + \\beta_k *xk$  \n",
    "                \n",
    "Applying such a model to our example dataset, each estimated coefficient is the expected change in the log odds of being in an honors class **for a one-unit increase in the corresponding predictor variable** holding the other variables constant at a certain value. Each exponentiated coefficient is the ratio of two odds, or the change in odds in the multiplicative scale for a one-unit increase in the corresponding predictor variable holding other variables at a certain value. Let's look at the following equation:\n",
    "\n",
    "$logit(p) = \\beta_0 + \\beta_1 * math + \\beta_2 * female + \\beta_3 * read$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code of glm hon~math+female+read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficient for *math* tells us that, holding *female* and *reading* at a fixed value, we will see a 13% increase in the odds of graduating with honors class for a one-unit increase in math score since exp(.12296) = 1.13. \n",
    "\n",
    "Can you attempt to interpret the above model and answer the following question?\n",
    "\n",
    "- Holding Female and Mathematics score constant, a one-unit increase in reading score improves the odds of graduating with honors by how much?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code exp of reading coeff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Example: Predicting Flight Delay\n",
    "Let's take a look at what happened when we try to predict flight delays using a logistic regression models where the predictor variables are `Month`, `DayofMonth`, and `DayofWeek` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>DayofMonth</th>\n",
       "      <th>DayofWeek</th>\n",
       "      <th>Carrier</th>\n",
       "      <th>CRSDepTime</th>\n",
       "      <th>DepDel15</th>\n",
       "      <th>CRSArrTime</th>\n",
       "      <th>OriginState</th>\n",
       "      <th>DestState</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>DL</td>\n",
       "      <td>1539</td>\n",
       "      <td>0</td>\n",
       "      <td>1824</td>\n",
       "      <td>FL</td>\n",
       "      <td>NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>WN</td>\n",
       "      <td>1400</td>\n",
       "      <td>1</td>\n",
       "      <td>1425</td>\n",
       "      <td>PA</td>\n",
       "      <td>IL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>AS</td>\n",
       "      <td>810</td>\n",
       "      <td>0</td>\n",
       "      <td>1614</td>\n",
       "      <td>WA</td>\n",
       "      <td>DC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>OO</td>\n",
       "      <td>804</td>\n",
       "      <td>0</td>\n",
       "      <td>1027</td>\n",
       "      <td>IL</td>\n",
       "      <td>OH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>DL</td>\n",
       "      <td>805</td>\n",
       "      <td>0</td>\n",
       "      <td>1117</td>\n",
       "      <td>NY</td>\n",
       "      <td>FL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year  Month  DayofMonth  DayofWeek Carrier  CRSDepTime  DepDel15  \\\n",
       "0  2013      9          16          1      DL        1539         0   \n",
       "1  2013      9          23          1      WN        1400         1   \n",
       "2  2013      9           7          6      AS         810         0   \n",
       "3  2013      7          15          1      OO         804         0   \n",
       "4  2013      5          16          4      DL         805         0   \n",
       "\n",
       "   CRSArrTime OriginState DestState  \n",
       "0        1824          FL        NY  \n",
       "1        1425          PA        IL  \n",
       "2        1614          WA        DC  \n",
       "3        1027          IL        OH  \n",
       "4        1117          NY        FL  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight = pd.read_csv(\"data_input/flight_sm.csv\")\n",
    "flight.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code build model glm DepDel15 ~ Month + DayofMonth + DayofWeek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a problem with the above logistic regression model: Can you tell which among the three key assumptions did it violate?\n",
    "- Multicollinearity  \n",
    "- Independence of Observations  \n",
    "- Linearity of predictor and log odds  \n",
    "\n",
    "### Application of Logistic Regression\n",
    "In the field of market research where its commonplace for business analysts to try and get as accurate as possible a prediction of a new product launch (success/failure), a new bundle pricing strategy (odds of success / odds of failure), or a new enrollment plan, logistic regression and its accompanying analysis plays a pivotal role. An example of this is the scenario of a company that is estimating the change of probability / odds of customer buy-in for every $1 dollar change in price. Another example of this is in election forecasts: where a campaign manager is trying to determine the odds of a likely voter to vote for a particular candidate, using demographic parameters such as gender, age, and education level. \n",
    "\n",
    "Another common use of logistic regression in business is in building models of customer retention, which can offer incredible insights into why some customers leave and others stay (drivers of customer retention). This is particular important in certain industries, where reducing customer defections by as little as five percent can double profits (Reichheld, 1996[^1])\n",
    "\n",
    "Interesting weekend read: Another interest project that models customer retention using historical data from a database (more than 500,000 clients) of a big mutual fund investment company and logistic regression (Eiben, Euverman, Kowalczyk, Slisser[^2]), which highlight the benefits of an interpretative model like the one we obtain with logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet another example is in Credit Risk Analysis, where machine learning is deployed to estimate probability of defaults (or in the measurement of other types of credit risk). The paper described how loan officers at bank use logistic regression \"to identify characteristics that are indicative of people who are likely to default on loans, and then use those characteristics to discriminate between good and bad credit risks\"[^4]. \n",
    "\n",
    "A quick summary of the findings:  \n",
    "- Number of years at current employment and number of years at current address have negative coefficients, indicating that customers who have spent less time at either their current employer or their current address are more likely to default  \n",
    "- Debt-to-income ratio (`dti`, a measurement we'll use in our project later) and amount of credit card debt both have positive coefficients, indicating that higher dti ratios or higher amounts of credit card debts are both associated with a greater likelihood of loan defaults.  \n",
    "\n",
    "### Credit Risk Analysis / Modeling: Loans from Q4 2017\n",
    "I've prepared the following data originally made available by [LendingClub](https://www.lendingclub). Some preprocessing steps have been applied to save you from the \"data cleansing\" work. We'll read the data into our workspace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>initial_list_status</th>\n",
       "      <th>purpose</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>dti</th>\n",
       "      <th>verification_status</th>\n",
       "      <th>grade</th>\n",
       "      <th>revol_bal</th>\n",
       "      <th>inq_last_12m</th>\n",
       "      <th>delinq_2yrs</th>\n",
       "      <th>home_ownership</th>\n",
       "      <th>not_paid</th>\n",
       "      <th>log_inc</th>\n",
       "      <th>verified</th>\n",
       "      <th>grdCtoA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>w</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>14.08</td>\n",
       "      <td>675.99</td>\n",
       "      <td>156700.0</td>\n",
       "      <td>19.11</td>\n",
       "      <td>Source Verified</td>\n",
       "      <td>C</td>\n",
       "      <td>21936</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>0</td>\n",
       "      <td>11.962088</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>9.44</td>\n",
       "      <td>480.08</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>19.35</td>\n",
       "      <td>Not Verified</td>\n",
       "      <td>B</td>\n",
       "      <td>5457</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>RENT</td>\n",
       "      <td>1</td>\n",
       "      <td>10.819778</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>w</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>28.72</td>\n",
       "      <td>1010.30</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>65.58</td>\n",
       "      <td>Verified</td>\n",
       "      <td>F</td>\n",
       "      <td>23453</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>OWN</td>\n",
       "      <td>1</td>\n",
       "      <td>10.126631</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>w</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>13.59</td>\n",
       "      <td>484.19</td>\n",
       "      <td>175000.0</td>\n",
       "      <td>12.60</td>\n",
       "      <td>Not Verified</td>\n",
       "      <td>C</td>\n",
       "      <td>31740</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>1</td>\n",
       "      <td>12.072541</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>w</td>\n",
       "      <td>major_purchase</td>\n",
       "      <td>15.05</td>\n",
       "      <td>476.33</td>\n",
       "      <td>109992.0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>Not Verified</td>\n",
       "      <td>C</td>\n",
       "      <td>2284</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>0</td>\n",
       "      <td>11.608163</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  initial_list_status             purpose  int_rate  installment  annual_inc  \\\n",
       "0                   w  debt_consolidation     14.08       675.99    156700.0   \n",
       "1                   f  debt_consolidation      9.44       480.08     50000.0   \n",
       "2                   w  debt_consolidation     28.72      1010.30     25000.0   \n",
       "3                   w  debt_consolidation     13.59       484.19    175000.0   \n",
       "4                   w      major_purchase     15.05       476.33    109992.0   \n",
       "\n",
       "     dti verification_status grade  revol_bal  inq_last_12m  delinq_2yrs  \\\n",
       "0  19.11     Source Verified     C      21936             3            0   \n",
       "1  19.35        Not Verified     B       5457             1            1   \n",
       "2  65.58            Verified     F      23453             0            0   \n",
       "3  12.60        Not Verified     C      31740             0            0   \n",
       "4  10.00        Not Verified     C       2284             3            0   \n",
       "\n",
       "  home_ownership  not_paid    log_inc  verified  grdCtoA  \n",
       "0       MORTGAGE         0  11.962088         1        0  \n",
       "1           RENT         1  10.819778         0        1  \n",
       "2            OWN         1  10.126631         1        0  \n",
       "3       MORTGAGE         1  12.072541         0        0  \n",
       "4       MORTGAGE         0  11.608163         0        0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loan = pd.read_csv(\"data_input/loan2017Q4.csv\")\n",
    "loan.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable of interest is the `not_paid` variable, a binary variable that indicate whether a loan is fully paid or not. A loan is considered \"not paid\" (not paid = 1) when it is **Defaulted**, **Charged Off**, or past due date (**Grace Period**). To prevent one class from dominating the other, the data I've prepared here over-sampled more \"bad\" loans so that the underlying characteristics of the empirically minority class is adequately represented.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code table of not paid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's important to note is that logistic regression is not susceptible to a \"class imbalance\" problem per-se, and an unbalanced class representation is for the most part dealt with as sample size grows anyway. That said, in the situation of highly imbalanced class representation, the patterns within the minority class may not be sufficiently \"described\" and in the case of an extreme imbalance you may be better off using an \"anomaly detection\" approach than through a classification approach.\n",
    "\n",
    "In the Unsupervised Machine Learning workshop within the Machine Learning Specialization, I will delve into the specific details of anomaly detection algorithms with far greater depth so let's stay on track and study the dataset we've just read into our environment:  \n",
    "- `initial_list_status`: Either `w` (whole) or `f` (fractional). This variable indicates if the loan was a whole loan or fractional loan. For background: Some institutional investors have a preference to purchase loans in their entirety to obtain legal and accounting treatment specific to their situation - with the added benefit of \"instant funding\" to borrowers  \n",
    "- `purpose`: Simplified from the original data; One of: `credit_card`, `debt_consolidation`, `home_improvement`, `major_purchase` and `small_business`  \n",
    "- `int_rate`: Interest rate in percentages  \n",
    "- `installment`: Monthly payment owed by the borrower  \n",
    "- `annual_inc`: Self-reported annual income provided by the borrower / co-borrowers during application  \n",
    "- `dti`: A ratio of the borrower's total monthly debt payments on his/her total obligations to the self-reported monthly income  \n",
    "- `verification_status`: is the reported income verified, not verified, or if the income source was verified  \n",
    "- `grade`: software-assigned loan grade  \n",
    "- `revol_bal`: total credit revolving balance (in the case of credit card, it refers to the portion of credit card spending that goes unpaid at the end of a billing cycle)  \n",
    "- `inq_last_12m`: number of credit inquiries in the last 12 months  \n",
    "- `delinq_2yrs`: number of 30+ days past-due incidences of delinquency in the borrower's credit file for the past 2 years  \n",
    "- `home_ownership`: one of `MORTGAGE`, `OWN` and `RENT`  \n",
    "- `not_paid`: 1 for fully-paid loans, 0 for charged-off, past-due / grace period or defaulted  \n",
    "- `log_inc`: log of `annual_inc`  \n",
    "- `verified`: 0 for \"Not verified\" under `verification_status`, 1 otherwise  \n",
    "- `grdCtoA`: 1 for a `grade` of A, B or C, 0 otherwise\n",
    "\n",
    "Before we dive into building our classification model, I'd like to encourage you to spend some time on the \"exploratory phase\". This is the phase where you investigate the relationships and discover rough structures of the data. You can use `summary()`, or `fivenum()`, or even `cor()` - take your time to write a few more lines of code below this chunk and be curious about your data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code of data exploratory of loan data between not_paid column with dti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation and Out-of-Sample Error\n",
    "Before we develop our classification model, I'll introduce you to the idea of estimating the accuracy of our model. Simply put, we are going to:  \n",
    "- Split our dataset into train and test sets  \n",
    "- Build our machine learning model using data **only** from our train set  \n",
    "- Obtain an unbiased measurement of the model's accuracy by predicting on test set  \n",
    "\n",
    "A related idea is known as **cross-validation**, in which we:  \n",
    "- Split our dataset into train, cross-validation, and test sets  \n",
    "- Develop the initial model using our train set  \n",
    "- Evaluate model on cross-validation set(s), returning to the previous step if necessary (say, pick different predictor variables, use a different parameter, or to tune other aspects of the model specification)  \n",
    "- Pick a final model based on an evaluation criteria (Adj.R-squared, accuracy, etc)  \n",
    "- Obtain an unbiased measurement of the model's accuracy by predicting on test set  \n",
    "\n",
    "We can repeat step(2) and step(3) as much as is necessary, testing out different algorithms or model specification, or combinations of predictor variables and pick a final model on which we will obtain our estimated accuracy by testing it on the test set. An important rule on this is that the **test set must not be used in any of the steps before the (5)**, such that the accuracy we obtain is an unbiased measurement of the out-of-sample accuracy of the model. \n",
    "\n",
    "The idea of obtaining an unbiased estimate of our model's out-of-sample performance is an important one as it is often the case that the in-sample error (the error you obtain from running your algorithm on the dataset it was trained on) is optimistic and tuned / adapted in a particular way to minimize the error in the training sample. Therefore - the in-sample error is not a good representation or indication of how our model will perform when it is applied on unseen data. \n",
    "\n",
    "Another way to think about is that our training data has two components to it: signal and noise. The goal of machine learning is to identify the signal but be robust enough to avoid modeling the \"noise\" component of the data. When we build a model, we want to know that our model is not overly adapted to the data set to the point that it captures both the signal and noise, a phenomenon known as \"overfitting\". When our model is guilty of overfitting, the in-sample accuracy will be very high (in some cases ~100%) but fail to perform on unseen data. The idea is to strike the right balance between accuracy (don't underfit) and robustness to noise (don't overfit).  \n",
    "\n",
    "### Predicting Credit Risk from Loans\n",
    "Applying what we've learned above, we'll split our data into the `loans.train` and `loans.test` set. I'll first show you this approach and later show you the cross-validation approach - I encourage you to follow this part of the workshop closely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code cross validation of loan data to be train and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already know how to build a binomial logistic regression and learned the \"manual\" way of obtaining those coefficients in previous sections. Here we'll cut to the chase and use `LogisticRegression()` for our model construction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code of glm not_paid ~ verified + purpose + installment + int_rate + home_ownership + grdCtoA + annual_inc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code of summary model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe from the model summary that holding other variables constant, obtaining an assigned grade of A to C reduce the log-odds (because it's a negative coefficient) of a loan default; Now let's use the `predict()` function, specifying the:  \n",
    "- Model to be used for prediction (`creditrisk`)  \n",
    "- Dataset on which the model should predict (`loans.test`)\n",
    "- A response type. The default `link` is on the scale of the linear predictors (log-odds) but we'll specify `response` so the prediction is on the scale of the response variable (which means: probabilities). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code of predict value from model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a look at the `pred.Risk` variable we appended to our `loans.test` dataframe: we can therefore set a \"risk\" threshold, say, at 0.5 and predict any loans that exceed that threshold as a \"default=1\". 0.5 may not always be the right threshold setting and we'll discuss that later in the section describing \"precision\" vs \"recall\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code of loans.test[1:10, 15:17]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Prediction Output\n",
    "As an exercise, are you able to append yet another variable (column) to the above dataframe. Name it `pred.not_paid` and make sure it's a binary (0 or 1). You can use `ifelse` for this task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code of if else condition when pred.Risk > 0.5 will classify in 1 else 0\n",
    "# code cross tabulation between predicted and actual data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table above is also known as the **confusion matrix**. \n",
    "\n",
    "Observe from the confusion matrix that: \n",
    "- Out of the 151 actual defaults we classified 97 of them correctly  \n",
    "- Out of the 161 fully-paid loans we classified 93 of them correctly  \n",
    "- Out of the 312 cases of loans in our test set, we classified 190 of them correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Classifiers: Sensitivity, Specificity and Precision\n",
    "Sensitivity and specificity are metrics commonly used to measures the performance of a binary classification.  \n",
    "\n",
    "- Sensitivity (also called the true positive rate, the **recall**, or probability of detection in some fields) measures the proportion of positives that are correctly identified as such (cancer cell detection, email spam, insurance fraud etc)  \n",
    "- Specificity (also called the true negative rate) measures the proportion of negatives that are correctly identified as such (e.g. the percentage of healthy people who are correctly identified as not having the condition, legitimate emails identified as such, legitimate insurance claims)  \n",
    "- Precision: Proportion of correctly identified positives from all classified as such  \n",
    "- Accuracy: Proportion of correctly identified cases from all cases \n",
    "\n",
    "![Source: Wikipedia](assets/sensitivity.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the confusion matrix, can you describe the precision, recall, and accuracy of our model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to find sensitivity, specificity, precision, and accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, you'll also find machine learning applications that uses the notion of a baseline measure in their model evaluation phase. The baseline performance is used to quantify the improvement of an applied solution to the problem and a **base rate** is just the accuracy of trivially predicting the most-frequent (or majority) class. An implementation of this is the ZeroR Classifier found in many data mining applications or related domains: since it ignores all predictors, ZeroR ends us classifying according to the prior probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code of prop table of not paid in test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our baseline rate is 0.51 - a classifier that does no better than 0.51 is not useful because we might as well have classify every class to the majority! \n",
    "\n",
    "False negatives and false positives are rarely equally costly to a business (or really, to any domain). For an insurance company, a false negative on an insurance payout is likely to cost the company more than a false positive for example. Finding the right precision-recall tradeoff comes with domain expertise - and let's make all of these more concrete by extending our credit risk example above.\n",
    "\n",
    "Say the bank's credit department would rather sacrifice some level of specificity or precision in favor of higher recall (or sensitivity). In simpler words, we want to be more sensitive to \"loan defaults\", how would you go about doing that? Try and think critically of the problem before scrolling down to the proposed solution.\n",
    "\n",
    "Well, one thing we can do is to set the threshold to be more sensitive to \"positive cases\": Let's see what happen if we were to predict a \"default\" when the probability exceed 0.4 (20% more sensitive than our previous classifier): \n",
    "\n",
    "```{r}\n",
    "table(\"predicted\"=as.numeric(loans.test$pred.Risk>=0.4), \"actual\"=loans.test$not_paid)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code fot table confusion matrix when pred.Risk >= 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We increased our Sensitivity or Recall rate from 0.64 to above 0.85! What is the cost of such an adjustment?\n",
    "\n",
    "### Performance evaluation and model selection\n",
    "On top of what we've learned so far, there are other tools we can use to evaluate and compare between the performances of our regression models:\n",
    "\n",
    "**1. AIC (Akaike Information Criteria)**  \n",
    "Like R-squared, AIC is a statistical measure of how close the data are to the fitted regression line. It gives us a measure of fit: we'll therefore choose the model with the lowest AIC value, as it helps us minimize residual error in our model:\n",
    "\n",
    "However, unlike adjusted R-squared, the number itself is not meaningful. If you have more than one similar candidate models (where all of the variables of the simpler model occur in the more complex models), then you should select the model that has the smallest AIC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for find AIC from model1 to model4 in honor data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Null Deviance and Residual Deviance** \n",
    "Null deviance indicates how well the response variable is predicted by a model that includes only the intercept (grand mean). The residual deviance shows how well the response variable is predicted by the model when all predictors are included.\n",
    "\n",
    "Intuitively, this means our first model - with no predictor variables **`glm(hon ~ 1)`** should see the same null deviance and residual deviance, and it is (table below)! As we add more / subtract predictor variables, notice how our residual deviance change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to find null deviance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice in the last model `honors.logit5`, we obtain an unrealistically small residual deviance and coefficients that are confusingly large. This is indicative of the \"Hauck Donner effect\": This is when the fitted probabilities are extremely close to zero or one. Consider a medical diagnosis problem with thousands of cases and around 50 binary explanatory variable (which may arise from coding fewer categorical variables); one of these indicators is rarely true but always indicates that the disease is present. Then the fitted probabilities of cases with that indicator should be one, which can only be achieved by taking $\\beta_i$ = Inf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code of exp coeff honor.logit[5]\n",
    "# Infinity minus any arbitarily large numbers still exceed 0.5\n",
    "# Inf-1000000 > 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More intuition:**\n",
    "This phenomenon where one or more predictors take on a coefficient value of infinity is sometimes referred to as _perfect separation_. Another example: imagine the scenario where reading >= 43 will perfectly predict honors=TRUE and reading < 43 will perfectly predict honors=FALSE, or where stock_opening >= 34 will perfectly predict end_high = TRUE and vice versa, then we can imagine that the probabilities where such cases do happen must be 1, and that is achieved by setting the coefficient to infinity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbour Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The k-nearest neighbor algorithm gets it name from the fact that it uses information about an example's k-nearest neighbors to classify unlabeled examples. Upon choosing _k_, the algorithm requires a training dataset made up of examples that have been classified into several categories, as labeled by a nominal variable. Then, for each unlabeled record in the test dataset, k-NN identifies _k_ records in the training data that are the \"nearest\" in similarity. The unlabeled test instance is assigned the class of the majority of the k-nearest neighbors.  \n",
    "\n",
    "Supposed we pick k=1, then the * in the following feature space will be assigned the square class, but if k=5, then the majority class of the five nearest point will be assigned to that point and our point will be classified as a round instead.\n",
    "\n",
    "![ ](assets/knn.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivational Example: Is Tomato a fruit?\n",
    "Suppose that prior to a blind tasting (or blind dining) experience, we created a dataset in which we recorded our impressions of a number of ingredients. For each ingredient, we rated its `sweetness` and `crunchiness` and then labeled them as one of the three types of food: *fruits*, *vegetables*, or *proteins*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code of create data set of food\n",
    "# food <- data.frame(list(c(\"apple\", \"bacon\", \"banana\", \"carrot\",\"celery\", \"cheese\",\"cucumber\", \"fish\", \"grape\", \"green bean\", \"lettuce\", \"nuts\", \"pear\", \"shrimp\",\"orange\"), c(10,1,10,6,3,1,2,3,10,3,1,3,10,2,9), c(9,4,1,10,10,1,8,2,5,7,10,5,7,2,3), c(\"fruit\", \"protein\", \"fruit\", \"vegetable\", \"vegetable\", \"protein\", \"vegetable\", \"protein\", \"fruit\", \"vegetable\", \"vegetable\", \"proteins\", \"fruit\",\"protein\", \"fruit\")))\n",
    "\n",
    "# Give each feature appropriate names\n",
    "# colnames(food)<- c(\"Ingredient\", \"Sweetness\", \"Crunchiness\", \"Type\")\n",
    "# food"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The k-NN algorithm treats the features as coordinates in a multidimensional feature space. As our dataset includes only two features, the feature space is two-dimensional. We can plot two-dimensional data on a scatterplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code of plotting food data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe here that similar types of food tend to be grouped closely together.\n",
    "\n",
    "Supposed we'd like to decide if tomato is a fruit or a vegetable, we would put tomato onto our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to put tomato in visualization\n",
    "# grob = grobTree(textGrob(\"tomato\", x=0.6, y=0.4, hjust=0, gp=gpar(col=\"darkorange\", fontsize=14)))\n",
    "\n",
    "# plot.fruit + annotation_custom(grob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that we would then use a *distance function* to find tomato's nearest neighbors. Traditionally, the k-NN algorithm assumes *Euclidean distance*, which is the shortest direct route (imagine using a ruler to connect two points). While Euclidean distance function is the most widely used distance metric in k-NN, you will sometimes see the Manhattan distance (which is based on the paths a pedestrian would take by walking city blocks) being used instead [^5]. \n",
    "\n",
    "A few academic papers on this literature may also reference the Minkowsky distance function[^6]:  \n",
    "dist_Minkowsky(A,B) = $(\\sum\\limits^m_{i=1} |x_i -y_i|^r)^{\\frac{1}{r}}$  \n",
    "\n",
    "While these distance functions exist, Euclidean distance is far more often seen in industrial applications and is therefore the focus of this chapter. As a side note, Minkowsky distance is typically used with _r_ being 1 or 2, where the former is equivalent to the Manhattan distance while the latter is the Euclidean distance.  \n",
    "\n",
    "### Euclidean Distance  \n",
    "Let A and B be represented by feature vectors A = ($x_1, x_2, …, x_m$) and B = ($y_1, y_2, …, y_m$), where _m_ is the dimensionality of the feature space. To calculate the distance between A and B, the Euclidean Distance formula can be represented as such:\n",
    "\n",
    "dist(A, B) = $\\sqrt{\\sum\\limits^{m}_{i=1}(x_i-y_i)^2}$\n",
    "\n",
    "Applying the above formula on our blind-tasting example, we can calculate the distance between:  \n",
    "- tomato (sweet: 6, crunchy: 4)  \n",
    "- green bean (sweet: 3, crunchy: 7)\n",
    "\n",
    "dist(tomato, greenbean) = `sqrt((6-3)^2 + (4-7)^2))`, which is 4.24\n",
    "\n",
    "Similarly, we can calculate the distance between the tomato and several of its closest neighbors. Supposed we've done that and choose to assign tomato the food type of its nearest neighbor, which in our case is the orange (distance: 1.4), we are doing what is formally a 1-NN classification. Under 1-NN then the orange would be classified as a fruit.\n",
    "\n",
    "Had we use the k-NN with 3 nearest neighbor instead: orange, grape, and nuts, the majority vote (2 fruits vs 1 protein) would again classify the tomato as a fruit. \n",
    "\n",
    "### Choosing an appropriate *k*\n",
    "The decision of how many neighbors to use for k-NN determines how well the model will generalize to future data. The balance between overfitting and underfitting the training data is a problem known as **bias-variance tradeoff**. Choosing a large k reduces the impact or variance caused by noisy data, but can bias the learner so that it runs the risk of ignoring small, but important patterns.\n",
    "\n",
    "If we use a very large *k*, say, a *k* value as large as the total number of observations in the training data, this would lead to the always predicting the majority class (ZeroR classifier), which we've learned about in the previous chapter.\n",
    "\n",
    "On the opposite extreme, using a single nearest neighbor allows the noisy data or outliers to unduly influence the classification of examples. If one of our training examples were accidentally mislabeled and happens to be a neighboring data point, choosing a k=1 will have resulted in a misclassification, even if the nine other nearest neighbors would have voted differently.\n",
    "\n",
    "In practice, one common strategy is to begin with *k* equal to the square root of the number of training examples. Another strategy is to choose a larger k but apply a weighted voting process in which the vote of the closer neighbors is considered more authoritative than the vote of the farther away neighbors.\n",
    "\n",
    "### Features rescaling\n",
    "Supposed, in addition to Sweetness and Crunchiness, we add a new feature \"Spiciness\" which is measured on a scale of 0 to 10,000. This range, or difference in scale, will allow the spice level of a food to have an amplified impact on the distance function. In fact, it's enlarged contribution to the distance function may end up being the singular decisive feature! \n",
    "\n",
    "We solve this by rescaling the features, i.e shrinking or expanding their range so that each feature's contribution to the distance formula is equally weighed. We want spiciness to be measured on the same scale as sweetness and crunchiness, which is a scale from 1 to 10. The two methods of rescaling features are: \n",
    "\n",
    "- Mix-Max normalization  \n",
    "- z-score standardization  \n",
    "\n",
    "**Min-max normalization** works by transforming a feature such that its values fall into a range of 0 to 1. \n",
    "\n",
    "The formula: $x_{new}$ = `(x-min(x)) / (max(x) - min(x))`  \n",
    "\n",
    "- Which essentially subtracts the min of feature *x* from each value and divides by the range of *x*.\n",
    "\n",
    "Normalized feature's values effectively communicates how far, in percentage terms, the original value fell along the range of all values of feature *x*.\n",
    "\n",
    "**z-score standardization** on the other hand subtracts the mean value of feature *x* and divides the outcome by the standard deviation of *x*.  \n",
    "\n",
    "The formula: $x_{new}$ = `(x-mean(x))/sd(x)`  \n",
    "\n",
    "Standardization rescales each of the feature's values in terms of how many standard deviations they fall above or below the mean values. The resulting value is called a *z-score*. Z-scores has no predefined bounds (minimum and maximum) and may be negative or positive numbers. A more detailed discussion of this is in the Practical Statistics coursebook you have received in an earlier workshop.\n",
    "\n",
    "### Characteristics of k-NN\n",
    "Classification methods using k-NN are called 'lazy learners'. Lazy learners do not build a model; There is no abstraction or generalization process -- compare this to the logistic regression method we've learned earlier to have an intuition of what 'building a model' means. More technically, we say that no 'parameters' are learned about the data.\n",
    "\n",
    "Let's summarize the process that goes into prediction with a k-NN classifier:  \n",
    "- Scaling (putting the variables on a same scale to avoid one variable overpowering the others)  \n",
    "- Select a positive integer *k*  \n",
    "- Select the _k_ nearest neighbor for each \"test\" sample  \n",
    "- Classify based on majority class\n",
    "\n",
    "Because k-NN makes prediction in a manner that is \"just-in-time\" by calculating the similarity between each input sample and the other training samples in the vector space, this method may be computationally expensive on dataset with high dimensionality (high memory requirement and constantly calculating \"distances\" over and over again). If we pick a small *k* value, our algorithm may also be vulnerable to the \"noise\" in our data. On its own, it is also sensitive to the \"scale\" of our data. \n",
    "\n",
    "Despite the limitations, k-NN is incredibly powerful and versatile. In fact some of its weaknesses (such as the outlier and scales) can be adequately mitigated with the scaling strategy we've learned in the earlier section. It is also generally insensitive to outlier and noise when an appropriate *k* value is picked. Unlike logistic regression or linear regression, it works well on non-linear data because k-NN does not make assumption about the data.  \n",
    "\n",
    "Under specific settings and requirements, k-NN is some of the most extensively used algorithms and have impressive accuracy. \n",
    "\n",
    "An example of Nearest Neighbor being used in performance benchmarking by the Microsoft's Kinect team:\n",
    "![Real-Time Human Pose Recognition in Parts from Single Depth Images](assets/kinect.png)\n",
    "Read: http://research.microsoft.com/pubs/145347/BodyPartRecognition.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a k-NN from scratch: Classifying customers by industry segment  \n",
    "Both in the regression models class and in our logistic regression classes, we've learned how to obtain the coefficients and constructing the model manually (from mathematical principles / without the use of \"libraries\"). In this section, I'd like to demonstrate how we can also develop our own classifier from the mathematical principles behind the k-NN algorithm. \n",
    "\n",
    "Imagine you're employed at a particular conglomerate distributing FMCG goods through a distribution network consisting of hotel, restaurant, cafes, and all variety of retail outlets. Our CRM system collected the annual spending in each of the product category for each of the customer, and we'd like to build an algorithm that automatically sort our customers into one of two segments:  \n",
    "- Horeca: Short for Hotel, Restaurant and Cafe  \n",
    "- Retail: Retail industry  \n",
    "\n",
    "You are provided some training datasets as part of the task. We would borrow from a dataset prepared by Margarida Cardoso and [available on the UCI Machine Learning repository](https://archive.ics.uci.edu/ml/datasets/Wholesale+customers):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel</th>\n",
       "      <th>Region</th>\n",
       "      <th>Fresh</th>\n",
       "      <th>Milk</th>\n",
       "      <th>Grocery</th>\n",
       "      <th>Frozen</th>\n",
       "      <th>Detergents_Paper</th>\n",
       "      <th>Delicassen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>12669</td>\n",
       "      <td>9656</td>\n",
       "      <td>7561</td>\n",
       "      <td>214</td>\n",
       "      <td>2674</td>\n",
       "      <td>1338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7057</td>\n",
       "      <td>9810</td>\n",
       "      <td>9568</td>\n",
       "      <td>1762</td>\n",
       "      <td>3293</td>\n",
       "      <td>1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6353</td>\n",
       "      <td>8808</td>\n",
       "      <td>7684</td>\n",
       "      <td>2405</td>\n",
       "      <td>3516</td>\n",
       "      <td>7844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>13265</td>\n",
       "      <td>1196</td>\n",
       "      <td>4221</td>\n",
       "      <td>6404</td>\n",
       "      <td>507</td>\n",
       "      <td>1788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>22615</td>\n",
       "      <td>5410</td>\n",
       "      <td>7198</td>\n",
       "      <td>3915</td>\n",
       "      <td>1777</td>\n",
       "      <td>5185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Channel  Region  Fresh  Milk  Grocery  Frozen  Detergents_Paper  Delicassen\n",
       "0        2       3  12669  9656     7561     214              2674        1338\n",
       "1        2       3   7057  9810     9568    1762              3293        1776\n",
       "2        2       3   6353  8808     7684    2405              3516        7844\n",
       "3        1       3  13265  1196     4221    6404               507        1788\n",
       "4        2       3  22615  5410     7198    3915              1777        5185"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wholesale = pd.read_csv(\"data_input/wholesale.csv\")\n",
    "wholesale.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert the 'Channel' make it a factor and change the labels of the levels into \"horeca\" and \"retail\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code change label of levels in channel column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice here that, unlike the credit risk analysis example, we do not have a balanced dataset. The prior or baseline accuracy for predicting the majority class would be 67.7%. \n",
    "\n",
    "Normalization to z-score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to scale predictor variable\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the dataset into train and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for cross validation\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to classify our target variable, we use a k-NN implementation from the `sklearn.neighbors` library. The `KNeighborsClassifier` function in the sklearn.neighbors library will go through each observation in our `wholesale_train` dataset, and identify the k-Nearest neighbors using Euclidean distance. Each test instance is then assigned the class of the majority of the neighbors - a tie vote is broken at random.\n",
    "\n",
    "\n",
    "We use `k=19` because it's the closest whole number to the square root of our 352, the number of our training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# code for build knn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From those model we can get our prediction to predict the `wholesale_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for predict wholesale_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the model performance, we can see our confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "# code for confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn-by-building Module  \n",
    "As this is a graded task for our students, completion of the task is not optional and count towards your final score.\n",
    "\n",
    "Applying what you’ve learned, present a simple Markdown document in which you demonstrate the use of logistic regression on the `lbb_loans.csv` dataset for a credit risk case or `wholesale.csv` dataset for customer segment prediction case. Explain your findings wherever necessary and show the necessary data preparation steps. To help you through the exercise, consider the following questions throughout the document:  \n",
    "\n",
    "- If you use a logistic regression, how do we correctly interpret the negative coefficients obtained from your logistic regression?  \n",
    "- How do we know which of the variables are more statistically significant as predictors in your logistic regression model? \n",
    "- What is your accuracy? Was the logistic regression better than kNN in terms of accuracy? (recall the lesson on obtaining an unbiased estimate of the model's accuracy)  \n",
    "- Was the logistic regression better than our kNN model at explaining which of the variables are good predictors?\n",
    "- What are some strategies to improve your model? \n",
    "- List down 1 disadvantage and 1 strength of each of the approach (kNN and logistic regression)  \n",
    "\n",
    "Students should be awarded the full points if:  \n",
    "1. The preprocessing steps are done, and the student show an understanding of holding out a test / cross validation set for an estimate of the model’s performance on unseen data\n",
    "2. Student document their analysis on how to improve both of their model. For logistic regression, students are expected to write the model / coefficient interpretation of the probability and for K-nn, students are expected to elaborate the process of finding optimum K.\n",
    "3. The model’s performance is sufficiently explained (accuracy may not be the most helpful metric here! Recall about what you’ve learned confusion matrix and its various metrics)  \n",
    "\n",
    "Student should receive 1 point for each of the above requirements, for a total of (3) points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Objectives\n",
    "In this workshop we'll focus our study on a set of widely-used unsupervised learning methods ranging from PCA (Principal Component Analysis), to Clustering, and other pattern discovery approaches where the target variable is not known or defined. Our goal is to develop a solid intuition behind the problem of dimensionality, the mechanism that is at our disposal, and finally solidify our understanding by working on two of the most common real-life business scenarios.  \n",
    "\n",
    "- **Unsupervised Learning Algorithms**\n",
    "- Clustering Methods\n",
    "- k-means  \n",
    "- Combining PCA with k-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning\n",
    "Throughout the Machine Learning Specialization, we've been learning about algorithms that are greatly useful in situations of regression and classification. More generally, we learn to find the parameters for X1, X2 ... Xn to explain or predict a \"target\" response Y. \n",
    "\n",
    "In the case of unsupervised learning, the situation differs in that there is no such a response Y but rather, we're interested in discovering the structure between X1, X2, to Xn - possibly to identify opportunities for dimensionality reduction or for clustering. Some people have likened unsupervised learning to an exploratory process because it is difficult or impossible to know if the model or any formulation is the \"right\" one since we don't have a \"ground truth\" that we use as a measuring stick. Techniques such as cross-validation and AUC do not apply due to the lack of a \"ground truth\" label. \n",
    "\n",
    "With that said, unsupervised learning methods can still be very powerful especially in the field of clustering and dimensionality reduction. In this workshop, we'll take an in-depth look at unsupervised algorithms such as k-means - and see why unsupervised methods such as these are great tools to add to your toolbox."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means Clustering\n",
    "Clustering refers to the practice of finding meaningful ways to group data (or create subgroups) within a dataset - and the resulting groups are usually called clusters. The objective is to have a number of partitions where the observations that fall into each partition are similar to others in that group, while the partitions are distinctive from one another. \n",
    "\n",
    "K-means is a centroid-based clustering algorithm that follows a simple procedure of classifying a given dataset into a pre-determined number of clusters, denoted as \"k\". This procedure is essentially a series of iterations where we:  \n",
    "1. Find cluster centers  \n",
    "2. Compute distances between each point to each cluster centers  \n",
    "3. Assign / re-assign cluster membership  \n",
    "\n",
    "A few technicality: Instead of saying \"cluster centers\", we'll call them \"centroids\"; Also, in the first iteration of the above procedure, because there are clusters in our feature space, we can't yet compute any centroids so in the first \"iteration\" we'll randomly assign our centroids. It turns out, with enough iteration, that the procedure can usually converge at a reasonably well solution, giving us very reasonable k centroids (remember: we define k, just as in the k-NN algorithm we learned) that we can use for clustering task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the image below:   \n",
    "\n",
    "![ ](assets/centroids.png)\n",
    "\n",
    "If we choose _k_ to be 2, these are the steps that a k-means algorithm take in assigning the original data (green dots) to two clusters:  \n",
    "Step (a): Our data on a two-dimensional space  \n",
    "Step (b): Iteration 1 - Randomly initialize our cluster centroids  \n",
    "Step (c): Iteration 1 - Assigning cluster membership based on a distance function  \n",
    "Step (d): Iteration 2 - Move cluster centroids to be at the center of clusters  \n",
    "Step (e): Iteration 2 - Re-Assigning cluster membership based on a distance function  \n",
    "Step (f): Iteration 3 ...\n",
    "\n",
    "We'll get into the mathematical details a bit later; For now, let's take a look at how we can use R's `kmeans()` function to solve a clustering problem in the absence of a target predictor.\n",
    "\n",
    "### Cluster-based Whisky Recommendation\n",
    "The data we'll be reading in is from Dr.Wisehart (University of St. Andrews), and comprise of 86 distilleries that produce malt whiskies. Each of the whiskies were scored between 0-4 under 12 different taste categories including `Body`, `Sweetness`, `Smoky`, `Medicinal`, `Tobacco`, `Honey`, `Nutty`, `Floral` etc. The original motivation also notes that \"by using correlation data it may be possible to provide whisky recommendations based upon an individual's particular preferences\"[]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowID</th>\n",
       "      <th>Distillery</th>\n",
       "      <th>Body</th>\n",
       "      <th>Sweetness</th>\n",
       "      <th>Smoky</th>\n",
       "      <th>Medicinal</th>\n",
       "      <th>Tobacco</th>\n",
       "      <th>Honey</th>\n",
       "      <th>Spicy</th>\n",
       "      <th>Winey</th>\n",
       "      <th>Nutty</th>\n",
       "      <th>Malty</th>\n",
       "      <th>Fruity</th>\n",
       "      <th>Floral</th>\n",
       "      <th>Postcode</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Aberfeldy</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>\\tPH15 2EB</td>\n",
       "      <td>286580</td>\n",
       "      <td>749680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Aberlour</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>\\tAB38 9PJ</td>\n",
       "      <td>326340</td>\n",
       "      <td>842570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>AnCnoc</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>\\tAB5 5LI</td>\n",
       "      <td>352960</td>\n",
       "      <td>839320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Ardbeg</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>\\tPA42 7EB</td>\n",
       "      <td>141560</td>\n",
       "      <td>646220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Ardmore</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>\\tAB54 4NH</td>\n",
       "      <td>355350</td>\n",
       "      <td>829140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowID Distillery  Body  Sweetness  Smoky  Medicinal  Tobacco  Honey  Spicy  \\\n",
       "0      1  Aberfeldy     2          2      2          0        0      2      1   \n",
       "1      2   Aberlour     3          3      1          0        0      4      3   \n",
       "2      3     AnCnoc     1          3      2          0        0      2      0   \n",
       "3      4     Ardbeg     4          1      4          4        0      0      2   \n",
       "4      5    Ardmore     2          2      2          0        0      1      1   \n",
       "\n",
       "   Winey  Nutty  Malty  Fruity  Floral     Postcode   Latitude   Longitude  \n",
       "0      2      2      2       2       2   \\tPH15 2EB     286580      749680  \n",
       "1      2      2      3       3       2   \\tAB38 9PJ     326340      842570  \n",
       "2      0      2      2       3       2    \\tAB5 5LI     352960      839320  \n",
       "3      0      1      2       1       0   \\tPA42 7EB     141560      646220  \n",
       "4      1      2      3       1       1   \\tAB54 4NH     355350      829140  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whiskies = pd.read_csv(\"data_input/whiskies.txt\")\n",
    "whiskies.head()\n",
    "# Distillery column is the name of each whisky\n",
    "# remove RowID, Postcode, Latitude and Longitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll perform scaling, just as we did in the PCA analyses earlier. With this dataset - scaling is rather arbitary and optional because all measurements assume the same range (0 to 4), but with most data this won't be the case. Recall that the `kmeans` procedure compute a distance (typically Euclidean distance) and as we've learned in the k-NN section of this Specialization, failing to scale may cause our model to perform adequately with the algorithm favoring variables on higher scales. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will have to set a seed for reproducibility because in the first iteration, our centroids are randomly picked on the feature space - thus each time we run the `KMeans()` we are bound to get a slightly different result. Given the objective of **minimizing the within-cluster sum of squared** the k-means algorithm is guaranteed to converge but is not guaranteed to a global optima - a point I'll illustrate later through some code experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# uild kmeans with center 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we use `$iter`, we see that k-means take only 3 iterations to converge, stopping at the third iteration: it already identified 4 sufficiently distinct clusters and further iteration wouldn't improve it any further. The objective has been satisfied. The original algorithm by Lloyd uses this as the objective (minimizing the within-cluster sum of squares):  \n",
    "\n",
    "$\\sum\\limits^k_{i=1}\\sum\\limits_{x_j \\in S_i} (x_j - \\mu_i)^2$\n",
    "\n",
    "Where $\\mu_i$ is the mean of all the points in cluster $S_i$\n",
    "\n",
    "Now let's make this whole idea a lot more concrete by working with some simulated data in code.\n",
    "\n",
    "### Dive Deeper: Understanding k-means\n",
    "In the following experiment, we'll observe how the initialization may not converge to the global optima by simulating some data and changing the seed number iteratively. Here's the code to generate some random data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code randomize x1,y1,x2,y2, and save it in a object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting `x` and `y` from a data will yield the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code plot a$x and a$y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By eye-balling the data, one may make a reasonable argument that there can be three clusters. I'm going to run `KMeans()` on the simulated data, specifying 3 so the k-means algorithm would use 3 number of clusters. Later, you may want to change the seed from 50 to 100 so you can get a visual idea of how the initialization of our centroids will lead to a rather different outcome:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code build kmeans clustering using k=3, print centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the kmeans model `a_k`, we'll now plot our clusters (square) and map the color of our points to the assigned clusters from our **`a_k$cluster`**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code plot x and y and adding center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `kmeans` model we obtained (we named it `a_k`) also has methods that output these two figures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to print within ss and tot of within ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we print `a_k` we will see \"between SS / total SS = 67.8%\", this can serve as another indicator for the goodness of fit. When we compute the sum of squared distances of each point to the global sample mean, we get the total sum of squares. That's also computed for us by `kmeans()` and accessible via `$totss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code of tot ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, when we choose to compute one per cluster (we have 3 clusters) instead of using the global sample mean - and then find the sum of squared distances of these three means to the global mean, we get the between sum of squares (`$betweenss`). When we do this, we also multiply the squared distance of each mean to the global mean by the number of data points it represents:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code of between ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the ratio of `betweenss` and `totss` we get 67.8% which is the same as what the output of `a_k` gives us when we print `a_k`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code of between ss devided by totss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We said earlier that this can be taken as a goodness of the clustering model our k-means has found. It can be thought of as the decomposition of deviance in deviance \"between\" and deviance \"within\" - we want a clustering model that has strong properties of internal cohesion and maximal external separation and so the between sum of squares and total sum of squares ratio as close to 1 as possible indicates a good fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that however, we can arbitrarily improve the \"goodness\" of model by just increasing *k*, so the quality as I've mentioned above is purely mathematical and may not reflect the user's requirement. Often times, as with the case of whiskies clustering, we want to consider external information when picking a good value of k.\n",
    "\n",
    "Run the function below and observe that our within sum of squares decrease as we naively increase the number of clusters - what we're looking for is a point where diminishing returns start to kick in (an elbow) and we start to lose substantial gains: we'll use that point as the number of clusters (*k*) for our kmeans model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for make wss plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing k=4, we'll create our kmeans model (`whi_km`) and we will append the cluster to our original data in a variable named `clust`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code build kmeans using k=4\n",
    "# put cluster centers in whiskies data and name the column with `clust`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:**\n",
    "Now assuming a long-time customer of ours reveal that him (and his spouse) enjoy Laphroig the most, what other whiskies can we recommend?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code whiskies `Laphroig`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn-by-building\n",
    "Using any of kmeans algorithms you've learned, produce a simple markdown document where you demonstrate an exercise of either clustering on one of either the `wholesale.csv` or the `nyc` dataset. \n",
    "\n",
    "Explain your choice of parameters (how you choose *k* for k-means clustering)from the original data. What are some business utility for the unsupervised model you've developed? The Markdown document should be not longer than 4 paragraph, and contain one or two visualization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
